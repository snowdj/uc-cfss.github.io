---
title: "Statistical learning: linear regression"
author: "MACS 30100 - Perspectives on Computational Modeling"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Objectives

* Introduce the functional form of linear regression
* Demonstrate how to estimate linear models using `lm()`
    * Single variable linear regression
    * Multiple linear regression
    * Qualitative predictors
    * Relaxing linear model assumptions
* Explain how to extract model statistics using [`broom`](https://cran.r-project.org/web/packages/broom/index.html)
* Use the [`modelr`](https://github.com/hadley/modelr) package to estimate predicted values and residuals

```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(modelr)
library(broom)

set.seed(1234)

theme_set(theme_minimal())
```

# Linear models

Linear models are the simplest functional form to understand. They adopt a generic form

$$Y = \beta_0 + \beta_{1}X$$

where $y$ is the **outcome of interest**, $x$ is the **explanatory** or **predictor** variable, and $\beta_0$ and $\beta_1$ are **parameters** that vary to capture different patterns. In algebraic terms, $\beta_0$ is the **intercept** and $\beta_1$ the **slope** for the linear equation. Given the empirical values you have for $x$ and $y$, you generate a **fitted model** that finds the values for the parameters that best fit the data.

```{r sim-plot}
ggplot(sim1, aes(x, y)) + 
  geom_point()
```

This looks like a linear relationship. We could randomly generate parameters for the formula $y = \beta_0 + \beta_1 * x$ to try and explain or predict the relationship between $x$ and $y$:

```{r sim-random-fit}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point()
```

But obviously some parameters are better than others. We need a definition that can be used to differentiate good parameters from bad parameters.

# Least squares regression

One approach widely used is called **least squares** - it means that the overall solution minimizes the sum of the squares of the errors made in the results of every single equation. The errors are simply the difference between the actual values for $y$ and the predicted values for $y$ (also known as the **residuals**).

```{r sim-error}
dist1 <- sim1 %>% 
  mutate(
    dodge = rep(c(-1, 0, 1) / 20, 10),
    x1 = x + dodge,
    pred = 7 + x1 * 1.5
  )

ggplot(dist1, aes(x1, y)) + 
  geom_abline(intercept = 7, slope = 1.5, color = "grey40") +
  geom_point(color = "grey40") +
  geom_linerange(aes(ymin = y, ymax = pred), color = "#3366FF")
```

To estimate a linear regression model in R, we use the `lm()` function. The `lm()` function takes two parameters. The first is a **formula** specifying the equation to be estimated (`lm()` translates `y ~ x` into $y = \beta_0 + \beta_1 * x$). The second is the data frame containing the variables:

```{r sim-lm}
sim1_mod <- lm(y ~ x, data = sim1)
```

We can use the `summary()` function to examine key model components, including parameter estimates, standard errors, and model goodness-of-fit statistics.

```{r sim-lm-summary}
summary(sim1_mod)
```

The resulting line from this regression model looks like:

```{r sim-lm-plot}
dist2 <- sim1 %>%
  add_predictions(sim1_mod) %>%
  mutate(
    dodge = rep(c(-1, 0, 1) / 20, 10),
    x1 = x + dodge
  )

ggplot(dist2, aes(x1, y)) + 
  geom_smooth(method = "lm", color = "grey40") +
  geom_point(color = "grey40") +
  geom_linerange(aes(ymin = y, ymax = pred), color = "#3366FF")
```

# Generating predicted values

We can use `sim1_mod` to generate **predicted values**, or the expected value for $Y$ given our knowledge of hypothetical observations with values for $X$, based on the estimated parameters using the `data_grid()` and `add_predictions()` functions from the `modelr` package. `data_grid()` generates an evenly spaced grid of data points covering the region where observed data lies. The first argument is a data frame, and subsequent arguments identify unique columns and generates all possible combinations.

```{r add-predict-data}
grid <- sim1 %>% 
  data_grid(x) 
grid
```

`add_predictions()` takes a data frame and a model, and uses the model to generate predictions for each observation in the data frame.

```{r add-predict}
grid <- grid %>% 
  add_predictions(sim1_mod) 
grid
```

Using this information, we can draw the best-fit line without using `geom_smooth()`, and instead build it directly from the predicted values.

```{r plot-lm-predict}
ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, color = "red", size = 1)
```

This looks like the line from before, but without the confidence interval. This is a bit more involved of a process, but it can work with any type of model you create - not just very basic, linear models.

# Generating residuals

We can also calculate the **residuals**, or the distance between the actual and predicted values of $Y$, using `add_residuals()`:

```{r resids}
sim1 <- sim1 %>% 
  add_residuals(sim1_mod)
sim1

ggplot(sim1, aes(resid)) + 
  geom_freqpoly(binwidth = 0.5)
```

Reviewing your residuals can be helpful. Sometimes your model is better at predicting some types of observations better than others. This could help you isolate further patterns and improve the predictive accuracy of your model.

# Exploring the `credit` dataset

Let's practice exploring data and estimating linear regression models using the `Credit` data set from [ISLR](http://www-bcf.usc.edu/~gareth/ISL/).

## Import data

The first thing we want to do is load the libraries that contain the functions we will use for our analysis. Here we want to load the following libraries:

* `dplyr` - functions for transforming data
* `ggplot2` - graphing functions
* `readr` - import data files
* `modelr` - helper functions for statistical modeling
* `broom` - functions for tidying the results of model objects

> Alternatively you can run `library(tidyverse)` which will automatically load the `ggplot2`, `tibble`, `tidyr`, `readr`, `purrr`, and `dplyr` libraries. Also by installing `tidyverse`, you will automatically install additional libraries used for important tasks such as handling dates and strings, importing SPSS/Stata files, and scraping web data. More information on `tidyverse` [here](https://github.com/tidyverse/tidyverse).

```{r packages2}
library(dplyr)
library(ggplot2)
library(readr)
library(modelr)
library(broom)
```

We can import the `.csv` file using the `read_csv()` function from [`readr`](https://github.com/tidyverse/readr). We also need to remove the ID column and convert the column names to lowercase for consistency and style.

```{r credit}
credit <- read_csv("data/Credit.csv") %>%
  # remove first ID column
  select(-X1)
names(credit) <- stringr::str_to_lower(names(credit))   # convert column names to lowercase
str(credit)
```

## Distribution of credit variable

Initially, we may just want to evaluate the distribution of `balance`. We can use the `ggplot2` library and `geom_histogram()` to generate a histogram plot:

```{r credit-hist}
ggplot(credit, mapping = aes(x = balance)) +
  geom_histogram() +
  labs(title = "Distribution of credit card balances",
       x = "Credit card balance",
       y = "Frequency count of individuals")
```

> Confused by all the `ggplot` functions? Use [this cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/11/ggplot2-cheatsheet-2.1.pdf) to master the syntax and functions.

## Estimate single variable linear regression model

Suppose we want to understand the relationship between an individual's credit limit and their current balance on the credit card. We could visualize the data using a scatterplot:

```{r lifeExp-by-country}
credit %>% 
  ggplot(mapping = aes(x = limit, y = balance)) +
    geom_point()
```

Not too bad. It seems like there is a clear positive trend. Why not estimate a simple linear model that summarizes this trend?

```{r credit-limit}
credit_limit <- lm(balance ~ limit, data = credit)
summary(credit_limit)

grid <- credit %>% 
  data_grid(limit) 
grid

grid <- grid %>% 
  add_predictions(credit_limit) 
grid

ggplot(credit, aes(x = limit)) +
  geom_point(aes(y = balance)) +
  geom_line(aes(y = pred), data = grid, color = "red", size = 1)
```

This is not too bad of a first model. Clearly it is not perfect as it suggests that individuals with a limit below approximately \$1,000 have a negative balance, but it is a good first cut.

## Extracting model statistics

Model objects are not very pretty in R. `lm()` objects are stored in [**lists**](http://r4ds.had.co.nz/vectors.html#lists). One important feature of lists is that they are **recursive** - lists can store other lists. We use the `str()` to print the structure of an object in R:

```{r credit-str}
str(credit_limit)
```

While there are lots of important statistics and information stored inside the `credit_limit` object, it is very difficult to access them. Instead, we might prefer this information could be retrieved in a [**tidy** data frame](http://r4ds.had.co.nz/tidy-data.html). [**Data frames** (and their variants **tibles**)](http://r4ds.had.co.nz/tibbles.html) are one of the most common data objects in R. There are three rules which make a data frame tidy:

1. Each variable must have its own column.
2. Each observation must have its own row.
3. Each value must have its own cell.

In order to extract model statistics and use them in a tidy manner, we can use a set of functions from the [`broom`](https://github.com/tidyverse/broom) package. For these functions, the input is always the model object generated by `lm()`, not the original data frame.

### `tidy()`

`tidy()` constructs a data frame that summarizes the model's statistical findings. This includes **coefficients** and **p-values** for each parameter in a model. Different models will report different elements.

```{r tidy}
library(broom)

tidy(credit_limit)
tidy(credit_limit) %>%
  str()
```

Notice that the structure of the resulting object is a tidy data frame. Every row contains a single parameter, every column contains a single statistic, and every cell contains exactly one value.

### `augment()`

`augment()` adds columns to the original data that was modeled. This could include predictions, residuals, and other observation-level statistics.

```{r augment}
augment(credit_limit) %>%
  tbl_df()
```

By default, `augment()` will only return statistics to the original data used to estimate the model, whereas `add_predictions()` is used to generate predictions for new data. For linear models, `augment()` generates columns for:

* `.fitted` - fitted (or predicted) values based on the model
* `se.fit` - standard errors of the fitted values
* `.resid` - residuals (same as generated by `add_residuals()`)
* `.hat` - diagonal of the hat matrix
* `.sigma` - estimate of the residual standard deviation when the corresponding observation is dropped from the model
* `.cooksd` - Cook's distance, useful for identifying **high leverage points**
* `.std.resid` - standardized residuals (similar in concept to **studentized residuals**)

### `glance()`

`glance()` constructs a concise one-row summary of the model. This typically contains values such as $R^2$, adjusted $R^2$, and residual standard error that are computed once for the entire model.

```{r glance}
glance(credit_limit)
```

For linear models, `glance()` generates several useful model metrics:

* `r.squared` - the percent of variance explained by the model
    * This is one of the metrics identified in ISL for evaluating model fit. It is relatively basic and we will soon consider more robust measures of fit, but it is a quick and dirty metric to compare the effectiveness of competing models of the same response variable.
* `adj.r.squared` - $R^2$ adjusted based on the degrees of freedom of the model
* `sigma` - the square root of the estimated residual variance
* `statistic` - $F$-statistic testing the hypothesis that all parameters are equal to 0
* `p.value` - the $p$-value from the F test
* `df` - degrees of freedom used by the coefficients
* `logLik` - the data's log-likelihood under the model
* `AIC` - the Akaike Information Criterion, used to compare models
* `BIC` - the Bayesian Information Criterion, also used to compare models
* `deviance` - deviance of the model
* `df.residual` - residual degrees of freedom

While `broom` may not work with every model in R, it is compatible with a wide range of common statistical models. A full list of models with which `broom` is compatible can be found on the [GitHub page for the package](https://github.com/tidyverse/broom).

## Generating predicted values with confidence intervals

`add_predictions()` generates predicted values for a dataset given a specified model, however it does not report the standard error of those predictions. To generate confidence intervals, we first use the `augment()` to generate predicted values for new data by using the `newdata` argument, then calculate the 95% confidence intervals manually. For example, what is the predicted credit card balance and 95% confidence interval for an individual with a credit limit of \$2,000, \$5,000, and \$10,000?

```{r limit-pred}
# create data frame with new values
(pred_data <- data_frame(limit = c(2000, 5000, 10000)))

# use augment to generate predictions
(pred_aug <- augment(credit_limit, newdata = pred_data))

# Calculate 95% confidence intervals
(pred_ci <- mutate(pred_aug,
                   ymin = .fitted - .se.fit * 1.96,
                   ymax = .fitted + .se.fit * 1.96))

# do it in one piped operation
(pred_ci <- augment(credit_limit, newdata = data_frame(limit = c(2000, 5000, 10000))) %>%
  mutate(ymin = .fitted - .se.fit * 1.96,
         ymax = .fitted + .se.fit * 1.96))
```

## Estimating multiple linear regression model

`lm()` allows you to estimate linear regression models with multiple variables. For instance, say we want to evaluate an individual's credit balance using both their credit limit and income.

```{r limit-income}
credit_limit_income <- lm(balance ~ limit + income, data = credit)
tidy(credit_limit_income)
```

Now that we have two predictor variables in our model, remember how to accurately interpret these results. $\beta_{j}$ is interpreted as the **average** effect of $Y$ of a one unit increase in $X_{j}$, **holding all other predictors constant**. So the parameter for credit limit tells us the estimated effect on credit card balance of a \$1 increase in the individual's credit limit, after controlling for the effects of income.

## Qualitative predictors

Predictor variables are frequently **quantitative**, but this is not a guarantee. In many datasets you will have **qualitiative** predictors, or variables that have discrete values. In the `credit` dataset, we have four such columns:

```{r qual}
select(credit, gender, student, married, ethnicity)
```

We can include these variables in a linear regression model and they will act as an **indicator** or **dummy** variable that takes on a value of 0 or 1.

### Qualitative predictors with 2 levels

For instance, let's use gender to explain an individual's credit card balance.

```{r gender}
gender <- lm(balance ~ gender, data = credit)
tidy(gender)
```

In the background, R converts the column into a series of 0s and 1s. For this column, it automatically converted `Female` to 0 and `Male` to 1 (it transform the columns alphabetically). If we wish to override this order, we can assign an ordering using the `factor()` function:^[See [R for Data Science](http://r4ds.had.co.nz/factors.html) for more on working with factor variables.]

```{r factor}
credit %>%
  mutate(gender = factor(gender, levels = c("Male", "Female"))) %>%
  lm(balance ~ gender, data = .) %>%
  tidy()
```

Note that the only difference is the directionality of the `genderFemale` parameter is reversed from the previous model.

Or we could convert the column directly to 0s and 1s:

```{r gender-01}
credit %>%
  mutate(female = ifelse(gender == "Female", 1, 0)) %>%
  lm(balance ~ female, data = .) %>%
  tidy()
```

Frequently your data will originally be coded using this 0/1 scheme. If you don't like this, you can always convert it back using the `factor()` approach outlined above:

```{r female}
credit %>%
  select(gender) %>%
  mutate(gender_f = factor(gender, levels = c("Male", "Female")),
         female = ifelse(gender == "Female", 1, 0),
         female_f = factor(female, levels = 0:1, labels = c("Male", "Female")))
```

### Qualitative predictors with more than 2 levels

If your qualitative predictor uses more than two levels (for instance, `ethnicity`), you will include the column in your linear regression model and R will automatically convert it into a series of dummy variables, using 0/1 switches for each dummy variable. R will always omit one of the levels and leave it out as the **baseline**.^[If you do not omit a category, your model will be perfectly multicollinear and you will not be able to estimate it. Alternatively, you can omit the intercept and keep all the original levels.]

```{r ethnicity}
ethnicity <- lm(balance ~ ethnicity, data = credit)
tidy(ethnicity)
```

Here R created dummy variables for "Asian" and "Caucasian", leaving out "African American" as the baseline category.

## Extending the linear model

Remember that because it is a basic functional form, the linear model is very unforgiving in a key assumption: it assumes an **additive** and **linear** shape. The additive assumption requires the effect of changes in any of the predictor variables $X_j$ on the response variable $Y$ to be independent of all other predictors. The linear assumption presumes that the change in $Y$ associated with a one-unit change in $X_j$ is constant regardless of the value of $X_j$. While other, more complicated functional forms can be used to relax this assumption, we can also directly alter these assumptions based on how we specify our model

### Removing the additive assumption: interaction terms

#### Two quantitative variables

Let's evaluate the effect of income and age on an individual's credit card balance.

```{r inc-age}
income_age <- lm(balance ~ income + age, data = credit)
tidy(income_age)
```

Both are statistically significant with sizeable effects on the response variable. However the effects are completely independent from one another. By this I mean, if I plot the relationship between income and predicted balance, the value for limit does not alter this relationship other than to adjust the intercept.

```{r inc-age-plot}
credit %>%
  data_grid(income, age) %>%
  add_predictions(income_age) %>%
  ggplot(aes(income, pred, group = age)) +
  geom_line(alpha = .5)
```

But is this really a safe assumption? After all, an individual's income is based (in part) on their age. The older your are and longer you have worked, the higher your expected income. So wouldn't we expect the relationship between income and credit balance to vary based on an individual's age? That is, **we expect the underlying relationship to directly violate the additive assumption**.

We should therefore relax this assumption by **interacting** income and age. The new linear regression model looks like

$$Y = \beta_0 + \beta_{1}X_1 + \beta_{2}X_2 + \beta_{3}X_{1}X_{2}$$

where $X_1$ is income and $X_2$ is age. To specify this model in R, we write the following code:

```{r inc-age-x}
inc_age_x <- lm(balance ~ income * age, data = credit)
tidy(inc_age_x)
```

Now what happens if we graph the relationship between income and predicted credit card balance, controlling for the credit limit?

```{r inc-age-x-plot}
credit %>%
  data_grid(income, age) %>%
  add_predictions(inc_age_x) %>%
  ggplot(aes(income, pred, group = age)) +
  geom_line(alpha = .5)
```

Not only have the intercepts changed, but so too have the slopes.

#### Quantitative and qualitative variable

We can also use interaction terms with a qualitative variable, such as `student`. Consider the regression model without an interaction

```{r inc-student}
inc_student <- lm(balance ~ income + student, data = credit)
tidy(inc_student)

credit %>%
  data_grid(income, student) %>%
  add_predictions(inc_student) %>%
  ggplot(aes(income, pred, color = student)) +
  geom_line()
```

As before with the quantitative variables, the parameter for income does not change based on the value for student, only the intercept for the model shifts. However for an interactive model:

```{r inc-student-x}
inc_student_x <- lm(balance ~ income * student, data = credit)
tidy(inc_student_x)

credit %>%
  data_grid(income, student) %>%
  add_predictions(inc_student_x) %>%
  ggplot(aes(income, pred, color = student)) +
  geom_line()
```

This suggests that changes in income may affect the credit card balance of students and non-students differently.

### Non-linear relationships

One way to relax the linearity assumption is to use **polynomials** in your regression model. For instance, take the `Auto` data set.

```{r auto}
auto <- read_csv("data/Auto.csv",
                 # make sure horsepower is parsed as numeric
                 col_types = cols(horsepower = col_number())) %>%
  # remove missing data
  na.omit(horsepower)

# estimate linear model of horsepower and mpg
horse <- lm(mpg ~ horsepower, data = auto)
tidy(horse)

# generate predicted values
horse_pred <- auto %>%
  add_predictions(horse)

# draw the graph
ggplot(horse_pred, aes(horsepower)) +
  geom_point(aes(y = mpg), alpha = .5) +
  geom_line(aes(y = pred), color = "orange", size = 1)
```

Is a linear regression line of the form $\text{mpg} = \beta_0 + \beta_{1}\text{horsepower}$ really the best fit here? The relationship appears to have a curvilinear shape to it. Instead, we can estimate a model of the form

$$\text{mpg} = \beta_0 + \beta_{1}\text{horsepower} + \beta_{2}\text{horsepower}^2$$

```{r auto2}
# estimate polynomial squared model of horsepower and mpg
horse2 <- lm(mpg ~ horsepower + I(horsepower^2), data = auto)
tidy(horse2)

# generate predicted values
horse2_pred <- auto %>%
  add_predictions(horse2)

# draw the graph
ggplot(horse2_pred, aes(horsepower)) +
  geom_point(aes(y = mpg), alpha = .5) +
  geom_line(data = horse_pred, aes(y = pred), color = "orange", size = 1) +
  geom_line(aes(y = pred), color = "blue", size = 1)
```

Or even a polynomial to the fifth power

$$\text{mpg} = \beta_0 + \beta_{1}\text{horsepower} + \beta_{2}\text{horsepower}^2 + \beta_{3}\text{horsepower}^3 + \beta_{4}\text{horsepower}^4 + \beta_{5}\text{horsepower}^5$$

```{r auto5}
# estimate polynomial fifth-order model of horsepower and mpg
# use the poly() function to generate the powers
horse5 <- lm(mpg ~ horsepower + poly(horsepower, degrees = 5), data = auto)
tidy(horse5)

# generate predicted values
horse5_pred <- auto %>%
  add_predictions(horse5)

# draw the graph
ggplot(horse5_pred, aes(horsepower)) +
  geom_point(aes(y = mpg), alpha = .5) +
  geom_line(data = horse_pred, aes(y = pred), color = "orange", size = 1) +
  geom_line(data = horse2_pred, aes(y = pred), color = "blue", size = 1) +
  geom_line(aes(y = pred), color = "green", size = 1)
```

How do we know which model is most appropriate? One thing we could do is compare model fit statistics:

```{r auto-fit}
# combine models into a list and use a map() function to apply
# glance() to each model and store the result in a tidy data frame
list("degree_1" = horse,
     "degree_2" = horse2,
     "degree_5" = horse5) %>%
  map_df(glance, .id = "id")
```

Based on the $R^2$ values, the fifth-order polynomial explains the most variation in `mpg`. But the fifth-order polynomial is also more complicated and throws extra bends and curves into the predicted values for `mpg`. Ultimately whatever form we specify for the model, we are still making an **assumption** that the true relationship between horsepower and mileage takes on this form - ultimately it is untestable.

# Session Info {.toc-ignore}

```{r child='_sessioninfo.Rmd'}
```






