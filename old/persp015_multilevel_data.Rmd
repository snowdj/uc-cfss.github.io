---
title: "Multilevel data"
author: "MACS 30200 - Perspectives on Computational Research"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Objectives

* Identify types of multilevel data structures
* Explain why classical GLM regression models don't work with multilevel data
* Introduce random effects models
* Estimate a random effects model using the partisan defection dataset
* Identify limitations and drawbacks to MLM with traditional approximation methods

```{r packages, cache = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(broom)
library(forcats)
library(modelr)
library(stringr)
library(car)
library(rcfss)
library(coefplot)
library(RColorBrewer)
library(lme4)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal())
```

# What influences partisan defection in voters?

What motivates partisan individuals to cross party lines and vote for a presidential candidate from the opposition? In an era of heightened partisan polarization Democrats and Republicans are increasingly reluctant to cast ballots for their partisan opponents, making these individuals even stronger of outliers. While only a relatively small number of self-identified partisan individuals cast a defecting vote in the United States, the outcome of closely contested elections may be decided by these actions. Understanding individual motivations to cast aside partisanship improves our understanding of how deeply PID influences individual political behavior.

Based upon previous studies of partisan defection, I formed several hypotheses for how these factors influence vote selection in partisan individuals:

* **Partisan intensity** - individuals who more strongly identify with a political party have a lower probability of defecting. If partisan identification is related to vote choice and individuals can possess different intensities of PID, stronger partisans should have more attachment to their party's candidate and be less likely to defect compared to weak or leaning partisans. This effect should be present for individuals affiliated with either major party and can apply in any election. Since partisanship is strongly associated with vote choice and is theorized to be a core psychological predisposition, PID intensity could moderate the effects of other causes of partisan defection.
* **Relative favorability** - as the perceived favorability of an opposing party's candidate increases relative to the own party's candidate, the probability of a voter's partisan defection increases. Under certain circumstances short-term influences have been shown to significantly impact an individual's vote choice. Intangible traits such as likability or relative warmth might conceivably influence individual vote decisions. Cited as the "with whom would you like to have a beer?" test, political commentators and scholars have explored this relationship with mixed results. If the opposing party's candidate seems more appealing, regardless of policy views, individuals may be persuaded to defect for the more charismatic candidate. This might potentially explain specific episodes of defection such as the Reagan Democrats in 1984 and Republicans for Obama in 2008 where one candidate was perceived as more charismatic than the other.
* **Correct is defect** - if an individual's correct vote decision requires him to defect, then the probability of defection increases. Correct voting decision factors include policy preferences, evaluation's of candidates' traits, and social group identification. As such, correct voting should be, and in fact is, closely associated with actual voting decisions. If an individual's correct vote decision requires the voter to defect, that individual should have a higher probability of actually defecting relative to voters whose correct vote decisions match their partisan preference. This influence should be relevant to all voters in any election.
* **Incumbency** - when the incumbent is a candidate for election, voters who identify with the opposition party have a higher probability of defecting. When an election occurs and the incumbent does not participate, incumbency advantage will have no effect (the candidate from the incumbent's party is not expected to receive any advantage). When the incumbent does participate, the probability of defection should only increase for voters who identify with the opposition's party. Incumbents are unlikely to consistently cause voters from their own party to defect, so under this hypothesis the probability of defection differs across parties. There should be no effect on voters affiliated with the incumbent's party (e.g. Republicans and Reagan in 1984), but the effect should be positive for opposition party members (Democrats in '84).

# Traditional GLM: issues related to pooling

**Pooling** is nothing more than combining data, either across units or time. In this instance, I am pooling [American National Election Studies (ANES)](http://www.electionstudies.org/) observations over time. The key to pooling is exchangability: the notion that, conditional on the values of the covariates, any two observations within the data are considered to be the same (exchangable).

## Why Pool?

Several reasons:

* **Pooling adds data** - this is the number one reason for pooling data. If the assumption of poolability holds, we can get "better" (read: more precise) estimates of our $\hat{\beta}$s.
* **Generalizability** - again, if a case can be made for model-conditional poolability, adding different cases means we can be more sure that our data generalized to broader sets of cases and/or longer time periods.
* **Readability** - if I estimate separate models for each election year, I have to report results for 10 different models.

In this study, I want to understand how partisan defection has occurred over time, not in a single election. Instead of having 752 observations from 2004, I can utilize over 10,000 data points from 10 different elections. I'd rather estimate and report a single model than have to report results from 10 different models. Pooling the data will also produce more generalizable results and only require interpretation of a single model.

## Issues with pooling

Implicit in this analysis, then, is the idea that the coefficients $\beta$ do not vary over subsets of the data defined along $\mathbf{E} \in \{1972 ,\dotsc, 2008 \}$. In (say) the general, restrictive model:

$$
\begin{align}
 Pr(Y_{ei} = 1) &= \text{logit}^{-1}[\alpha + \beta_{1}\text{PID Intensity}_{ei} + \beta_{2}{\text{Relative Favorability}_{ei}} \nonumber \\
 &\: + \beta_{3}\text{Defect is Correct}_{ei}  + \beta_{4}\text{Incumbent Candidate}_{ei}] \nonumber \\
  \text{logit}^{-1}[x] &= \frac{e^x}{1+e^x}
\end{align}
$$

the implicit assumption is one of **exchangability** -- i.e., that all of the data come from the same **regime**, that is,

* that the process governing the relationship between $X$ and $Y$ is exactly the same for each $e$,
* that the process governing the relationship between $X$ and $Y$ is the same for all $i$,
* that the process governing the $u$s is the same $\forall \: e$ and $i$ as well.

If any of these assumptions are not true, the pooled estimator $\hat{\beta}_P$ will be biased towards the regime with:

* the larger $N$,
* the larger values of the coefficients, and/or
* the smaller standard errors of $\hat{\beta}$

## The error term

Note as well that, throughout all this discussion, we've been assuming that the error term $u_{it}$ is homoscedastic and uncorrelated, both within and across $i$ and $t$. Formally, that means we need to have:

$$u_{ij} \sim i.i.d.N(0, \sigma^2), \: \forall\: i,j$$

This is exactly what we've done in the previous model specifications with $\sigma^2_y$. If you stop and think about it, that's a pretty tall order. In particular, it requires that:

$$
\begin{align}
Var(u_{ab}) &= Var(u_{cb}) \: \forall \: a \neq b  \text{ (i.e., no cross-unit heteroscedasticity)},  \\
Var(u_{ab}) &= Var(u_{ac}) \: \forall \: b \neq c \text{ (i.e., no temporal heteroscedasticity)},  \\
Cov(u_{ab},u_{cd}) &= 0 \: \forall \: a \neq c,\: \forall \: b \neq d \text{ (i.e., no auto- or spatial correlation)}
\end{align}
$$

Remember: Residuals are (among other things) just an indicator of how good a job the model does of explaining $Y$ with $\mathbf{X}$. In that light, these assumptions are violated if (for example):

* Cross-unit differences mean that the model does a better job of explaining some units than others,
* Time effects (such as socialization, institutionalization, learning, or other such dynamics) cause the model to do a better or worse job of explaining $Y$ over time,
* Omitted variables lead to residual correlation, either across units or (more commonly) over time.

While in a linear model, at least, problems with the error term don't bias coefficient estimates, they can screw up one's inferences pretty badly. And in nonlinear models (i.e. logits and all other GLMs) they can also lead to biases in the point estimates as well.

Additionally, pooling can lead to biased standard errors. This is due to the assumption of GLMs that the errors for each observation are independent of one another. Formally, $E(u_i*u_j)=0 \: \forall \: i,j \: \text{combinations}$. If this assumption is violated and errors in some observations are related to the errors in other observations (likely within election year), the model appears to have more power than it otherwise should. This will create artificially deflated standard errors and make point estimates appear more precise.

## Simulation of bad pooling

$$
\begin{align}
y_{i} &\sim N(\alpha_{j[i]} + \beta x_{i},\sigma^2_y), &\text{for } i=1,\dotsc,n \nonumber \\
\alpha_j &\sim N(\mu_{\alpha},\sigma^2_{\alpha}), &\text{for } j=1,\dotsc,J
\end{align}
$$

```{r sim-pool}
obs <- 60
group.obs <- 6
groups <- rep(1:group.obs, times = obs / group.obs)

#varying intercept
beta <- 2
alpha <- rnorm(group.obs,2,2)

vary_int <- data_frame(x = runif(obs,0,3),
                       y = alpha + beta * x,
                       groups = factor(groups))

p <- ggplot(vary_int, aes(x, y)) +
  geom_point(aes(color = groups, shape = groups)) +
  labs(title = "Simulated data with varying intercepts",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")
p

p +
  geom_smooth(method = "lm") +
  labs(subtitle = "OLS")
```

## Partisan defection and pooling issues

Am I likely to violate any of these assumptions using this pooled dataset? Yes. The mean probability of partisan defection differs significantly across elections.

```{r anes-data}
library(haven)

# read in data
anes <- read_dta("data/anes_pres.dta")

# generate variables
anes <- anes %>%
  #generate binary party measures
  mutate(rep = ifelse(pid < 4, 0,
                      ifelse(pid > 4, 1, NA))) %>%
  #generate measure of defection and whether or not respondent actually voted correctly
  mutate(defect = NA,
    defect = replace(defect, which((pres_vote_r == 0) &
                                     rep == 0), 0),
    defect = replace(defect, which((pres_vote_r == 1) &
                                     rep == 1), 0),
    defect = replace(defect, which((pres_vote_r == 1 | pres_vote_r == 6) &
                                     rep == 0), 1),
    defect = replace(defect, which((pres_vote_r == 0 | pres_vote_r == 6) &
                                     rep == 1), 1),
    vote_cor_actual = ifelse(pres_vote_r == vote_cor, 1, 0)) %>%
  # does the correct vote require one to defect?
  mutate(defect_cor = ifelse(vote_cor != rep, 1, 0)) %>%
  # generate index of PID intensity
  mutate(pid_abs = abs(pid - 4) - 1) %>%
  # remove 1948
  filter(year != 1948) %>%
  # does feeling thermometer towards candidates explain defection?
  mutate(feel_def = ifelse(rep == 1, (dem_feel - rep_feel) / 2,
                           ifelse(rep == 0, (rep_feel - dem_feel) / 2, NA))) %>%
  #does incumbency influence partisan defection?
  mutate(inc = ifelse(year %in% c(1956, 1964, 1972, 1976,
                                  1980, 1984, 1992, 1996, 2004), 1, 0),
         inc_opp = ifelse((rep == 1 & year %in% c(1964, 1980, 1996)) |
                            (rep == 0 & year %in% c(1956, 1972, 1976,
                                                    1984, 1992, 2004)), 1, 0))
```

```{r prop-defect}
#generate a plot of the percentage of partisan defectors in each election
##get weighted proportions for all, dems only, and reps only
anes %>%
  group_by(year) %>%
  summarize(defect = weighted.mean(defect, std_wt, na.rm = TRUE)) %>%
  ggplot(aes(year, defect)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Percentage of partisan defectors",
       x = "Presidential election year",
       y = "Percentage of partisan defectors")
```

Unless the composition of the electorate is changing significantly from year-to-year in a specific way, this assumption will not hold. Regardless, this assumption should be tested and explored, which is not possible under a standard GLM approach.

# Multilevel data structures

Consider the classical linear model:

$$y_{i} \sim N(\alpha + \beta x_{i}, \sigma^2_{y}), \: \text{for}\: i=1,\dotsc,n$$

Implicitly, this model assumes (along with classical OLS assumptions):

* That the constant term is constant across different $i$s, and
* That the effect of any given variable $X$ on $Y$ is constant across observations^[At least, to the extent that non-constancy isn't specified in the model, e.g. through interaction terms.]

We can write a similar model in the panel context as follows:

$$y_{ij} \sim N(\alpha + \beta x_{ij}, \sigma^2_{y}), \: \text{for}\: i=1,\dotsc,n; j = 1, \dotsc,k$$

Note that this model assumes the same things as the earlier ones, especially about the effects of constants and covariates.

In **any** regression context, the two assumptions mentioned are critical; violating them leads to a form of specification bias. In the panel context, these two assumptions are often going to be problematic. This is because, since we're observing multiple units over time or across groups, there's often (in fact, usually) some reason to believe that there may be differences in either $\alpha$ or $\beta$ over either $i$ or $j$. How could we correct for these possibilities?

## Variable intercepts

One possible violation of the above assumptions is that the intercepts vary. The most common way this occurs is for different units to have varying intercepts:

$$y_{ij} \sim N(\alpha_i + \beta x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n$$

The slopes for each unit are the same, but the intercepts are different. It's also possible that the intercepts vary over time, rather than over units:

$$y_{ij} \sim N(\alpha_t + \beta x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n$$

If we have data that corresponds to the first formula, but estimate a model like the second formula, we can get biased coefficients.

## Variable slopes

The other obvious possibility is that we have a constant intercept, but the effects of $X$ on $Y$ differs across either units or (less likely) time:

$$y_{it} \sim N(\alpha + \beta_{i} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n$$

We could also have variation in $\beta$ over time, or even over both units and time.

A model like above assumes that the regression lines all pass through the same point on the $Y$-axis, but that their slopes differ. The idea of a common intercept, however, is a bit strange (at least to social scientists). Instead what is more likely...

## Variable slopes and intercepts

This is when things really start to get difficult. We might, for example, have variable slopes and intercepts for each unit $i$:

$$y_{ij} \sim N(\alpha_{i} + \beta_{i} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n$$

Moreover, we could instead have different $\alpha$s and $\beta$s for every time point, rather than for every unit:

$$y_{ij} \sim N(\alpha_{t} + \beta_{j} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n$$

or for both different units and time points:

$$y_{ij} \sim N(\alpha_{it} + \beta_{it} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n$$

Using the incorrect model specification for the data can lead to odd, even nonsensical, results: underestimating some slopes, overestimating others, and in some cases even getting the sign wrong.

All of this leads to...

# Multilevel modeling

What many social scientists refer to as **random effects**, [Gelman and Hill](http://www.stat.columbia.edu/~gelman/arm/) simply call **multilevel modeling**. **Fixed effects** are usually defined as varying coefficients that are not themselves modeled. For example, a classical regression including $J-1$ unit indicators as regression predictors is sometimes called a **fixed-effects model**. As we will see, fixed effects are a subtype of random effects; I (and others) recommend always using random effects and focusing on the description of the model itself (for example, varying intercepts and constant slopes), with the understanding that batches of coefficients (for example, $\alpha_1,\dotsc,\alpha_J$) will themselves be modeled.

## Fixed effects

**Fixed effects** has several potential meanings in a regression model. Most commonly, fixed effects are when an input variable with $J$ categories is incorporated into a regression model as $J-1$ dummy (dichotomous) variables, with one category held out to serve as the baseline. In essence, this adds a coefficient for each category. The goal of this is to parse out all between-category effects on $Y$, allowing the remaining variables to explain only within-category effects.

This approach is fine (assuming $J$ is small relative to $n$) as long as you have no category-level predictors or explanatory variables in the model. So in our partisan defection model, this is fine if we include fixed effects for election year and test partisan intensity, relative favorability, and correct is defect hypotheses:

```{r fixed-eff}
basic.fix <- glm(defect ~ pid_abs + feel_def + defect_cor + factor(year),
             data = anes, family = binomial(link = "logit"))
tidy(basic.fix)
coefplot(basic.fix,
         title = "Fixed effects GLM of partisan defection",
         newNames = c("pid_abs" = "PID intensity",
                      "feel_def" = "Relative favorability",
                      "defect_cor" = "Defect is correct",
                      "inc" = "Incumbent candidate",
                      "factor(year)1976" = "1976",
                      "factor(year)1980" = "1980",
                      "factor(year)1984" = "1984",
                      "factor(year)1988" = "1988",
                      "factor(year)1992" = "1992",
                      "factor(year)1996" = "1996",
                      "factor(year)2000" = "2000",
                      "factor(year)2004" = "2004",
                      "factor(year)2008" = "2008"),
         decreasing = TRUE)
```

We estimate coefficients for the three explanatory variables, as well as dummy coefficients for each election in the model.^[We don't have information on most of the explanatory variables for pre-1972 elections.] Each dummy coefficient represents the comparison to the baseline category, 1972. For example, the odds of partisan defection in the 2000 election was `r exp(coef(basic.fix)[["factor(year)2000"]])` of the odds of defection in 1972.

One major problem with this approach is that is assumes all the variation in the outcome of interest is systematically explainable only by individual-level variables. That is, we cannot add group-level predictors to the model:

```{r fixef-collin}
summary(glm(defect ~ pid_abs + feel_def + defect_cor + inc + factor(year),
            data = anes, family = binomial(link = "logit")))
```

The problem is one of [**perfect collinearity**](http://cfss.uchicago.edu/persp012_regression_diagnostics.html#perfect_collinearity):

```{r year-inc}
ggplot(anes, aes(year, inc)) +
  geom_point() +
  labs(title = "Perfect prediction",
       x = "Election year",
       y = "Incumbent in election")

# correlation coefficient, by year
anes %>%
  group_by(year) %>%
  summarize(cor = cor(year, inc))
```

Because incumbency doesn't vary at the election-level, it is completely constant within elections. You cannot even estimate a correlation coefficient because `inc` is a constant! Therefore R has to drop one of the dummy variables just to estimate the model.

## Random effects

Multilevel regression can be thought of as a method for compromising between the two extremes of excluding a categorical predictor from a model (**complete pooling**), or estimating separate models within each level of the categorical predictor (**no pooling**). The former ignores within-unit variation, while the latter ignores between-unit variation. Additionally, the no pooling approach prevents the researcher from conducting inference using categorical predictors.

Instead, multilevel modeling uses a compromise between these two approaches. A **soft constraint** is applied to the coefficients: they are assigned a probability distribution.

### Varying intercepts

A varying intercept model could take the following form:

$$
\begin{align}
y_i &\sim N(\alpha_{j[i]} + \beta x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
\alpha_{j} &\sim N(\mu_{\alpha},\sigma^2_{\alpha}), \: \text{for} \: j=1,\dotsc,J,
\end{align}
$$

$\alpha_{j}$ is now assumed to be distributed normally with a mean $\mu_{\alpha}$ and variance $\sigma^2_{\alpha}$ estimated from the data. This pools the estimates of $\alpha_{j}$ toward the mean level $\mu_{\alpha}$, but not all the way; thus, group-level estimates are the result of a **partial-pooling** compromise. In the limit of $\sigma_{\alpha} \to \infty$, the soft constraints do nothing, and there is no pooling; as $\sigma_{\alpha} \to 0$, they pull the estimates all the way to zero, yielding the complete-pooling estimate. This demonstrates that even if there is no dependence in the errors between groups, multilevel regression will still produce accurate point estimates and standard errors. **Instead of making an assumption about independence, we can directly model it and let the data tell us if any exists.**

This approach also reduces the number of parameters needing to be estimated. For example, a fixed effects approach with 20 groups would require estimating $J-1$ parameters (19). Introducing a soft-constraint and modeling $\alpha$ as a random draw from a single distribution, all we need to estimate is the parameters for this distribution. Typically we assume that the distribution is normal, so we only need to estimate two parameters (the mean $\mu$ and the variance $\sigma^2$). This increases our degrees of freedom (which could be an issue if we have lots of groups in the dataset). As seen below, this also allows us to introduce group-level predictors into the model.

```{r sim-vary-int}
#draw x and c from random normal distribution, adjusting for group differences
x <- runif(obs, 0, 3) + groups
c <- rep(rnorm(group.obs, 2, 1), times = obs / group.obs) * groups
beta <- 2		#set beta
y <- beta*(x) - c #+ rnorm(obs,0,1)		#generate y from x, c, beta, and random noise

sim_data_mlm <- data_frame(x, y, groups)

# plot data
ggplot(sim_data_mlm, aes(x, y)) +
  geom_point(aes(color = factor(groups), shape = factor(groups))) +
  geom_smooth(aes(color = factor(groups)), method = "lm", se = FALSE,
              fullrange = TRUE, size = .5) +
  geom_smooth(method = "lm") +
  labs(title = "Simulated data with varying intercepts",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")
```

#### Adding group-level predictors

Perhaps we believe certain group-level variables systematically influence the baseline level of $y_{ij}$. Using this flexible framework, we can simply include them directly in the model as predictors of $\alpha_{J}$:

$$
\begin{align}
y_i &\sim N(\alpha_{j[i]} + \beta x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
\alpha_{j} &\sim N(\gamma_{0} + \gamma_{1}u_{j},\sigma^2_{\alpha}), \: \text{for} \: j=1,\dotsc,J,
\end{align}
$$

where $x_{i}$ is a unit-specific indicator and $u_{j}$ is a group-level measure. Since we are leveraging multiple observations within each group, we can now include group-level measures without worrying about overspecifying the model or inducing perfect collinearity. The $\gamma$s can be interpreted in the same manner as the $\beta$s as they all are measured on the same dimension.

* For linear regression: a one-unit increase on $u_{j}$ is associated with a $\gamma$ increase in $y_{ij}$
* For logistic regression: a one-unit increase on $u_{j}$ is associated with a $\gamma$ increase in the log-odds of the $Pr(y_{ij}=1)$).

### Varying slopes

Now what if we believe the effects of $\beta$ vary by group? We can treat this the same as we did with the varying intercept; thus,

$$
\begin{align}
y_i &\sim N(\alpha_{j[i]} + \beta_{j[i]} x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
 \begin{pmatrix}
 \alpha_{j} \\
 \beta_{j} \\
 \end{pmatrix} &\sim N\left(\begin{pmatrix}
 \mu_{\alpha} \\
 \mu_{\beta} \\
 \end{pmatrix},\begin{pmatrix}
 \sigma^2_{\alpha} & \rho \sigma_{\alpha} \sigma_{\beta} \\
 \rho \sigma_{\alpha} \sigma_{\beta} & \sigma^{2}_{\beta} & \\
 \end{pmatrix}\right), \: \text{for} \: j=1,\dotsc,J,
\end{align}
$$

with variation in the $\alpha_{j}$'s and the $\beta_{j}$'s and also a between-group correlation parameter.

```{r sim-vary-slope}
# constant intercept
alpha <- 0
beta <- rnorm(group.obs, 2, 1)
y <- alpha + beta * x

sim_data_mlm <- data_frame(x, y, groups)

# plot data
ggplot(sim_data_mlm, aes(x, y)) +
  geom_point(aes(color = factor(groups), shape = factor(groups))) +
  geom_smooth(aes(color = factor(groups)), method = "lm", se = FALSE,
              fullrange = TRUE, size = .5) +
  geom_smooth(method = "lm") +
  labs(title = "Simulated data with varying slopes",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")

# varying intercept
alpha <- rnorm(group.obs,2,2)
y <- alpha + beta * x

sim_data_mlm <- data_frame(x, y, groups)

# plot data
ggplot(sim_data_mlm, aes(x, y)) +
  geom_point(aes(color = factor(groups), shape = factor(groups))) +
  geom_smooth(aes(color = factor(groups)), method = "lm", se = FALSE,
              fullrange = TRUE, size = .5) +
  geom_smooth(method = "lm") +
  labs(title = "Simulated data with varying intercepts and slopes",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")
```

We can also include group-level predictors:

$$
\begin{align}
y_i &\sim N(\alpha_{j[i]} + \beta_{j[i]} x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
 \begin{pmatrix}
 \alpha_{j} \\
 \beta_{j} \\
 \end{pmatrix} &\sim N\left(\begin{pmatrix}
 \gamma^{\alpha}_{0} + \gamma^{\alpha}_{1} u_{j} \\
 \gamma^{\beta}_{0} + \gamma^{\beta}_{1} u_{j} \\
 \end{pmatrix},\begin{pmatrix}
 \sigma^2_{\alpha} & \rho \sigma_{\alpha} \sigma_{\beta} \\
 \rho \sigma_{\alpha} \sigma_{\beta} & \sigma^{2}_{\beta} & \\
 \end{pmatrix}\right), \: \text{for} \: j=1,\dotsc,J,
\end{align}
$$

In this formulation, we believe the same group-level variable explains both the intercept and the observation-specific covariate. This does not have to be true; we can include any combination of group-specific covariates in the separate models for unit-specific effects. Note that we should not use unit-specific variables in the models for $\alpha$ and $\beta$. If we believe such a relationship exists, we should specify it as an interaction in the model specific to $y_{ij}$.

## Other specifications of MLM

Multilevel modeling is an extremely flexible framework which allows researchers to introduce all sorts of interesting specifications of models. Different types of data structures and specifications include:

* Three or more levels of data
* Repeated measures (i.e. panel data)
* Time-series cross sections
* Non-nested structures
* Varying slopes without varying intercepts
* Cross-level interactions
* Discrete and non-normally distributed outcomes (e.g. dichotomous outcomes, binomial trials, count data, ordinal and multinomial outcomes)
* Bayesian estimation procedures
* Measuring sample size and power calculations
* Multilevel regression, imputation, and post-stratification (MRP)

# Implementing MLM for partisan defection

Let the classical model be specified as

$$
\begin{align}
 Pr(Y_{ei} = 1) &= \text{logit}^{-1}[\alpha + \beta_{1}\text{PID Intensity}_{ei} + \beta_{2}{\text{Relative Favorability}_{ei}} \nonumber \\
 &\: + \beta_{3}\text{Defect is Correct}_{ei}  + \beta_{4}\text{Incumbent Candidate}_{e}] \nonumber \\
  \text{logit}^{-1}[x] &= \frac{e^x}{1+e^x}
\end{align}
$$

where $Y_{ei}$ is a dichotomous variable coded one if the respondent reports voting for a presidential candidate from a different party other than his/her own, $\alpha$ is a constant, $\beta$s represent the log-odds association of the covariates with the probability of defection, and observations are pooled over elections ($e \in \{1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008\}$). This is different from a traditional time series-cross sectional design in that respondents $i$ are not repeatedly interviewed: each election consists of a different representative sample of U.S. citizens. Therefore, the primary focus of the multilevel model should be on independence between elections. Will the baseline probability of defection differ across elections? Will the effects of partisan intensity or the other unit-specific measures differ across elections?

## Classical logistic regression

Let's first examine the results of a classical logit model, without controlling for dependence within elections:

```{r anes-logit}
basic <- glm(defect ~ pid_abs + feel_def + defect_cor + inc,
             data = anes, family = binomial(link = "logit"))
tidy(basic)
coefplot(basic,
         title = "Classical GLM of partisan defection",
         newNames = c("pid_abs" = "PID intensity",
                      "feel_def" = "Relative favorability",
                      "defect_cor" = "Defect is correct",
                      "inc" = "Incumbent candidate"),
         decreasing = TRUE)
```

These results indicate when the covariates equal zero, individuals are more likely to vote for a candidate from their own party rather than defect ($\alpha=-0.83$). PID intensity is also negatively associated with partisan defection (consistent with the hypothesized direction), while relative favorability, defect is correct, and incumbent candidate are all positive. Importantly, the standard errors for each coefficient are low with $\textit{p-value} < 0.01$. A classical logistic regression model suggests our coefficient estimates are very precise, and effects do not vary across election years (because we did not allow them to vary).

## Varying intercept

What if we relax the assumption that the baseline probability of defection is constant for all observations? Let us model $\alpha$ directly from the data, specifying the model as:

$$
\begin{align}
 Pr(Y_{ei} = 1) &= \text{logit}^{-1}[\alpha_{e[i]} + \beta_{1}\text{PID Intensity}_{ei} \nonumber \\
 &\quad + \beta_{2}{\text{Relative Favorability}_{ei}} + \beta_{3}\text{Defect is Correct}_{ei}]  \nonumber \\
 \alpha_{e} &\sim N(\gamma_{0}^{\alpha} + \gamma_{1}^{\alpha}\text{Incumbent},\sigma^2_{\alpha}), \: \text{for} \: e=1976,\dotsc,2008, \nonumber \\
 \text{logit}^{-1}[x] &= \frac{e^x}{1+e^x}
\end{align}
$$

We can use the `glmer()` function in `lme4` library to estimate generalized linear multilevel models:

```{r anes-vary-int}
vary.int <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc + (1|year),
                  data = anes,
                  family = binomial(link = "logit"))
tidy(vary.int)
summary(vary.int)

multiplot(basic, vary.int,
         title = "Regression models of partisan defection",
         newNames = c("pid_abs" = "PID intensity",
                      "feel_def" = "Relative favorability",
                      "defect_cor" = "Defect is correct",
                      "inc" = "Incumbent candidate"),
         names = c("Classical GLM", "Varying intercept"),
         decreasing = TRUE) +
  theme(legend.position = "bottom")
```

The top of the summary reports general model statistics such as log-likelihood, deviance, AIC, and BIC. These statistics can be used for model comparisons and relative goodness-of-fit tests. Below this is a table with the variance components. Since logistic regression assumes a constant unit-level variance of $\frac{\pi^2}{3}$, there is no variance component for this level. The only variance component estimated in the model is $\sigma^2_{\alpha}$. The standard deviation is 0.34, suggesting a significant amount of variance around the average intercept. This result confirms our expectation since we know the mean defection rate differs across elections.

The remaining estimated coefficients do not differ significantly from the original non-MLM results.

Multilevel modeling also allows us to retain group-specific estimates of varying coefficients. These estimates are combinations of the fixed and random effects.^[Alternative approaches include Bayesian shrinkage estimators, which are combinations of complete and no pooling estimates using the reliability of the group-specific estimate to determine the weighted average. Though biased towards the overall mean, this estimator is more precise than either complete or no pooling estimates since it accounts for uncertainty in the model. Groups with larger sample sizes will be weighted more heavily towards the mean value of the observations within the group since we possess more certainty about the group-specific contribution to the model.] These group-specific effects can be generated using the `coef()` function:

```{r vary-int-coef}
coef(vary.int)
```

In every year, respondents on average have a low baseline log-odds of defecting (holding all covariates equal to zero). As suggested by $\sigma^2_{\alpha}$, the magnitude of this coefficient varies significantly, ranging from a low of $`r min(coef(vary.int)$year[1])`$ to a high of $`r max(coef(vary.int)$year[1])`$.

## Varying slope(s) and intercept

Now let us consider potential varying slopes. While we could estimate the model with a varying slope for each coefficient, unless we have hundreds of thousands of observations we will quickly run into issues maximizing the log-likelihood (this will be covered shortly under the limitations section). First and foremost we should use theory to guide our selection of varying slopes. However if it seems plausible that any of the covariates should need a varying slope, we need a method to determine which coefficients should receive one in the final model.

One can use a likelihood ratio test to determine whether or not the additional variance component significantly improves overall model fit. Note that since GLM implementations of MLM utilize the Laplace approximation rather than full MLE, deviance or likelihood ratio tests can only be applied to models with identical constant parameters. That is, not only must the dataset be the same, the model specifications must be identical except for the variance component. The results of a test to determine if a varying slope should be utilized for PID intensity is:

```{r lrtest}
defect <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc + (1|year),
                data = anes, family = binomial(link = "logit"))
defect.pid3.rc1 <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc +
                           (1|year) + (pid_abs|year),
                         data = anes, family = binomial(link = "logit"))
defect.pid3.test <- anova(defect, defect.pid3.rc1)
tidy(defect.pid3.test)
```

The ANOVA test reports the degrees of freedom, AIC, BIC, and log-likelihood for each model. The $\chi^2$ test statistic is simply the difference in log-likelihoods, with the degrees of freedom for the test statistic derived from the difference in the overall models' degrees of freedom. Notice that by specifying a varying slope for PID intensity, we're actually adding three parameters to the model:

1. $\mu_{\beta_{1}}$
1. $\sigma^2_{\beta_{1}}$
1. The correlation $\rho$ between $\sigma^2_{\alpha}$ and $\sigma^2_{\beta_{1}}$

The p-value for the test statistic is $`r tidy(defect.pid3.test)$p.value[[2]]`$, far above the traditional threshold of $0.05$. Adding a varying slope for PID intensity does not significantly improve overall model fit, so it is unnecessary in the final model.

```{r anova-tests}
defect <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc + (1|year),
                data = anes, family = binomial(link = "logit"))
defect.xfeel <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc +
                        pid_abs * feel_def + (1|year),
                      data = anes, family = binomial(link = "logit"))
defect.xdef <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc +
                       pid_abs * defect_cor + (1|year),
                     data = anes, family = binomial(link = "logit"))
defect.xinc <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc +
                       pid_abs * inc_opp + (1|year),
                     data = anes, family = binomial(link = "logit"))
defect.cor.rc1 <- glmer(defect ~ pid_abs + feel_def + defect_cor +
                          inc + (1|year) + (defect_cor|year),
                        data = anes, family = binomial(link = "logit"))
defect.pid3.rc1 <- glmer(defect ~ pid_abs + feel_def + defect_cor +
                           inc + (1|year) + (pid_abs|year),
                         data = anes, family = binomial(link = "logit"))
defect.feel.rc1 <- glmer(defect ~ pid_abs + feel_def + defect_cor +
                           inc + (1|year) + (feel_def|year),
                         data = anes, family = binomial(link = "logit"))
defect.inc.rc1 <- glmer(defect ~ pid_abs + feel_def + defect_cor +
                          inc + (1|year) + (inc|year),
                        data = anes, family = binomial(link = "logit"))
  
# likelihood ratio tests to compare model fit
anova(defect, defect.xfeel)
anova(defect, defect.xdef)
anova(defect, defect.xinc)
anova(defect, defect.cor.rc1)
anova(defect, defect.pid3.rc1)
anova(defect, defect.feel.rc1)
anova(defect, defect.inc.rc1)
```

Sequential tests of the other covariates indicate relative favorability should vary across elections, so the final model is specified as:

$$
\begin{align}
 Pr(Y_{ei} = 1) &= \text{logit}^{-1}[\alpha_{e[i]} + \beta_{1}\text{PID Intensity}_{ei} \nonumber \\
 &\quad + \beta_{2e[i]}{\text{Relative Favorability}_{ei}} + \beta_{3}\text{Defect is Correct}_{ei}]  \nonumber \\
 \begin{pmatrix}
 \alpha_{e} \\
 \beta_{2e} \\
 \end{pmatrix} &\sim N\left(\begin{pmatrix}
 \gamma_{0}^{\alpha} + \gamma_{1}^{\alpha}\text{Incumbent} \\
 \gamma_{0}^{\beta_{2}} \\
 \end{pmatrix},\begin{pmatrix}
 \sigma^2_{\alpha} & \rho\sigma_{\alpha}\sigma_{\beta_2}  \\
 \rho\sigma_{\alpha}\sigma_{\beta_2} & \sigma^{2}_{\beta_2}  \\
 \end{pmatrix}\right) \nonumber \\
  \text{logit}^{-1}[x] &= \frac{e^x}{1+e^x}
\end{align}
$$

```{r final}
final <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc +
                 (1|year) + (feel_def|year),
               data = anes, family = binomial(link = "logit"))
tidy(final)

multiplot(basic, vary.int, final,
         title = "Regression models of partisan defection",
         newNames = c("pid_abs" = "PID intensity",
                      "feel_def" = "Relative favorability",
                      "defect_cor" = "Defect is correct",
                      "inc" = "Incumbent candidate"),
         names = c("Classical GLM", "Varying intercept", "Varying slopes"),
         decreasing = TRUE) +
  theme(legend.position = "bottom")

coef(final)
```

```{r final-sim-se}
# generate posterior simulations of beta to estimate group-specific standard errors
sim.final <- arm::sim(final, n.sims = 1000)

se.fix <- apply(sim.final@fixef, 2, function(x) sd(x, na.rm = TRUE))
se.fix <- matrix(rep(se.fix,times=10),nrow=nrow(coef(final)$year),ncol=length(se.fix),byrow=TRUE)	#expand to matrix for addition with random se

se.ran <- matrix(NA, nrow = nrow(coef(final)$year), ncol = ncol(ranef(final)$year))
se.ran[, 1] <- apply(sim.final@ranef[[1]], 2, function(x) sd(x, na.rm = TRUE))
se.ran[, 2] <- apply(sim.final@ranef[[2]][,,1], 2, function(x) sd(x, na.rm = TRUE))
se.ran[, 3] <- apply(sim.final@ranef[[2]][,,2], 2, function(x) sd(x, na.rm = TRUE))

se.ran <- cbind(se.ran[, 1] + se.ran[, 2], 0, se.ran[, 3], 0, 0)
se.final <- se.fix + se.ran
```

```{r final-by-year}
final_year <- anes %>%
  filter(year >= 1972) %>%
  group_by(year) %>%
  nest() %>%
  mutate(model = map(data, ~ glm(defect ~ pid_abs + feel_def + defect_cor + inc,
               data = .x, family = binomial(link = "logit"))),
         coef = map(model, tidy))

# get intercept and standard errors for MLM into a tidy data frame
mlm_int <- data_frame(year = rownames(coef(final)$year),
           term = "(Intercept)",
           estimate = coef(final)$year[["(Intercept)"]],
           std.error = se.final[, 1],
           method = "MLM")

# extract intercepts and standard errors
glm_int <- final_year %>%
  unnest(coef) %>%
  mutate(method = "Separate GLM",
         year = as.character(year)) %>%
  filter(term == "(Intercept)")

# join together
bind_rows(glm_int,
          mlm_int,
          tidy(basic) %>%
            mutate(year = "Overall",
                   method = "Separate GLM"),
          tidy(final) %>%
            mutate(year = "Overall",
                   method = "MLM")) %>%
  filter(term == "(Intercept)") %>%
  ggplot(aes(fct_rev(year), estimate, color = method)) +
  geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                      ymax = estimate + 1.96 * std.error),
                  position = position_dodge(.75)) +
  geom_linerange(aes(ymin = estimate - .67 * std.error,
                     ymax = estimate + .67 * std.error),
                 position = position_dodge(.75),
                 size = 1) +
  geom_hline(yintercept = 0) +
  coord_flip() +
  labs(title = "Election-specific estimates of the intercept",
       y = "Estimated coefficient with 50% and 95% CI",
       x = NULL,
       color = NULL) +
  theme(legend.position = "bottom")

# feel_def plot now
# get intercept and standard errors for MLM into a tidy data frame
mlm_feel_def <- data_frame(year = rownames(coef(final)$year),
                           term = "feel_def",
                           estimate = coef(final)$year[["feel_def"]],
                           std.error = se.final[, 2],
                           method = "MLM")

glm_feel_def <- final_year %>%
  unnest(coef) %>%
  mutate(method = "Separate GLM",
         year = as.character(year)) %>%
  filter(term == "feel_def")

bind_rows(glm_feel_def,
          mlm_feel_def,
          tidy(basic) %>%
            mutate(year = "Overall",
                   method = "Separate GLM"),
          tidy(final) %>%
            mutate(year = "Overall",
                   method = "MLM")) %>%
  filter(term == "feel_def") %>%
  ggplot(aes(fct_rev(year), estimate, color = method)) +
  geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                      ymax = estimate + 1.96 * std.error),
                  position = position_dodge(.75)) +
  geom_linerange(aes(ymin = estimate - .67 * std.error,
                     ymax = estimate + .67 * std.error),
                 position = position_dodge(.75),
                 size = 1) +
  geom_hline(yintercept = 0) +
  coord_flip() +
  labs(title = "Election-specific estimates of relative favorability",
       y = "Estimated coefficient with 50% and 95% CI",
       x = NULL,
       color = NULL) +
  theme(legend.position = "bottom")
```

# Limitations of MLM

## Likelihood maximization

Multilevel modeling is not without its drawbacks. Perhaps the most important in this application is the requirement of approximations for the likelihood. MLE for a linear multi-level model works because the portions of the likelihood function which need maximizing are (relatively) easily calculated by optimizers. Specifically, all the variance components are additive combinations of several normally distributed variables which themselves have a normal distribution. The variance is the same for all possible predicted values of $Y_{ei}$. This would also be true for a logistic regression with no multilevel component.

When you make a non-linear model multilevel, you have to add the group-level variability to the non-linear system of equations. There is a probability distribution of fitted values and a probability of a yes response goes with each fitted value (based upon the link function and probability process). The probabilities for each point along the linear predictor $\eta$ are weighted by group-level variability. Since the function is non-linear, the computation of probability for each case is terribly complex to integrate. Computing MLE is enormously more complex for generalized (nonlinear) multilevel models.

The `lme4` package uses an approximation of the likelihood through the Laplace approximation. Another approach is Restricted Estimation of Maximum Likelihood (REML). These approximations work pretty well for estimating $\gamma$s, but less so for $\sigma$s. If the researcher cares most about understanding how and why parameters vary across groups/elections, approximate likelihood approaches will not be useful. I cannot be confident that the $\hat{\sigma}$ are fully accurate, so any measures of statistical significance or associated confidence intervals could be suspect (though the decision to utilize such an interpretation remains with the researcher).

One could potentially optimize the full MLE, but this may be difficult to do and would require a significant amount of time to maximize the likelihood function if it is even possible. Alternatively we could use Bayesian estimation methods to simulate election-specific effects, but again these methods are computationally intensive and require an understanding of Bayesian approaches to inference.

## Determining which coefficients should have varying slopes

We would like to have a theoretical guide to determining which coefficients should be allowed to vary. Ideally, we could apply soft constraints to each coefficient and allow the data to tell us whether or not there is significant variance between years. The more observations one has in the data, the more parameters that can be estimated (more data $=$ more information about the likelihood the model fits the data). However even with the reduced number of parameters compared to a fixed effects approach, estimating approximate likelihoods with more than two or three varying slopes becomes extremely lengthy and laborious. We would need hundreds of thousands of observations to estimate even a half-dozen varying slopes.

# Acknowledgments {.toc-ignore}

* [Gelman, Andrew, and Jennifer Hill. *Data analysis using regression and multilevel/hierarchical models*. Cambridge university press, 2006.](http://www.stat.columbia.edu/~gelman/arm/)

# Session Info {.toc-ignore}

```{r child='_sessioninfo.Rmd'}
```




