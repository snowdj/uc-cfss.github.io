---
title: "Statistical learning: logistic regression"
author: "MACS 30100 - Perspectives on Computational Modeling"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Objectives

* Define a classification problem
* Demonstrate why linear regression is a poor method to use for classification problems
* Introduce the logistic regression functional form
* Distinguish between probability, odds, and log-odds
* Identify how to calculate predicted probabilities
* Implement multiple logistic regression and practice interpreting the parameters
* Introduce methods for assessing the accuracy of a classifier model

```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(modelr)
library(broom)
library(forcats)

set.seed(1234)

theme_set(theme_minimal())
```

# Classification problems

The sinking of [RMS Titanic](https://en.wikipedia.org/wiki/RMS_Titanic) provided the world with many things:

* A fundamental shock to the world as its faith in supposedly indestructible technology was shattered by a chunk of ice
* Perhaps the best romantic ballad of all time

    <iframe width="853" height="480" src="https://www.youtube.com/embed/FHG2oizTlpY?rel=0" frameborder="0" allowfullscreen></iframe>
    
* A tragic love story

    ![[Titanic (1997)](https://en.wikipedia.org/wiki/Titanic_(1997_film))](http://i.giphy.com/KSeT85Vtym7m.gif)
    
Why did Jack have to die? Why couldn't he have made it onto a lifeboat like Cal? We may never know the answer, but we can generalize the question a bit: why did some people survive the sinking of the Titanic while others did not?

In essence, we have a **classification** problem. The response is a **qualitative variable**, in this case a binary variable indicating whether a specific passenger survived. This differs from a **regression** problem. In a regression problem, the response variable is **quantitative** and could take on potentially an infinite range of values. In classification problems, we want to develop a model that assigns observations to **categories** or **classes** of the response variable. Given our knowledge of survivors and diers on the Titanic, if we combine this with predictors that describe each passenger we might be able to estimate a general model of survival.^[General at least as applied to the Titanic. I'd like to think technology has advanced some since the early 20th century that the same patterns do not apply today. [Not that sinking ships have gone away.](https://en.wikipedia.org/wiki/Costa_Concordia_disaster)]

Kaggle is an online platform for predictive modeling and analytics. They run regular competitions where they provide the public with a question and data, and anyone can estimate a predictive model to answer the question. They've run a popular contest based on a [dataset of passengers from the Titanic](https://www.kaggle.com/c/titanic/data). The datasets have been conveniently stored in a package called `titanic`. Let's load the package and convert the desired data frame to a tibble.^[`library(titanic)` contains both `titanic_train` and `titanic_test`. We want to use only `titanic_train` today. We'll discuss the importance of `titanic_test` next week.]

```{r titanic_data, message = FALSE}
library(titanic)
titanic <- titanic_train %>%
  as_tibble() %>%
  # remove missing values
  na.omit()

titanic %>%
  head() %>%
  knitr::kable()
```

The codebook contains the following information on the variables:

```
VARIABLE DESCRIPTIONS:
Survived        Survival
                (0 = No; 1 = Yes)
Pclass          Passenger Class
                (1 = 1st; 2 = 2nd; 3 = 3rd)
Name            Name
Sex             Sex
Age             Age
SibSp           Number of Siblings/Spouses Aboard
Parch           Number of Parents/Children Aboard
Ticket          Ticket Number
Fare            Passenger Fare
Cabin           Cabin
Embarked        Port of Embarkation
                (C = Cherbourg; Q = Queenstown; S = Southampton)

SPECIAL NOTES:
Pclass is a proxy for socio-economic status (SES)
 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower

Age is in Years; Fractional if Age less than One (1)
 If the Age is Estimated, it is in the form xx.5

With respect to the family relation variables (i.e. sibsp and parch)
some relations were ignored.  The following are the definitions used
for sibsp and parch.

Sibling:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic
Spouse:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)
Parent:   Mother or Father of Passenger Aboard Titanic
Child:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic

Other family relatives excluded from this study include cousins,
nephews/nieces, aunts/uncles, and in-laws.  Some children travelled
only with a nanny, therefore parch=0 for them.  As well, some
travelled with very close friends or neighbors in a village, however,
the definitions do not support such relations.
```

So if this is our data, `Survived` is our **response variable**, and the remaining variables are **predictors**, how can we determine who survives and who dies?

## A linear regression approach

Let's concentrate first on the relationship between age and survival. Using the methods we previously learned, we could estimate a linear regression model:

$$Y = \beta_0 + \beta_{1}X$$

```{r titanic_ols}
ggplot(titanic, aes(Age, Survived)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Age",
       y = "???")
```

Hmm. Not terrible, but you can immediately notice a couple of things. First, the only possible values for `Survival` are $0$ and $1$. Yet the linear regression model gives us predicted values such as $.4$ and $.25$. How should we interpret those?

One possibility is that these values are **predicted probabilities**. That is, the estimated probability a passenger will survive given their age. So someone with a predicted probability of $.4$ has a 40% chance of surviving. Okay, but notice that because the line is linear and continuous, it extends infinitely in both directions of age.

```{r titanic_ols_old}
ggplot(titanic, aes(Age, Survived)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  xlim(0, 200) +
  labs(x = "Age",
       y = "???")
```

What happens if a 200 year old person is on the Titanic?^[Given Rose's condition in the present-day of the film, it's not outside the realm of possibility.] They would have a $-.1$ probability of surviving. **But you cannot have a probability outside of the $[0,1]$ interval!** Admittedly this is a trivial example, but in other circumstances this can become a more realistic scenario.

Or what if we didn't want to predict survival, but instead predict the port from which an individual departed (Cherbourg, Queenstown, or Southampton). We could try and code this as a numeric response variable:

Numeric value | Port
--|---
1 | Cherbourg
2 | Queenstown
3 | Southampton

But why not instead code it:

Numeric value | Port
--|---
1 | Queenstown
2 | Cherbourg
3 | Southampton

Or even:

Numeric value | Port
--|---
1 | Southampton
2 | Cherbourg
3 | Queenstown

**There is no inherent ordering to this variable.** Any claimed linear relationship between a predictor and port of embarkation is completely dependent on how we convert the classes to numeric values.

# Logistic regression

## Probability

Rather than modeling the response $Y$ directly, logistic regression instead models the **probability** that $Y$ belongs to a particular category. In our first Titanic example, the probability of survival can be written as:

$$p(X) = p(\text{survival} = \text{yes} | \text{age})$$

The linear regression model above attempted to predict or explain $p(X)$ using the functional form

$$p(X) = \beta_0 + \beta_{1}X$$

```{r linear-demo}
# simulate fake binary data
sim_logit <- data_frame(x = runif(1000, -5, 5),
                        y = 0 + 1 * x)

# graph it
ggplot(sim_logit, aes(x, y)) +
  geom_line() +
  labs(title = "The linear function",
       x = "X",
       y = "p(X)")
```

But we now know that by using this functional form, we will always have for at least some subset of $\text{age}$ predicted values outside the $[0,1]$ range. Instead, we need to model the functional form to prevent values from exceeding this range. One such function is the **logistic function**:[^logit]

$$p(X) = \frac{e^{\beta_0 + \beta_{1}X}}{1 + e^{\beta_0 + \beta_{1}X}}$$

```{r logit}
logit2prob <- function(x){
  exp(x) / (1 + exp(x))
}
```

```{r logit-demo}
sim_logit <- sim_logit %>%
  mutate(prob = logit2prob(y))

# graph it
ggplot(sim_logit, aes(x, prob)) +
  geom_line() +
  labs(title = "The logistic function",
       x = "X",
       y = "Prob(X)")
```

The logistic transformation produces an S-shaped curve that preserves ordering; that is, larger values of $X$ will also result in larger values of $p(X)$, but the relationship is now **curvilinear**. $P(X)$ will never decrease below 0 and never exceed 1, so the predicted probabilities will always have inherent meaning.

### Probability of surviving the Titanic

$$p(\text{Survival}) = \frac{e^{\beta_0 + \beta_{1}\text{Age}}}{1 + e^{\beta_0 + \beta_{1}\text{Age}}}$$

The values of $P(\text{survival} = \text{Yes} | \text{age})$ (or simply $p(\text{survival})$ will range between 0 and 1. Given that predicted probability, we could predict anyone with for whom $p(\text{survival}) > .5$ will survive the sinking, and anyone else will die.^[The threshold can be adjusted depending on how conservative or risky of a prediction you wish to make.]

We can estimate the logistic regression model using the `glm()` function.

```{r titanic_age_glm}
survive_age <- glm(Survived ~ Age, data = titanic, family = binomial)
summary(survive_age)
```

Which produces a line that looks like this:

```{r titanic_age_glm_plot}
# generate predicted values
survive_age_pred <- titanic %>%
  add_predictions(survive_age) %>%
  # predicted values are in the log-odds form - convert to probabilities
  mutate(prob = logit2prob(pred))

ggplot(survive_age_pred, aes(Age)) +
  geom_point(aes(y = Survived)) +
  geom_line(aes(y = prob), color = "blue", size = 1) +
  labs(x = "Age",
       y = "Probability of surviving the Titanic")
```

It's hard to tell, but the line is not perfectly linear. Let's expand the range of the x-axis to prove this:

```{r titanic_age_glm_plot_wide}
ggplot(titanic, aes(Age, Survived)) +
  geom_point() +
  # use geom_smooth for out-of-sample range plotting
  geom_smooth(method = "glm", method.args = list(family = "binomial"),
              se = FALSE, fullrange = TRUE, color = "blue", size = 1) +
  xlim(0,200) +
  labs(x = "Age",
       y = "Probability of surviving the Titanic")
```

No more predictions that a 200 year old has a $-.1$ probability of surviving!

### Generating predicted probabilities

To generate predicted probabilities, just substitute your specified values for $X$ in the original $p(X)$ equation. So for instance, the predicted probability of surviving the Titanic for a 30 year old is

$$p(\text{Survival}) = \frac{e^{\beta_0 + \beta_{1} \times 30}}{1 + e^{\beta_0 + \beta_{1} \times 30}}$$

$$p(\text{Survival}) = \frac{e^{`r coef(survive_age)[[1]]` + `r coef(survive_age)[[2]]` \times 30}}{1 + e^{`r coef(survive_age)[[1]]` + `r coef(survive_age)[[2]]` \times 30}}$$

```{r prob-30, include = FALSE}
prob30 <- predict(survive_age, newdata = data_frame(Age = 30), type = "response")
```

$$p(\text{Survival}) = `r formatC(prob30[[1]], digits = 3)`$$

## Odds

We can rewrite the probability function in the alternative form

$$\frac{p(X)}{1 - p(x)} = e^{\beta_0 + \beta_{1}X}$$

This quantity is called the **odds** and its range is $[0,\infty]$.

```{r odds}
prob2odds <- function(x){
  x / (1 - x)
}
```

```{r odds-demo}
sim_logit <- sim_logit %>%
  mutate(odds = prob2odds(prob))

# graph it
ggplot(sim_logit, aes(x, odds)) +
  geom_line() +
  labs(title = "The logistic function",
       x = "X",
       y = "Odds(X)")
```

Essentially it is the probability of one outcome divided by the probability of the other outcome. So in this example, it is

$$\frac{p(\text{Survived})}{p(\text{Died})}$$

So an odds of $4$ for an individual means they are 4 times more likely to survive than die, whereas an odds of $\frac{1}{4}$ means the odds of surviving are $.25$ as large as the odds of dying (i.e. the odds are higher you will die than survive).

Odds are typically used in the context of gambling since they are easier to follow when establishing a betting strategy. For example, consider an American football game between the Cleveland Browns and the Pittsburgh Steelers. If the odds are 1 in 4 ($\frac{1}{4}$) of the Pittsburgh Steelers winning a game against the Cleveland Browns and you believe the Steelers will win, this means you must bet \$4 in order to win a payoff of \$1 if the Steelers win.^[Given [recent history](http://www.footballdb.com/teams/nfl/pittsburgh-steelers/teamvsteam?opp=8), this will probably occur.] However if you are a die-hard Browns fan and want to bet on them winning, the odds of the Browns winning are 4 to 1 ($\frac{4}{1}$) - it means you must bet \$1 to earn a payoff of \$4 if the Browns in fact win.

### Odds of surviving the Titanic

We can convert the probability of surviving the Titanic to the odds of surviving the Titanic.

$$\frac{p(\text{Survival})}{1 - p(\text{Survival})} = e^{\beta_0 + \beta_{1}\text{Age}}$$

The resulting graph is:

```{r titanic-odds-plot}
survive_age_pred <- survive_age_pred %>%
  mutate(odds = prob2odds(prob))

ggplot(survive_age_pred, aes(Age, odds)) +
  geom_line(color = "blue", size = 1) +
  labs(x = "Age",
       y = "Odds of surviving the Titanic")
```

Regardless of age, the odds of surviving the Titanic are always below 1. Considering the probability of even a 1 year old surviving was less than $.50$, this should be expected. The relationship between age and the odds of survival is still curvilinear.

## Log-odds

By taking the logarithm of both sides of the odds function, we get

$$\log\left(\frac{p(X)}{1 - p(x)}\right) = \beta_0 + \beta_{1}X$$

The result is the **log-odds** or **logit**. Notice that this function now imposes a linear relationship between $X$ and $p(X)$.

```{r log-odds}
prob2logodds <- function(x){
  log(prob2odds(x))
}
```

```{r log-odds-demo}
sim_logit <- sim_logit %>%
  mutate(logodds = prob2logodds(prob))

# graph it
ggplot(sim_logit, aes(x, logodds)) +
  geom_line() +
  labs(title = "The logistic function",
       x = "X",
       y = "Log-Odds(X)")
```

A one-unit increase in $X$ corresponds to a monotonic increase or decrease in the log-odds of $Y$. But because of the exponential transformation between log-odds and odds, **a one-unit increase in $X$ corresponds to a differential increase or decrease in the odds of $Y$**.  The amount of the change in the odds of $Y$ depends on the initial value of $X$. The directionality of the relationship is always the same, but not the magnitude. The same thing applies to the relationship between $X$ and the probability of $Y$. This explains why we see curvilinear shapes for probabilities and odds, but not for log-odds.

### Log-odds of surviving the Titanic

When you estimate a logistic regression model the log-odds function is actually the function for which you are estimating parameters.

$$\log\left(\frac{p(\text{Survival})}{1 - p(\text{Survival})}\right) = \beta_0 + \beta_{1}\text{Age}$$

Hence the parameters for the Titanic survival model

```{r survive-age-param}
tidy(survive_age)
```

are actually expressed in terms of log-odds -- **for every one-unit increase in age, we expect the log-odds of survival to decrease by `r formatC(abs(coef(survive_age)[[2]]), digits = 3)`**.

$$\log\left(\frac{p(\text{Survival})}{1 - p(\text{Survival})}\right) = `r coef(survive_age)[[1]]` + `r coef(survive_age)[[2]]` x \text{Age}$$

Generating predicted values from logistic regression models using `add_predictions()` always returns the log-odds of the outcome, so we can plot that directly to see the relationship between age and the log-odds of survival.

```{r log-odds-plot}
ggplot(survive_age_pred, aes(Age, pred)) +
  geom_line(color = "blue", size = 1) +
  labs(x = "Age",
       y = "Log-odds of surviving the Titanic")
```

While we can graph the log-odds, they don't make any inherent sense. Nobody discusses the likelihood of events occuring in log-odds term. They are necessary to understanding and estimating logistic regression models, but are not directly interpretable.

#### Discussing the results in terms of probabilities

To interpret them in terms of probabilities, you need to specify your initial value for age. So for example, an increase in age from 20 to 30 is calculated by taking the **first difference**, or the expected change in probability given a particular change in the predictor. While it looks messy, all you do is calculate the predicted probability of survival for age 20 and 30 and calculate the difference.

$$p(\text{Survival}_{30 - 20}) = \frac{e^{\beta_0 + \beta_{1}30}}{1 + e^{\beta_0 + \beta_{1}30}} - \frac{e^{\beta_0 + \beta_{1}20}}{1 + e^{\beta_0 + \beta_{1}20}}$$

$$p(\text{Survival}_{30 - 20}) = \frac{e^{`r coef(survive_age)[[1]]` + `r coef(survive_age)[[2]]` \times 30}}{1 + e^{`r coef(survive_age)[[1]]` + `r coef(survive_age)[[2]]` \times 30}} - \frac{e^{`r coef(survive_age)[[1]]` + `r coef(survive_age)[[2]]` \times 20}}{1 + e^{`r coef(survive_age)[[1]]` + `r coef(survive_age)[[2]]` \times 20}}$$

```{r first-diff-2030, include = FALSE}
pred_fd_2030 <- predict(survive_age, newdata = data_frame(Age = c(20, 30)), type = "response")
```

$$p(\text{Survival}_{30 - 20}) = `r pred_fd_2030[[2]]` - `r pred_fd_2030[[1]]`$$

$$p(\text{Survival}_{30 - 20}) = `r formatC(pred_fd_2030[[2]] - pred_fd_2030[[1]], digits = 3)`$$

However, compare this to the change in predicted probability of survival from age 40 to 50.

$$p(\text{Survival}_{50 - 40}) = \frac{e^{\beta_0 + \beta_{1}50}}{1 + e^{\beta_0 + \beta_{1}50}} - \frac{e^{\beta_0 + \beta_{1}40}}{1 + e^{\beta_0 + \beta_{1}40}}$$

$$p(\text{Survival}_{50 - 40}) = \frac{e^{`r coef(survive_age)[[1]]` + `r coef(survive_age)[[2]]` \times 50}}{1 + e^{`r coef(survive_age)[[1]]` + `r coef(survive_age)[[2]]` \times 50}} - \frac{e^{`r coef(survive_age)[[1]]` + `r coef(survive_age)[[2]]` \times 40}}{1 + e^{`r coef(survive_age)[[1]]` + `r coef(survive_age)[[2]]` \times 40}}$$

```{r first-diff-4050, include = FALSE}
pred_fd_4050 <- predict(survive_age, newdata = data_frame(Age = c(40, 50)), type = "response")
```

$$p(\text{Survival}_{50 - 40}) = `r pred_fd_4050[[2]]` - `r pred_fd_4050[[1]]`$$

$$p(\text{Survival}_{50 - 40}) = `r formatC(pred_fd_4050[[2]] - pred_fd_4050[[1]], digits = 3)`$$

Again, the relationship between age and the probability of survival is only weakly curvilinear, but you can already see that the first difference will depend on your initial starting value for age.

## Estimating the parameters

Logistic regression is typically estimated using [**maximum likelihood estimation**](https://github.com/UC-MACSS/persp-model/blob/master/demos/MLE/MLest.ipynb).

## Generating predicted probabilities using `add_predictions()`

To visualise the predictions from a model, we start by generating an evenly spaced grid of values that covers the region where our data lies. First we use `data_grid()` from the `modelr` package to create a cleaned data frame of potential values:

```{r make_age_pred}
titanic_age <- titanic %>%
  data_grid(Age)
titanic_age
```

Next we use the `add_predictions()` function to produce the predicted probabilities. This worked very well for linear models; unfortunately it is not perfect for logistic regression because as you I mentioned above, logistic regression directly estimates the [**log-odds**](https://wiki.lesswrong.com/wiki/Log_odds) for the outcome. Instead, we want the plain old predicted probability. To do this, we use this custom function to convert from log-odds to predicted probabilties:[^augment]

```{r logit2, ref.label="logit", eval = FALSE}
logit2prob <- function(x){
  exp(x) / (1 + exp(x))
}
```

```{r extract_modelr, dependson="make_age_pred"}
library(modelr)

titanic_age <- titanic_age %>%
  add_predictions(survive_age) %>%
  mutate(pred = logit2prob(pred))
titanic_age
```

With this information, we can now plot the logistic regression line using the estimated model:

```{r plot_pred, dependson="make_age_pred"}
ggplot(titanic_age, aes(Age, pred)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1)) +
  labs(title = "Relationship between age and surviving the Titanic",
       y = "Predicted probability of survival")
```

## Multiple predictors

But as the old principle of the sea goes, ["women and children first"](https://en.wikipedia.org/wiki/Women_and_children_first). What if age isn't the only factor effecting survival? Fortunately logistic regression handles multiple predictors in the form:

$$p(X) = \frac{e^{\beta_0 + \beta_{1}X_1 + \dots + \beta_{p}X_{p}}}{1 + e^{\beta_0 + \beta_{1}X_1 + \dots + \beta_{p}X_{p}}}$$

In our next model, let's consider the relationship between age, sex, and survival. The logistic function takes the form:

$$p(\text{Survival}) = \frac{e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Sex}}}{1 + e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Sex}}}$$

```{r survive_age_woman}
survive_age_woman <- glm(Survived ~ Age + Sex, data = titanic,
                         family = binomial)
summary(survive_age_woman)
```

The parameters essentially tell us the relationship between each individual predictor and the response, **independent of other predictors**. So this model tells us the relationship between age and survival, after controlling for the effects of gender. Likewise, it also tells us the relationship between gender and survival, after controlling for the effects of age. To get a better visualization of this, let's use `data_grid()` and `add_predictions()` again:

```{r survive_age_woman_pred}
titanic_age_sex <- titanic %>%
  data_grid(Age, Sex) %>%
  add_predictions(survive_age_woman) %>%
  mutate(pred = logit2prob(pred))
titanic_age_sex
```

With these predicted probabilities, we can now plot the separate effects of age and gender:

```{r survive_age_woman_plot, dependson="survive_age_woman"}
ggplot(titanic_age_sex, aes(Age, pred, color = Sex)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1)) +
  labs(title = "Probability of surviving the Titanic",
       subtitle = "Age + Sex",
       y = "Predicted probability of survival",
       color = "Sex")
```

Substantively, this graph illustrates a key fact about surviving the sinking of the Titanic - age was not really a dominant factor. Instead, one's gender was much more important. Females survived at much higher rates than males, regardless of age.

## Predicted probabilities and first differences in multiple variable models

Mathematically, the result is two lines that are parallel to each other **in the log-odds functional form**. The addition of the gender parameter acts like an second intercept when we compare age to probability of survival. All that happens is the line shifts up or down based on the estimated gender parameter.

In terms of predicted probabilities and first differences, **this is not true**. Remember the logistic function:

$$p(X) = \frac{e^{\beta_0 + \beta_{1}X_1 + \beta_{2}X_2}}{1 + e^{\beta_0 + \beta_{1}X_1 + \beta_{2}X_2}}$$

Because of the exponential transformation, $\beta_2$ now has a non-linear impact on the predicted probabilities. For clarity, let's estimate a logistic regression model of the effect of age and fare on survival:

```{r fd-non-parallel}
# estimate logistic regression model of age and fare
age_fare <- glm(Survived ~ Age + Fare, data = titanic, family = binomial)
tidy(age_fare)
```

And generate predicted log-odds and probabilities of survival for a series of hypothetical passengers, varying the fare amount paid.

```{r fd-non-parallel-pred}
# generate predicted values
age_fare_pred <- titanic %>%
  data_grid(Age, Fare = seq(0, max(Fare), by = 100)) %>%
  add_predictions(age_fare) %>%
  mutate(prob = logit2prob(pred),
         Fare = factor(Fare, levels = rev(seq(0, max(Fare), by = 100))))
```

Below is the plot of the log-odds for this regression model. To draw the line, all we did was draw the line for the relationship between age and expected log-odds of survival, holding fare constant at different levels.

$$\log\left(\frac{p(\text{Survival})}{1 - p(\text{Survival})}\right) = \beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Fare}$$

$$\log\left(\frac{p(\text{Survival})}{1 - p(\text{Survival})}\right) = (\beta_0 + \beta_{2}\text{Fare}) + \beta_{1}\text{Age}$$

Notice that each line is parallel to each other, because all that $\hat{\beta}_{2}$ did was change the intercept for the controlled relationship between age and log-odds of survival.

```{r fd-non-parallel-logodds}
# plot the new log-odds lines
ggplot(age_fare_pred, aes(Age, pred, group = Fare, color = Fare)) +
  geom_line() +
  labs(title = "Log-odds of survival",
       subtitle = "Age + Fare model",
       x = "Age",
       y = "Log-odds of survival")
```

Now see what happens when we do the same thing but using predicted probabilities instead.

$$p(\text{Survival}) = \frac{e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Fare}}}{1 + e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Fare}}}$$

$$p(\text{Survival}) = \frac{e^{(\beta_0 + \beta_{2}\text{Fare}) + \beta_{1}\text{Age}}}{1 + e^{(\beta_0 + \beta_{2}\text{Fare}) + \beta_{1}\text{Age}}}$$

Because of the exponential transformation, even if we hold fare constant, **the resulting probability curve will non-linearly change for different fare values**.

```{r fd-non-parallel-prob}
# plot the new probability lines
ggplot(age_fare_pred, aes(Age, prob, group = Fare, color = Fare)) +
  geom_line() +
  labs(title = "Predicted probability of survival",
       subtitle = "Age + Fare model",
       x = "Age",
       y = "Predicted probability of survival")
```

So when calculating first differences and predicted probabilities for a single variable in a multiple variable logistic regression models, **the values at which you hold the other variables constant will directly influence your results**. The general rule of thumb is to hold the other variables constant at their **median** (if continuous) or **modal** (if discrete) values.

## Interactive terms

The additive assumption of linear regression also holds true for logistic regression: the relationships between predictors and responses are independent from one another. So for the age and gender example, we assume our function $f$ is:

$$p(\text{Survival}) = \frac{e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Sex}}}{1 + e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Sex}}}$$

However once again, that is an assumption. What if the relationship between age and the probability of survival is actually dependent on whether or not the individual is a female? That is, the parameter for age is different for men and women? This possibility would take the functional form:

$$p(\text{Survival}) = \frac{e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Sex} + \beta_{3} \times \text{Age} \times \text{Sex}}}{1 + e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Sex} + \beta_{3} \times \text{Age} \times \text{Sex}}}$$

This is considered an **interaction** between age and gender. To estimate this in R, we simply specify `Age * Sex` in our formula for the `glm()` function:^[R automatically includes constituent terms, so this turns into `Age + Sex + Age * Sex`. [Generally you always want to include constituent terms in a regression model with an interaction.](https://pan-oxfordjournals-org.proxy.uchicago.edu/content/14/1/63.full.pdf+html)]

```{r age_woman_cross}
survive_age_woman_x <- glm(Survived ~ Age * Sex, data = titanic,
                           family = binomial)
summary(survive_age_woman_x)
```

How can we interpret these parameters? Because the gender variable is binary (0 if female, 1 if male), it acts as a switch on the regression equation. So for women, the relationship between age and probability of survival is:^[$\beta_{2}\text{sex}$ can be factored out since it does not contain age as a constituent term.]

$$p(\text{Survival}_{female}) = \frac{e^{\beta_0 + \beta_{1}\text{Age} + \beta_{3} \times \text{Age} \times 0}}{1 + e^{\beta_0 + \beta_{1}\text{Age} + \beta_{3} \times \text{Age} \times 0}}$$

which reduces to

$$p(\text{Survival}_{female}) = \frac{e^{\beta_0 + \beta_{1}\text{Age}}}{1 + e^{\beta_0 + \beta_{1}\text{Age}}}$$

However for men, the resulting equation looks different:

$$p(\text{Survival}_{male}) = \frac{e^{\beta_0 + \beta_{1}\text{Age} + \beta_{3} \times \text{Age} \times 1}}{1 + e^{\beta_0 + \beta_{1}\text{Age} + \beta_{3} \times \text{Age} \times 1}}$$

which reduces to

$$p(\text{Survival}_{male}) = \frac{e^{\beta_0 + \beta_{1}\text{Age} + \beta_{3} \times \text{Age}}}{1 + e^{\beta_0 + \beta_{1}\text{Age} + \beta_{3} \times \text{Age}}}$$

$$p(\text{Survival}_{male}) = \frac{e^{\beta_0 + (\beta_{1} + \beta_{3})\text{Age}}}{1 + e^{\beta_0 + (\beta_{1} + \beta_{3})\text{Age}}}$$

Let's plot the interactive effects of age and gender by estimating the log-odds and predicted probability of survival.

```{r age_woman_cross_pred, dependson="age_woman_cross"}
titanic_age_sex_x <- titanic %>%
  data_grid(Age, Sex) %>%
  add_predictions(survive_age_woman_x) %>%
  mutate(prob = logit2prob(pred))
titanic_age_sex_x
```

```{r age_woman_plot_logodds, dependson="age_woman_cross"}
ggplot(titanic_age_sex_x, aes(Age, pred, color = Sex)) +
  geom_line() +
  labs(title = "Log-odds of surviving the Titanic",
       x = "Age",
       y = "Log-odds of survival",
       color = "Sex")
```

For the log-odds (i.e. based directly on the parameter values), we can see that the interactive term changes the **slope** of the line for age. For women the slope of the line is positive (`r formatC(coef(survive_age_woman_x)[[1]], digits = 3)`), whereas for men the slope is negative (`r formatC(coef(survive_age_woman_x)[[1]] + coef(survive_age_woman_x)[[3]], digits = 3)`).

```{r age_woman_plot_prob, dependson="age_woman_cross"}
# join data frames of interactive and non-interactive model
bind_rows(list("Age + Sex" = titanic_age_sex %>%
                 rename(prob = pred),
               "Age x Sex" = titanic_age_sex_x), .id = "id") %>%
  # plot the two models
  ggplot(aes(Age, prob, color = Sex, linetype = id)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 1)) +
  labs(title = "Probability of surviving the Titanic",
       x = "Age",
       y = "Predicted probability of survival",
       color = "Sex",
       linetype = "Model")
```

And now our minds are blown once again! For women, as age increases the probability of survival also increases. However for men, we see the opposite relationship: as age increases the probability of survival **decreases**. Again, the basic principle of saving women and children first can be seen empirically in the estimated probability of survival. Male children are treated similarly to female children, and their survival is prioritized. Even still, the probability of survival is always worse for men than women, but the regression function clearly shows a difference from our previous results.

You may think then that it makes sense to throw in interaction terms (and potentially quadratic terms) willy-nilly to all your regression models since we never know for sure if the relationship is strictly linear and independent. You could do that, but once you start adding more predictors (3, 4, 5, etc.) that will get very difficult to keep track of (five-way interactions are extremely difficult to interpret - even three-way get to be problematic). The best advice is to use theory and your domain knowledge as your guide. Do you have a reason to believe the relationship should be interactive? If so, test for it. If not, don't.

# Evaluating model accuracy

We need a method to evaluate the overall accuracy of a model. This allows us to determine if we have a good or bad model (at least based on predictive power), and compare alternative model specifications to each other. For classification methods such as logistic regression, there are a few different metrics we can use:

* Accuracy/error rate
* Proportional reduction in error
* Receiver operating characteristics (ROC) curve and area under the curve (AUC)

## Accuracy of predictions

One evalation criteria simply asks: how accurate are the predictions? For instance, how often did our basic model just using age correctly predict who survived and died? First we need to get the predicted probabilities for each individual in the original dataset and convert the probability to a prediction - that is, classify the individual as either a **survivor** or **dead**. The standard threshold for binary classification problems is $.5$; any individual with a predicted probability greater than or equal to $.5$ would be classified as a 1 (or survivor), and any predicted probability less than $.5$ would be classified as a 0 (or dead).^[We will discuss later on the effect of different threshold values.] Once we make our predictions, then we calculate what percentage of predictions were correct.

```{r accuracy_age, dependson="titanic_age_glm"}
age_accuracy <- titanic %>%
  add_predictions(survive_age) %>%
  mutate(pred = logit2prob(pred),
         pred = as.numeric(pred > .5))

mean(age_accuracy$Survived == age_accuracy$pred, na.rm = TRUE)
```

$`r formatC(mean(age_accuracy$Survived == age_accuracy$pred, na.rm = TRUE) * 100, digits = 3)`\%$ of the predictions based on age only were correct. Is this good or bad? Well, that depends on what you consider your **baseline**. It's certainly better than an accuracy rate of $0\%$, but is that what we should compare it to?

```{r mode}
# create a function to calculate the modal value of a vector
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
```

Instead we should compare the model's accuracy rate to a simple but useless classifier that says predicts **no individuals will survive the sinking of the Titanic**. Because death is the [modal](https://en.wikipedia.org/wiki/Mode_(statistics)) category, if we predict that every single person will die we would achieve an accuracy rate of $`r formatC(mean(age_accuracy$Survived == getmode(age_accuracy$Survived), na.rm = TRUE) * 100, digits = 3)`\%$. Which coincidentially is exactly the accuracy rate for our age-only model. What gives?

```{r plot-pred2, ref.label="plot_pred"}
```

Oh yeah. If you look at the graph, the predicted probability curve for age is always below $.5$ for the potential age range, so the model actually does act like a modal model - it always predicts death. So our fancy logistic regression model does no better than the useless classifier. What about our interactive age and gender model?

```{r accuracy_age_gender_x, dependson="age_woman_cross"}
x_accuracy <- titanic %>%
  add_predictions(survive_age_woman_x) %>%
  mutate(pred = logit2prob(pred),
         prob = pred,
         pred = as.numeric(pred > .5))

mean(x_accuracy$Survived == x_accuracy$pred, na.rm = TRUE)
```

This model is much better. Just by knowing an individual's age and gender, we can predict with `r formatC(mean(x_accuracy$Survived == x_accuracy$pred, na.rm = TRUE) * 100, digits = 3)`% whether he/she lives or dies.

## Proportional reduction in error

**Proportional reduction in error** (PRE) is a formalized way of comparing a model's accuracy rate to the baseline. It is defined by

$$PRE = \frac{E_1 - E_2}{E_1}$$

where $E_1$ is the number of prediction errors in the null model (i.e. useless classifier) and $E_2$ is the number of prediction errors in the statistical learning model. It will range in value between $[0,1]$ or $[0\%,100\%]$. $0\%$ means the statistical model reduced none of the prediction error, and $100\%$ means the statistical model eliminated all of the prediction error.

```{r pre}
# function to calculate PRE for a logistic regression model
PRE <- function(model){
  # get the actual values for y from the data
  y <- model$y
  
  # get the predicted values for y from the model
  y.hat <- round(model$fitted.values)
  
  # calculate the errors for the null model and your model
  E1 <- sum(y != median(y))
  E2 <- sum(y != y.hat)
  
  # calculate the proportional reduction in error
  PRE <- (E1 - E2) / E1
  return(PRE)
}
```

So for the age-only model, the PRE is

$$PRE_{\text{Age}} = \frac{`r sum(survive_age$y != median(survive_age$y))` - `r sum(survive_age$y != round(survive_age$fitted.values))`}{`r sum(survive_age$y != median(survive_age$y))`}$$

$$PRE_{\text{Age}} = \frac{`r sum(survive_age$y != median(survive_age$y)) - sum(survive_age$y != round(survive_age$fitted.values))`}{`r sum(survive_age$y != median(survive_age$y))`}$$

$$PRE_{\text{Age}} = `r (sum(survive_age$y != median(survive_age$y)) - sum(survive_age$y != round(survive_age$fitted.values))) / sum(survive_age$y != median(survive_age$y))`\%$$

For the interactive age and gender model, the PRE is

$$PRE_{\text{Age x Gender}} = \frac{`r sum(survive_age_woman_x$y != median(survive_age_woman_x$y))` - `r sum(survive_age_woman_x$y != round(survive_age_woman_x$fitted.values))`}{`r sum(survive_age_woman_x$y != median(survive_age_woman_x$y))`}$$

$$PRE_{\text{Age x Gender}} = \frac{`r sum(survive_age_woman_x$y != median(survive_age_woman_x$y)) - sum(survive_age_woman_x$y != round(survive_age_woman_x$fitted.values))`}{`r sum(survive_age_woman_x$y != median(survive_age_woman_x$y))`}$$

$$PRE_{\text{Age x Gender}} = `r formatC((sum(survive_age_woman_x$y != median(survive_age_woman_x$y)) - sum(survive_age_woman_x$y != round(survive_age_woman_x$fitted.values))) / sum(survive_age_woman_x$y != median(survive_age_woman_x$y)) * 100, digits = 3)`\%$$

We've reduced the error from the useless classifier by $`r formatC(PRE(survive_age_woman_x) * 100, digits = 3)`\%$

## Receiver operating characteristics (ROC) curve

### Types of error

Binary classifiers such as logistic regression make two types of error.

![](http://marginalrevolution.com/wp-content/uploads/2014/05/Type-I-and-II-errors1-625x468.jpg)

Personally I hate the terms type I and type II because I never remember which is which. The above diagram may help, but instead let's adopt the terms **false positive** and **false negative**.

In the context of the Titanic model, we could make two mistakes:

1. False positive - predict an individual survived the Titanic, when in fact the individual died.
1. False negative - predict an individual died on the Titanic, when in fact the individual survived.

### Confusion matrix

Previously all we cared about was the overall accuracy rate of the model, but we may also care about the accuracy or error rate for both types of errors. To do that, we generate a **confusion matrix** using the `confusionMatrix()` function from the `caret` library.

```{r threshold-5, cache = FALSE}
library(caret)  # load the caret package to use the confusionMatrix function
cm_5 <- confusionMatrix(x_accuracy$pred, x_accuracy$Survived)
cm_5
```

The rows define the predicted outcomes and the columns define the actual (or reference) outcomes for individuals in the dataset. Ideally we want the off-diagonal cells to be 0 because we perfectly predicted all observations. However we can see that isn't always the case with this model.

### Alternative thresholds

Perhaps instead of boosting overall model accuracy, we instead care more about improving class-specific accuracy. For example, if we want to increase **sensitivity/recall** we need to increase the true positive rate (TPR):

$$TPR = \frac{\text{Number of actual positives correctly predicted}}{\text{Number of actual positives}}$$

However if we are concerned with increasing **specificity** we need to increase the true negative rate (TNR):

$$TNR = \frac{\text{Number of actual negatives correctly predicted}}{\text{Number of actual negatives}}$$

While the only way to achieve higher sensitivity **and** specificity is to estimate a better model, we can use our existing model to meet one of these goals. We do this by adjusting our **threshold**, or the cut-off point for classifying individuals as survivors or dead. The confusion matrix above also calculates the sensitivity and specificity of the predictive model. For the original threshold of $.5$, our true positive rate (or sensitivity) was `r formatC(cm_5$byClass[["Sensitivity"]], digits = 3)`. What about for a threshold of $.8$?

```{r threshold-8}
threshold_8 <- titanic %>%
  add_predictions(survive_age_woman_x) %>%
  mutate(pred = logit2prob(pred),
         pred = as.numeric(pred > .8))

cm_8 <- confusionMatrix(threshold_8$pred, threshold_8$Survived)
cm_8
```

With a threshold of $.8$, sensitivity increases to `r formatC(cm_8$byClass[["Sensitivity"]], digits = 3)`, but notice the trade-off: overall accuracy has decreased from `r formatC(cm_5$overall[["Accuracy"]], digits = 3)` to `r formatC(cm_8$overall[["Accuracy"]], digits = 3)`, and specificity has also declined substantially. This reveals the trade-off we have to make between sensitivity and specificity.

```{r threshold-compare}
# function to calculate key stats for the Titanic data
threshold_compare <- function(threshold, data, model){
  # generate predictions
  pred <- data %>%
    add_predictions(model) %>%
    mutate(pred = logit2prob(pred),
           pred = as.numeric(pred > threshold))
  
  # get confusion matrix
  cm <- confusionMatrix(pred$pred, pred$Survived)
  
  # extract sensitivity and threshold to data_frame
  data_frame(threshold = threshold,
             sensitivity = cm$byClass[["Sensitivity"]],
             specificity = cm$byClass[["Specificity"]],
             accuracy = cm$overall[["Accuracy"]])
}

threshold_x <- seq(0, 1, by = .001) %>%
  map_df(threshold_compare, titanic, survive_age_woman_x) 

threshold_x %>%
  gather(measure, value, -threshold) %>%
  mutate(measure = factor(measure, labels = c("Accuracy", "Sensitivity", "Specificity"))) %>%
  ggplot(aes(threshold, value, color = measure, linetype = measure)) +
  geom_line() +
  labs(x = "Threshold",
       y = "Accuracy rate",
       color = "Measure",
       linetype = "Measure")
```

By graphing across many alternative thresholds, we can select a threshold level at the trade-off for which we are comfortable based on a visual inspection. But is that really the best approach? In order to make that determination we must have **domain knowledge** about about the benefits and costs to making that decision.

### ROC curve

A receiver operating characteristic (ROC) curve is an alternative graphical method for comparing the types of errors for differing thresholds. In this graph, you plot the false positive rate vs. the true positive rate (i.e. 1- specificity vs. sensitivity).

```{r roc-ggplot}
ggplot(threshold_x, aes(1 - specificity, sensitivity)) +
  geom_line() +
  geom_abline(slope = 1, linetype = 2, color = "grey")
```

The overall performance of the classifier across all potential thresholds is the **area under the (ROC) curve** (AUC). The ideal AUC curve hugs the top left corner of the graph, so a larger AUC indicates a better classifier. An AUC of $1$ means perfect prediction for any threshold value. The dashed line represents the null model where we randomly guess whether individuals are survivors or dead (i.e. flipped a coin) and would have an AUC of $.5$.

```{r roc-auc, cache = FALSE}
library(pROC)
roc_x <- roc(x_accuracy$Survived, x_accuracy$prob)
plot(roc_x)   # use pROC to draw the ROC curve

# calculate the AUC
auc_x <- auc(x_accuracy$Survived, x_accuracy$prob)
auc_x
```

With an AUC of `r formatC(auc_x, digits = 3)`, this model performs decently.

# Session Info {.toc-ignore}

```{r child='_sessioninfo.Rmd'}
```

[^logit]: Alternatively, you may see the function written

    $$p(X) = \frac{1}{1 + e^{-(\beta_0 + \beta_{1}X)}}$$

[^augment]: Alternatively, we can use `broom::augment()` to add predicted probabilities for the original dataset:

    ```{r augment_broom, dependson="make_age_pred", cache = FALSE}
    library(broom)

    augment(survive_age, newdata = titanic, type.predict = "response") %>%
      as_tibble()
    ```

    * `newdata = titanic` - produces a data frame containing all the original variables + the predicted probability for the observation
    * `type.predict = "response"` - ensures we get the predicted probabilities, not the logged version

