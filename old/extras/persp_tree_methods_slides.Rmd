---
title: "Statistical learning: tree-based methods"
author: |
  | MACS 30100
  | University of Chicago
date: "February 27, 2017"
output: rcfss::cfss_slides
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE)

library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(tree)
library(randomForest)
library(stringr)
library(ISLR)
library(gridExtra)
library(grid)
library(titanic)
library(rcfss)
library(pROC)
library(gbm)
# to get the tree graphs with the labels and values, use the forked
# version of ggdendro
# devtools::install_github("bensoltoff/ggdendro")
library(ggdendro)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal(base_size = 18))
```

## {.scrollable}

![](https://s-media-cache-ak0.pinimg.com/originals/7a/89/ff/7a89ff67b4ce34204c23135cbf35acfa.jpg)

## {.scrollable}

![](https://eight2late.files.wordpress.com/2016/02/7214525854_733237dd83_z1.jpg?w=700)

## {.scrollable}

![](https://s-media-cache-ak0.pinimg.com/564x/0b/87/df/0b87df1a54474716384f8ec94b52eab9.jpg)

## Decision trees

* Intuitive
* Regression and classification
* Split observations into regions, and base predictons on mean or mode in that region

## Single predictor

```{r auto-lm}
# add 95% confidence intervals to fitted values from augment()
add_ci <- function(df_augment) {
  df_augment %>%
    mutate(.fitted.low = .fitted - 1.96 * .se.fit,
           .fitted.high = .fitted + 1.96 * .se.fit)
}

# draw 95% confidence interval plot using results of add_ci()
plot_ci <- function(df_ci, x){
  ggplot(df_ci, aes_string(x, ".fitted")) +
  geom_line() +
  geom_line(aes(y = .fitted.low), linetype = 2) +
  geom_line(aes(y = .fitted.high), linetype = 2)
}

auto_lm <- glm(mpg ~ horsepower, data = Auto)

augment(auto_lm, newdata = data_grid(Auto, horsepower)) %>%
  add_ci() %>%
  plot_ci("horsepower") +
  geom_point(data = Auto, aes(y = mpg), alpha = .2) +
  labs(title = "Linear model of highway mileage",
       x = "Horsepower",
       y = "Highway mileage")
```

## Stratification

1. Divide the predictor space ($X_1, X_2, \dots, X_p$) into $J$ distinct and non-overlapping regions $R_1, R_2, \dots, R_J$.
1. For every observation in region $R_j$, we make the same prediction which is the mean of the response variable $Y$ for all observations in $R_j$.

* Iterative process

```{r part-tree-data}
# hackish function to get line segment coordinates for ggplot
partition.tree.data <- function (tree, label = "yval", add = FALSE, ordvars, ...) 
{
  ptXlines <- function(x, v, xrange, xcoord = NULL, ycoord = NULL, 
                       tvar, i = 1L) {
    if (v[i] == "<leaf>") {
      y1 <- (xrange[1L] + xrange[3L])/2
      y2 <- (xrange[2L] + xrange[4L])/2
      return(list(xcoord = xcoord, ycoord = c(ycoord, y1, 
                                              y2), i = i))
    }
    if (v[i] == tvar[1L]) {
      xcoord <- c(xcoord, x[i], xrange[2L], x[i], xrange[4L])
      xr <- xrange
      xr[3L] <- x[i]
      ll2 <- Recall(x, v, xr, xcoord, ycoord, tvar, i + 
                      1L)
      xr <- xrange
      xr[1L] <- x[i]
      return(Recall(x, v, xr, ll2$xcoord, ll2$ycoord, tvar, 
                    ll2$i + 1L))
    }
    else if (v[i] == tvar[2L]) {
      xcoord <- c(xcoord, xrange[1L], x[i], xrange[3L], 
                  x[i])
      xr <- xrange
      xr[4L] <- x[i]
      ll2 <- Recall(x, v, xr, xcoord, ycoord, tvar, i + 
                      1L)
      xr <- xrange
      xr[2L] <- x[i]
      return(Recall(x, v, xr, ll2$xcoord, ll2$ycoord, tvar, 
                    ll2$i + 1L))
    }
    else stop("wrong variable numbers in tree.")
  }
  if (inherits(tree, "singlenode")) 
    stop("cannot plot singlenode tree")
  if (!inherits(tree, "tree")) 
    stop("not legitimate tree")
  frame <- tree$frame
  leaves <- frame$var == "<leaf>"
  var <- unique(as.character(frame$var[!leaves]))
  if (length(var) > 2L || length(var) < 1L) 
    stop("tree can only have one or two predictors")
  nlevels <- sapply(attr(tree, "xlevels"), length)
  if (any(nlevels[var] > 0L)) 
    stop("tree can only have continuous predictors")
  x <- rep(NA, length(leaves))
  x[!leaves] <- as.double(substring(frame$splits[!leaves, "cutleft"], 
                                    2L, 100L))
  m <- model.frame(tree)
  if (length(var) == 1L) {
    x <- sort(c(range(m[[var]]), x[!leaves]))
    if (is.null(attr(tree, "ylevels"))) 
      y <- frame$yval[leaves]
    else y <- frame$yprob[, 1L]
    y <- c(y, y[length(y)])
    if (add) {
      # lines(x, y, type = "s", ...)
    }
    else {
      a <- attributes(attr(m, "terms"))
      yvar <- as.character(a$variables[1 + a$response])
      xo <- m[[yvar]]
      if (is.factor(xo)) 
        ylim <- c(0, 1)
      else ylim <- range(xo)
      # plot(x, y, ylab = yvar, xlab = var, type = "s", ylim = ylim,
      #      xaxs = "i", ...)
    }
    data_frame(x = x, y = y)
  }
  else {
    if (!missing(ordvars)) {
      ind <- match(var, ordvars)
      if (any(is.na(ind))) 
        stop("unmatched names in vars")
      var <- ordvars[sort(ind)]
    }
    lab <- frame$yval[leaves]
    if (is.null(frame$yprob)) 
      lab <- format(signif(lab, 3L))
    else if (match(label, attr(tree, "ylevels"), nomatch = 0L)) 
      lab <- format(signif(frame$yprob[leaves, label], 
                           3L))
    rx <- range(m[[var[1L]]])
    rx <- rx + c(-0.025, 0.025) * diff(rx)
    rz <- range(m[[var[2L]]])
    rz <- rz + c(-0.025, 0.025) * diff(rz)
    xrange <- c(rx, rz)[c(1, 3, 2, 4)]
    xcoord <- NULL
    ycoord <- NULL
    xy <- ptXlines(x, frame$var, xrange, xcoord, ycoord, 
                   var)
    xx <- matrix(xy$xcoord, nrow = 4L)
    yy <- matrix(xy$ycoord, nrow = 2L)

    return(list(data_frame(xmin = xx[1L,],
                           ymin = xx[2L,],
                           xmax = xx[3L,],
                           ymax = xx[4L,]),
                data_frame(x = yy[1L,],
                           y = yy[2L,],
                           label = lab)))
    # if (!add) 
    #   plot(rx, rz, xlab = var[1L], ylab = var[2L], type = "n", 
    #        xaxs = "i", yaxs = "i", ...)
    # segments(xx[1L, ], xx[2L, ], xx[3L, ], xx[4L, ])
    # text(yy[1L, ], yy[2L, ], as.character(lab), ...)
  }
}
```

----

```{r auto-tree2}
# estimate model
auto_tree <- tree(mpg ~ horsepower, data = Auto,
     control = tree.control(nobs = nrow(Auto),
                            mindev = 0))

mod <- prune.tree(auto_tree, best = 2)

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(alpha = .2) +
  geom_step(data = partition.tree.data(mod), aes(x, y), size = 1.5) +
  geom_vline(data = partition.tree.data(mod), aes(xintercept = x), linetype = 2) +
  coord_cartesian(xlim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  ylim = c(min(Auto$mpg), max(Auto$mpg)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
grid.arrange(ptree, preg, ncol = 2,
             top = textGrob(str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)),
                            gp = gpar(fontsize = 20)))
```

----

```{r auto-tree3}
mod <- prune.tree(auto_tree, best = 3)

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(alpha = .2) +
  geom_step(data = partition.tree.data(mod), aes(x, y), size = 1.5) +
  geom_vline(data = partition.tree.data(mod), aes(xintercept = x), linetype = 2) +
  coord_cartesian(xlim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  ylim = c(min(Auto$mpg), max(Auto$mpg)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
grid.arrange(ptree, preg, ncol = 2,
             top = textGrob(str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)),
                            gp = gpar(fontsize = 20)))
```

----

```{r auto-treeall}
mod <- auto_tree

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(alpha = .2) +
  geom_step(data = partition.tree.data(mod), aes(x, y), size = 1.5) +
  geom_smooth(data = partition.tree.data(mod), aes(x, y), se = FALSE) +
  coord_cartesian(xlim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  ylim = c(min(Auto$mpg), max(Auto$mpg)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
grid.arrange(ptree, preg, ncol = 2,
             top = textGrob(str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)),
                            gp = gpar(fontsize = 20)))
```

## Multiple predictors

```{r auto-tree-weight}
auto_tree <- tree(mpg ~ horsepower + weight, data = Auto,
     control = tree.control(nobs = nrow(Auto),
                            mindev = 0))

mod <- prune.tree(auto_tree, best = 3)

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(weight, horsepower)) +
  geom_point(alpha = .2) +
  geom_segment(data = partition.tree.data(mod)[[1]],
               aes(x = xmin, xend = xmax, y = ymin, yend = ymax)) +
  geom_text(data = partition.tree.data(mod)[[2]],
            aes(x = x, y = y, label = label)) +
  coord_cartesian(xlim = c(min(Auto$weight), max(Auto$weight)),
                  ylim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
grid.arrange(ptree, preg, ncol = 2,
             top = textGrob(str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)),
                            gp = gpar(fontsize = 20)))
```

## Additional nodes {.scrollable}

```{r auto-tree-weight-i}
for(i in c(4:10, 20, 50)){
  mod <- prune.tree(auto_tree, best = i)
  
  # plot tree
  tree_data <- dendro_data(mod)
  ptree <- ggplot(segment(tree_data)) +
    geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
                 alpha = 0.5) +
    geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
    geom_text(data = leaf_label(tree_data), 
              aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
    theme_dendro()
  
  # plot region space
  preg <- ggplot(Auto, aes(weight, horsepower)) +
    geom_point(alpha = .2) +
    geom_segment(data = partition.tree.data(mod)[[1]],
                 aes(x = xmin, xend = xmax, y = ymin, yend = ymax)) +
    geom_text(data = partition.tree.data(mod)[[2]],
              aes(x = x, y = y, label = label)) +
    coord_cartesian(xlim = c(min(Auto$weight), max(Auto$weight)),
                    ylim = c(min(Auto$horsepower), max(Auto$horsepower)),
                    expand = FALSE) +
    theme(panel.border = element_rect(fill = NA, size = 1))
  
  # display plots side by side
  grid.arrange(ptree, preg, ncol = 2,
               top = textGrob(str_c("Terminal Nodes = ", i),
                              gp = gpar(fontsize = 20))) 
}
```

## Estimation procedure

* Reduce the sum of the squared error

    $$\sum_{j=1}^J \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2$$

* Recursive binary strategy
    * Top-down
    * Greedy
* Continue until stopping point reached

## Pruning the tree

![](https://growingtogether.areavoices.com/files/2015/11/pruning.jpg)

## Pruning the tree

* Balance accuracy and complexity
* Cost complexity pruning
* Tuning parameter
* Use with $k$-fold cross-validation

## Pruning Auto tree

```{r auto-tree-default}
auto_tree <- tree(mpg ~ horsepower + weight, data = Auto,
     control = tree.control(nobs = nrow(Auto),
                            mindev = 0))
mod <- auto_tree

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(weight, horsepower)) +
  geom_point(alpha = .2) +
  geom_segment(data = partition.tree.data(mod)[[1]],
               aes(x = xmin, xend = xmax, y = ymin, yend = ymax)) +
  geom_text(data = partition.tree.data(mod)[[2]],
            aes(x = x, y = y, label = label)) +
  coord_cartesian(xlim = c(min(Auto$weight), max(Auto$weight)),
                  ylim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
grid.arrange(ptree, preg, ncol = 2,
             top = textGrob(str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)),
                            gp = gpar(fontsize = 20)))
```

## Pruning Auto tree

```{r auto-tree-default-prune}
# generate 10-fold CV trees
auto_cv <- crossv_kfold(Auto, k = 10) %>%
  mutate(tree = map(train, ~ tree(mpg ~ horsepower + weight, data = .,
     control = tree.control(nobs = nrow(Auto),
                            mindev = 0))))

# calculate each possible prune result for each fold
auto_cv <- expand.grid(auto_cv$.id, 2:10) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(.id = Var1,
         k = Var2) %>%
  left_join(auto_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),
         mse = map2_dbl(prune, test, mse))

auto_cv %>%
  select(k, mse) %>%
  group_by(k) %>%
  summarize(test_mse = mean(mse),
            sd = sd(mse, na.rm = TRUE)) %>%
  ggplot(aes(k, test_mse)) +
  geom_point() +
  geom_line() +
  labs(x = "Number of terminal nodes",
       y = "Test MSE")
```

## Optimal Auto tree

```{r auto-tree-7}
mod <- prune.tree(auto_tree, best = 7)

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(weight, horsepower)) +
  geom_point(alpha = .2) +
  geom_segment(data = partition.tree.data(mod)[[1]],
               aes(x = xmin, xend = xmax, y = ymin, yend = ymax)) +
  geom_text(data = partition.tree.data(mod)[[2]],
            aes(x = x, y = y, label = label)) +
  coord_cartesian(xlim = c(min(Auto$weight), max(Auto$weight)),
                  ylim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
grid.arrange(ptree, preg, ncol = 2,
             top = textGrob(str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)),
                            gp = gpar(fontsize = 20)))
```

## Classification trees

* Class prediction
* Class proportion
* Classification error rate

    $$E = 1 - \max_{k}(\hat{p}_{mk})$$

* Gini index

    $$G = \sum_{k = 1}^k \hat{p}_{mk} (1 - \hat{p}_{mk})$$

* Cross-entropy

    $$D = - \sum_{k = 1}^K \hat{p}_{mk} \log(\hat{p}_{mk})$$

## Titanic tree

```{r titanic_tree}
titanic <- titanic_train %>%
  as_tibble() %>%
  mutate(Survived = factor(Survived, levels = 0:1, labels = c("Died", "Survived")),
         Female = factor(Sex, levels = c("male", "female")))

# estimate model
titanic_tree <- tree(Survived ~ Age + Female, data = titanic,
                     control = tree.control(nobs = nrow(titanic),
                            mindev = .001))

# plot unpruned tree
mod <- titanic_tree

tree_data <- dendro_data(mod)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Titanic survival tree",
       subtitle = "Age + Gender")
```

## Titanic tree

```{r titanic-tree-prune, dependson="titanic-tree"}
err.rate.tree <- function(model, data) {
  data <- as_tibble(data)
  response <- as.character(model$terms[[2]])
  
  pred <- predict(model, newdata = data, type = "class")
  actual <- data[[response]]
  
  return(mean(pred != actual, na.rm = TRUE))
}

# generate 10-fold CV trees
titanic_cv <- titanic %>%
  na.omit() %>%
  crossv_kfold(k = 10) %>%
  mutate(tree = map(train, ~ tree(Survived ~ Age + Female, data = .,
     control = tree.control(nobs = nrow(titanic),
                            mindev = .001))))

# calculate each possible prune result for each fold
titanic_cv <- expand.grid(titanic_cv$.id,
                          seq(from = 2, to = ceiling(length(mod$frame$yval) / 2))) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(.id = Var1,
         k = Var2) %>%
  left_join(titanic_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.misclass(.x, best = .y)),
         mse = map2_dbl(prune, test, err.rate.tree))

titanic_cv %>%
  group_by(k) %>%
  summarize(test_mse = mean(mse),
            sd = sd(mse, na.rm = TRUE)) %>%
  ggplot(aes(k, test_mse)) +
  geom_point() +
  geom_line() +
  labs(title = "Titanic survival tree",
       subtitle = "Age + Gender",
       x = "Number of terminal nodes",
       y = "Test error rate")
```

## Optimal Titanic tree

```{r titanic-tree-6, dependson="titanic-tree"}
mod <- prune.tree(titanic_tree, best = 6)

tree_data <- dendro_data(mod)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Titanic survival tree",
       subtitle = "Age + Gender")
```

## Increasing node purity

```{r titanic-m-13}
titanic_m13 <- titanic %>%
  filter(Female == "male", Age >= 13) %>%
  count(Survived)

knitr::kable(titanic_m13,
             caption = "Males older than or equal to 13 on the Titanic",
             col.names = c("Outcome", "Number of training observations"))
```

## Increasing node purity

```{r titanic-m-13-split}
titanic %>%
  filter(Female == "male", Age >= 13) %>%
  mutate(age25 = Age < 24.75) %>%
  count(age25, Survived) %>%
  complete(age25, Survived, fill = list(n = 0)) %>%
  knitr::kable(col.names = c("Less than 24.75 years old", "Outcome", "Number of training observations"))
```

## Trees vs. regression

* Linear functional form

    $$f(X) = \beta_0 + \sum_{j = 1}^p X_j \beta_j$$

* Decision tree functional form

    $$f(X) = \sum_{m = 1}^M c_m \cdot 1_{X \in R_m}$$

## Benefits/drawbacks to decision trees

* Easy to explain
* Visualizing
* Qualitative predictors are easily handled
* Accuracy rates are lower
* Trees can be non-robust

## Non-robust tree

```{r auto-tree-val}
auto_val_test <- function(){
  # split data
  auto_split <- resample_partition(Auto, p = c(test = 0.3, train = 0.7))
  
  # estimate model
  val <- tree(mpg ~ horsepower + weight, data = auto_split$train)
  
  # estimate test mse
  mse(val, auto_split$test)
}

# repeat the procedure 100 times
val_mse <- data_frame(id = 1:1000,
                      mse = map_dbl(id, ~ auto_val_test()))

# distribution of the mse
ggplot(val_mse, aes(mse)) +
  geom_histogram() +
  geom_vline(xintercept = mean(val_mse$mse))
```

## Bagging

* Trees -> high variance and low bias
* Want low variance and low bias
* Bootstrap aggregating

    $$\hat{f}_{\text{avg}}(x) = \frac{1}{B} \sum_{b = 1}^B \hat{f}^b(x)$$

    $$\hat{f}_{\text{bag}}(x) = \frac{1}{B} \sum_{b = 1}^B \hat{f}^b(x)$$

## Out-of-bag estimates

```{r boot-prop}
# generate sample index
samp <- data_frame(x = seq.int(1000))

# generate bootstrap sample and count proportion of observations in each draw
prop_drawn <- bootstrap(samp, n = nrow(samp)) %>%
  mutate(strap = map(strap, as_tibble)) %>%
  unnest(strap) %>%
  mutate(drawn = TRUE) %>%
  complete(.id, x, fill = list(drawn = FALSE)) %>%
  distinct %>%
  group_by(x) %>%
  mutate(n_drawn = cumsum(drawn),
         .id = as.numeric(.id),
         n_prop = n_drawn / .id)

ggplot(prop_drawn, aes(.id, n_prop, group = x)) +
  geom_line(alpha = .05) +
  labs(x = "b-th bootstrap sample ",
       y = "Proportion i-th observation in samples 1:b")
```

## Out-of-bag estimates {.scrollable}

```{r titanic-bag-oob}
titanic_rf_data <- titanic %>%
    select(-Name, -Ticket, -Cabin, -Sex, -PassengerId) %>%
    mutate_each(funs(as.factor(.)), Pclass, Embarked) %>%
    na.omit

(titanic_bag <- randomForest(Survived ~ ., data = titanic_rf_data,
                             mtry = 7, ntree = 500))
```

##### Estimation time for OOB error rate

```{r titanic-bag-oob-time}
system.time({
  randomForest(Survived ~ ., data = titanic_rf_data,
                              mtry = 7, ntree = 500)
})
```

##### Estimation time for $10$-fold CV error rate

```{r err-rate-rf}
err.rate.rf <- function(model, data) {
  data <- as_tibble(data)
  response <- as.character(model$terms[[2]])
  
  pred <- predict(model, newdata = data, type = "response")
  actual <- data[[response]]
  
  return(mean(pred != actual, na.rm = TRUE))
}
```

```{r titanic-bag-cv-time}
system.time({
  crossv_kfold(titanic_rf_data, k = 10) %>%
    mutate(model = map(train, ~ randomForest(Survived ~ ., data = .,
                              mtry = 7, ntree = 500)),
           test.err = map2_dbl(model, test, err.rate.rf)) %>%
    summarize(mean(test.err))
})
```

## Variable importance measures

```{r titanic-varimp}
data_frame(var = rownames(importance(titanic_bag)),
           MeanDecreaseGini = importance(titanic_bag)[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreaseGini, fun = median)) %>%
  ggplot(aes(var, MeanDecreaseGini)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting survival on the Titanic",
       subtitle = "Bagging",
       x = NULL,
       y = "Average decrease in the Gini Index")
```

## Random forests

* Correlated trees
* Random forests

## Bagged vs. random forest

```{r titanic-bag-reprint}
titanic_bag
(titanic_rf <- randomForest(Survived ~ ., data = titanic_rf_data,
                            ntree = 500))
```

## Bagged vs. random forest

```{r titanic-bag-rf-varuse}
seq.int(titanic_bag$ntree) %>%
  map_df(~ getTree(titanic_bag, k = ., labelVar = TRUE)[1,]) %>%
  count(`split var`) %>%
  knitr::kable(caption = "Variable used to generate the first split in each tree",
               col.names = c("Variable used to split", "Number of training observations"))

seq.int(titanic_rf$ntree) %>%
  map_df(~ getTree(titanic_rf, k = ., labelVar = TRUE)[1,]) %>%
  count(`split var`) %>%
  knitr::kable(caption = "Variable used to generate the first split in each tree",
               col.names = c("Variable used to split", "Number of training observations"))
```

## Bagged vs. random forest

```{r titanic-rf}
data_frame(var = rownames(importance(titanic_rf)),
           `Random forest` = importance(titanic_rf)[,1]) %>%
  left_join(data_frame(var = rownames(importance(titanic_rf)),
           Bagging = importance(titanic_bag)[,1])) %>%
  mutate(var = fct_reorder(var, Bagging, fun = median)) %>%
  gather(model, gini, -var) %>%
  ggplot(aes(var, gini, color = model)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting survival on the Titanic",
       x = NULL,
       y = "Average decrease in the Gini Index",
       color = "Method")
```

## Boosting

* Sequential growth
* Predict residuals, not response variable $Y$
* Updating over time
* Learning slowly

## Boosting parameters

1. The **number of trees** $B$
1. The **shrinkage parameter** $\lambda$
1. The **number of $d$ splits in each tree**

## Method comparison

```{r titanic-compare-all}
titanic_split <- resample_partition(titanic_rf_data, p = c("test" = .3,
                                                           "train" = .7))

titanic_models <- list("bagging" = randomForest(Survived ~ ., data = titanic_split$train,
                                                mtry = 7, ntree = 10000),
                       "rf_mtry2" = randomForest(Survived ~ ., data = titanic_split$train,
                                                 mtry = 2, ntree = 10000),
                       "rf_mtry4" = randomForest(Survived ~ ., data = titanic_split$train,
                                                 mtry = 4, ntree = 10000),
                       "boosting_depth1" = gbm(as.numeric(Survived) - 1 ~ .,
                                               data = titanic_split$train,
                                               n.trees = 10000, interaction.depth = 1),
                       "boosting_depth2" = gbm(as.numeric(Survived) - 1 ~ .,
                                               data = titanic_split$train,
                                               n.trees = 10000, interaction.depth = 2),
                       "boosting_depth4" = gbm(as.numeric(Survived) - 1 ~ .,
                                               data = titanic_split$train,
                                               n.trees = 10000, interaction.depth = 4))

boost_test_err <- data_frame(bagging = predict(titanic_models$bagging,
                                               newdata = as_tibble(titanic_split$test),
                                               predict.all = TRUE)[[2]] %>%
                               apply(2, function(x) x != as_tibble(titanic_split$test)$Survived) %>%
                               apply(2, mean),
                             rf_mtry2 = predict(titanic_models$rf_mtry2,
                                                newdata = as_tibble(titanic_split$test),
                                                predict.all = TRUE)[[2]] %>%
                               apply(2, function(x) x != as_tibble(titanic_split$test)$Survived) %>%
                               apply(2, mean),
                             rf_mtry4 = predict(titanic_models$rf_mtry4,
                                                newdata = as_tibble(titanic_split$test),
                                                predict.all = TRUE)[[2]] %>%
                               apply(2, function(x) x != as_tibble(titanic_split$test)$Survived) %>%
                               apply(2, mean),
                             boosting_depth1 = predict(titanic_models$boosting_depth1,
                                                       newdata = as_tibble(titanic_split$test),
                                                       n.trees = 1:10000) %>%
                               apply(2, function(x) round(x) == as.numeric(as_tibble(titanic_split$test)$Survived) - 1) %>%
                               apply(2, mean),
                             boosting_depth2 = predict(titanic_models$boosting_depth2,
                                                       newdata = as_tibble(titanic_split$test),
                                                       n.trees = 1:10000) %>%
                               apply(2, function(x) round(x) == as.numeric(as_tibble(titanic_split$test)$Survived) - 1) %>%
                               apply(2, mean),
                             boosting_depth4 = predict(titanic_models$boosting_depth4,
                                                       newdata = as_tibble(titanic_split$test),
                                                       n.trees = 1:10000) %>%
                               apply(2, function(x) round(x) == as.numeric(as_tibble(titanic_split$test)$Survived) - 1) %>%
                               apply(2, mean))

boost_test_err %>%
  mutate(id = row_number()) %>%
  mutate_each(funs(cummean(.)), bagging:rf_mtry4) %>%
  gather(model, err, -id) %>%
  mutate(model = factor(model, levels = names(titanic_models),
                        labels = c("Bagging", "Random forest: m = \\sqrt(p)",
                                   "Random forest: m = 4",
                                   "Boosting: depth = 1",
                                   "Boosting: depth = 2",
                                   "Boosting: depth = 4"))) %>%
  ggplot(aes(id, err, color = model)) +
  geom_line() +
  scale_color_brewer(type = "qual", palette = "Dark2") +
  labs(x = "Number of trees",
       y = "Test classification error",
       color = "Model")
```

## Optimal iterations for boosting

```{r titanic-boost-opt}
data_frame(depth = c(1, 2, 4),
           model = titanic_models[c("boosting_depth1", "boosting_depth2", "boosting_depth4")],
           optimal = map_dbl(model, gbm.perf, plot.it = FALSE)) %>%
  select(-model) %>%
  knitr::kable(caption = "Optimal number of boosting iterations",
               col.names = c("Depth", "Optimal number of iterations"))
  
```




