---
title: "Statistical learning: support vector machines"
author: |
  | MACS 30100
  | University of Chicago
date: "March 1, 2017"
output: rcfss::cfss_slides
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE)

library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(tree)
library(randomForest)
library(stringr)
library(ISLR)
library(titanic)
library(rcfss)
library(pROC)
library(gbm)
library(e1071)
library(grid)
library(gridExtra)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal(base_size = 22))
```

## Hyperplanes

* Two-dimensional hyperplane

    $$\beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0$$

* $p$-dimensional hyperplane

    $$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p = 0$$

* Points on the hyperplane

    $$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p > 0$$

    $$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p < 0$$

----

```{r hyperplane}
sim_hyper <- data_frame(x1 = seq(-1.5, 1.5, length.out = 20),
                        x2 = seq(-1.5, 1.5, length.out = 20)) %>%
  expand(x1, x2) %>%
  mutate(y = 1 + 2 * x1 + 3 * x2,
         group = ifelse(y < 0, -1,
                        ifelse(y > 0, 1, 0)),
         group = factor(group))

sim_hyper_line <- data_frame(x1 = seq(-1.5, 1.5, length.out = 20),
                             x2 = (-1 - 2 * x1) / 3)

ggplot(sim_hyper, aes(x1, x2, color = group)) +
  geom_point() +
  geom_line(data = sim_hyper_line, aes(color = NULL)) +
  labs(title = "Hyperplane in two dimensions") +
  theme(legend.position = "none")
```

## Classification using a separating hyperplane

$$x_1 = \begin{pmatrix}
  x_{11} \\
  \vdots \\
  x_{1p}
 \end{pmatrix},
 \dots, x_n = \begin{pmatrix}
  x_{n1} \\
  \vdots \\
  x_{np}
 \end{pmatrix}$$

* $y_1, \dots, y_n \in \{-1, 1 \}$
* Test observation $x^*$
* Separating hyperplane

    $$\beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} > 0, \text{if } y_i = 1$$
    $$\beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} < 0, \text{if } y_i = -1$$
    
----

```{r sim}
sim <- data_frame(x1 = runif(20, -2, 2),
                  x2 = runif(20, -2, 2),
                  y = ifelse(1 + 2 * x1 + 3 * x2 < 0, -1, 1)) %>%
  mutate_each(funs(ifelse(y == 1, . + 1.5, .)), x2) %>%
  mutate(y = factor(y, levels = c(-1, 1))) %>%
  mutate(line1 = (-1 - 2 * x1) / 3,
         line2 = .5 + (-1 - 1.5 * x1) / 3,
         line3 = .25 - .05 * x1)

ggplot(sim, aes(x1)) +
  geom_point(aes(y = x2, color = y)) +
  geom_line(aes(y = line1, color = NULL)) +
  geom_line(aes(y = line2, color = NULL)) +
  geom_line(aes(y = line3, color = NULL)) +
  labs(title = "Examples of separating hyperplanes") +
  theme(legend.position = "none")
```

----

```{r sim-decision}
sim_mod <- svm(y ~ x1 + x2, data = sim, kernel = "linear", cost = 1e05,
               scale = FALSE)
sim_coef <- c(sim_mod$rho, t(sim_mod$coefs) %*% sim_mod$SV)

sim_grid <- data_frame(x1 = seq(-2, 2, length.out = 100),
                  x2 = seq(-2, 3.5, length.out = 100)) %>%
  expand(x1, x2) %>%
  mutate(y = ifelse(-sim_coef[[1]] + sim_coef[[2]] * x1 + sim_coef[[3]] * x2 > 0, -1, 1),
         y = factor(y, levels = c(-1, 1)))

sim_plane <- data_frame(x1 = seq(-2, 2, length.out = 100),
                        x2 = (sim_coef[[1]] - sim_coef[[2]] * x1) / sim_coef[[3]])

ggplot(sim, aes(x1)) +
  geom_point(data = sim_grid, aes(x1, x2, color = y), alpha = .25, size = .25) +
  geom_point(aes(y = x2, color = y)) +
  geom_line(data = sim_plane, aes(x1, x2)) +
  labs(title = "Maximal margin classification") +
  theme(legend.position = "none")
```

## Maximal margin classifier

```{r mult-sep-hp}
ggplot(sim, aes(x1)) +
  geom_point(aes(y = x2, color = y)) +
  geom_line(aes(y = line1, color = NULL)) +
  geom_line(aes(y = line2, color = NULL)) +
  geom_line(aes(y = line3, color = NULL)) +
  labs(title = "Examples of separating hyperplanes") +
  theme(legend.position = "none")
```

## Maximal margin classifier

```{r sim-margin}
sim_pred <- predict(sim_mod, sim, decision.values = TRUE)
sim_dist <- attr(sim_pred, "decision.values")

ggplot(sim, aes(x1)) +
  geom_point(aes(y = x2, color = y)) +
  geom_point(data = sim_grid, aes(x1, x2, color = y), alpha = .1, size = .25) +
  geom_line(data = sim_plane, aes(x1, x2)) +
    geom_line(data = mutate(sim_plane, x2 = x2 - min(abs(sim_dist))),
              aes(x1, x2), linetype = 2) +
    geom_line(data = mutate(sim_plane, x2 = x2 + min(abs(sim_dist))),
              aes(x1, x2), linetype = 2) +
  labs(title = "Maximal margin classification") +
  theme(legend.position = "none")
```

## Maximal margin hyperplane

$n$ training observations with predictors $x_1, \dots, x_n \in \mathbb{R}^p$ and associated class labels $y_1, \dots, y_n \in \{-1, 1\}$

$$\begin{aligned}
& \underset{\beta_0, \beta_1, \dots, \beta_p}{\text{maximize}} & & M \\
& \text{s.t.} & &  \sum_{j=1}^p \beta_j^2 = 1, \\
& & & y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) \geq M \; \forall \; i = 1, \dots, n \\
\end{aligned}$$

----

```{r sim-nosep}
data_frame(x1 = runif(20, -2, 2),
           x2 = runif(20, -2, 2),
           y = c(rep(-1, 10), rep(1, 10))) %>%
  mutate(y = factor(y, levels = c(-1, 1))) %>%
  ggplot(aes(x1, x2, color = y)) +
  geom_point() +
  labs(title = "Non-separable data") +
  theme(legend.position = "none")
```

## Support vector classifier

* Imperfect separating hyperplane
* Why do this?
    * Perfect separating hyperplane doesn't exist
    * More confidence in predictions for majority of observations
    * Avoid overfitting the data

----

```{r sim-sensitive}
# original model
sensitive <- data_frame(x1 = runif(20, -2, 2),
                  x2 = runif(20, -2, 2),
                  y = ifelse(1 + 2 * x1 + 3 * x2 < 0, -1, 1)) %>%
  mutate_each(funs(ifelse(y == 1, . + .5, .)), x2) %>%
  mutate(y = factor(y, levels = c(-1, 1)))

sens_mod <- svm(y ~ x1 + x2, data = sensitive, kernel = "linear",
                cost = 1e05, scale = FALSE)
sens_coef <- c(sens_mod$rho, t(sens_mod$coefs) %*% sens_mod$SV)
sens_plane <- data_frame(x1 = seq(-2, 2, length.out = 100),
                        x2 = (sens_coef[[1]] - sens_coef[[2]] * x1) / sens_coef[[3]])

ggplot(sensitive, aes(x1)) +
  geom_point(aes(y = x2, color = y)) +
  geom_line(data = sens_plane, aes(x1, x2)) +
  labs(title = "Maximal margin classification") +
  theme(legend.position = "none")
```

----

```{r sim-sensitive2}
# slight tweak
sensitive2 <- data_frame(x1 = with(sensitive, x1[which(x2 == max(x2[y == -1]))]),
                         x2 = with(sensitive, max(x2[y == -1])) + .1,
                         y = factor(1, levels = c(-1, 1))) %>%
  bind_rows(sensitive)

sens2_mod <- svm(y ~ x1 + x2, data = sensitive2, kernel = "linear",
                cost = 1e05, scale = FALSE)
sens2_coef <- c(sens2_mod$rho, t(sens2_mod$coefs) %*% sens2_mod$SV)
sens2_plane <- data_frame(x1 = seq(-2, 2, length.out = 100),
                        x2 = (sens2_coef[[1]] - sens2_coef[[2]] * x1) / sens2_coef[[3]])

ggplot(sensitive2, aes(x1)) +
  geom_point(aes(y = x2, color = y)) +
  geom_line(data = sens2_plane, aes(x1, x2)) +
  geom_line(data = sens_plane, aes(x1, x2), linetype = 2) +
  labs(title = "Maximal margin classification") +
  theme(legend.position = "none")
```

----

![](http://i.imgur.com/kLGD6Bv.jpg)

## Support vector classifier

* Allows observations to exist on the wrong side of the margin
* Allows observations to exist on the wrong side of the hyperplane

## Support vector classifier

$$\begin{aligned}
& \underset{\beta_0, \beta_1, \dots, \beta_p, \epsilon_1, \dots, \epsilon_n}{\text{maximize}} & & M \\
& \text{s.t.} & &  \sum_{j=1}^p \beta_j^2 = 1, \\
& & & y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) \geq M(1 - \epsilon_i), \\
& & & \epsilon_i \geq 0, \sum_{i = 1}^n \epsilon_i \leq C \\
\end{aligned}$$

* Purpose of $\epsilon_i$
* Role of $C$
    * $C = 0$
    * Selecting $C$

----

```{r sim-c, fig.asp=1}
sim_c <- data_frame(x1 = rnorm(20),
                    x2 = rnorm(20),
                    y = ifelse(2 * x1 + x2 + rnorm(20, 0, .25) < 0, -1, 1)) %>%
  mutate(y = factor(y, levels = c(-1, 1)))

plot_svm <- function(df, cost = 1){
  # estimate model
  sim_mod <- svm(y ~ x1 + x2, data = df, kernel = "linear",
                 cost = cost,
                 scale = FALSE)
  
  # extract separating hyperplane
  sim_coef <- c(sim_mod$rho, t(sim_mod$coefs) %*% sim_mod$SV)
  sim_plane <- data_frame(x1 = seq(min(df$x1), max(df$x1), length.out = 100),
                          x2 = (-sim_coef[[1]] - sim_coef[[2]] * x1) / sim_coef[[3]])
  
  # extract properties to draw margins
  sim_pred <- predict(sim_mod, df, decision.values = TRUE)
  sim_dist <- attr(sim_pred, "decision.values")
  
  ggplot(df, aes(x1)) +
    geom_point(aes(y = x2, color = y)) +
    geom_line(data = sim_plane, aes(x1, x2)) +
    geom_line(data = mutate(sim_plane, x2 = x2 - min(abs(sim_dist))),
              aes(x1, x2), linetype = 2) +
    geom_line(data = mutate(sim_plane, x2 = x2 + min(abs(sim_dist))),
              aes(x1, x2), linetype = 2) +
    labs(subtitle = str_c("Cost = ", cost)) +
    coord_equal(xlim = range(df$x1),
                    ylim = range(df$x2)) +
    theme(legend.position = "none")
}

grid.arrange(grobs = list(plot_svm(sim_c, cost = 1),
                  plot_svm(sim_c, cost = 10),
                  plot_svm(sim_c, cost = 100),
                  plot_svm(sim_c, cost = 200)), ncol = 2)
```

## Non-linear decision boundaries

```{r sim-nonlinear}
set.seed(1)
x <- matrix(rnorm(200 * 2), ncol = 2)
x[1:100, ] <- x[1:100, ] + 2
x[101:150, ] <- x[101:150, ] - 2
y <- c(rep(1, 150), rep(2, 50))
sim_nonlm <- data.frame(x = x, y = as.factor(y)) %>%
  as_tibble %>%
  rename(x1 = x.1,
         x2 = x.2)

radial_p <- ggplot(sim_nonlm, aes(x1, x2, color = y)) +
  geom_point() +
  theme(legend.position = "none")
radial_p
```

## Adding quadratic terms

$$X_1, X_1^2, X_2, X_2^2, \dots, X_p, X_p^2$$

$$\begin{aligned}
& \underset{\beta_0, \beta_{11}, \beta_{12}, \dots, \beta_{p1}, \beta_{p2}, \epsilon_1, \dots, \epsilon_n}{\text{maximize}} & & M \\
& \text{s.t.} & & y_i \left( \beta_0 + \sum_{j = 1}^p \beta_{j1} x_{ij} + \sum_{j = 1}^p \beta_{j2} x_{ij}^2 \right) \geq M(1 - \epsilon_i), \\
& & & \epsilon_i \geq 0, \sum_{i = 1}^n \epsilon_i \leq C, \sum_{j = 1}^p \sum_{k = 1}^2 \beta_{jk}^2 = 1 \\
\end{aligned}$$

## Support vector classifier

* Inner product: $\langle a,b \rangle = \sum_{i = 1}^r a_i b_i$

    ```{r inner-prod}
    (x <- 1:5)
    (y <- 1:5)
    
    x %*% y
    ```

    $$\langle x_i, x_{i'} \rangle = \sum_{j = 1}^p x_{ij} x_{i'j}$$

* Linear support vector

    $$f(x) = \beta_0 + \sum_{i = 1}^n \alpha_i \langle x, x_i \rangle$$

    $$f(x) = \beta_0 + \sum_{i \in \mathbb{S}} \alpha_i \langle x, x_i \rangle$$

## Kernels

* Generalization of the inner product

    $$K(x_i, x_{i'})$$

* Linear kernel

    $$K(x_i, x_{i'}) = \sum_{j = 1}^p x_{ij} x_{i'j}$$

## Polynomial kernel

$$K(x_i, x_{i'}) = (1 + \sum_{j = 1}^p x_{ij} x_{i'j})^d$$

$$f(x) = \beta_0 + \sum_{i \in \mathbb{S}} \alpha_i K(x,x_i)$$

## Polynomial kernel

```{r svm-poly}
sim_nonlm <- data_frame(x1 = runif(100, -2, 2),
                  x2 = runif(100, -2, 2),
                  y = ifelse(x1 + x1^2 + x1^3 - x2 < 0 +
                               rnorm(100, 0, 1), -1, 1)) %>%
  mutate(y = factor(y, levels = c(-1, 1)))

ggplot(sim_nonlm, aes(x1, x2, color = y)) +
  geom_point() +
  theme(legend.position = "none")
```

## Polynomial kernel

```{r svm-poly2}
svm(y ~ x1 + x2, data = sim_nonlm, kernel = "polynomial", scale = FALSE, cost = 1) %>%
  plot(sim_nonlm, x2 ~ x1)
```

## Radial kernel {.scrollable}

$$K(x_i, x_{i'}) = \exp(- \gamma \sum_{j=1}^p (x_{ij} - x_{i'j})^2)$$

```{r svm-radial}
sim_rad_mod <- svm(y ~ x1 + x2, data = sim_nonlm,
                     kernel = "radial", cost = 5, scale = FALSE)

radial_p
plot(sim_rad_mod, sim_nonlm, x2 ~ x1)
```

## Why use kernels

* Does not enlarge feature space $p$
* Compute $K(x_i, x_{i'})$ for all $\binom{n}{2}$ distinct pairs $i, i'$

## Applying and interpreting SVMs

* Predictive models
* Not good for inference
* How to interpret

## Titanic - linear SVM {.scrollable}

```{r titanic-data}
titanic <- titanic_train %>%
  as_tibble %>%
  select(-Name, -Ticket, -Cabin, -PassengerId) %>%
  mutate_each(funs(as.factor(.)), Survived, Pclass, Embarked) %>%
  na.omit

titanic_split <- resample_partition(titanic, p = c("test" = .3, "train" = .7))
```

```{r titanic-linear-tune, dependson="titanic-data"}
titanic_tune <- tune(svm, Survived ~ Age + Fare, data = as_tibble(titanic_split$train),
                     kernel = "linear",
                     range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(titanic_tune)
```

```{r titanic-linear-pred}
titanic_best <- titanic_tune$best.model
summary(titanic_best)

# get predictions for test set
fitted <- predict(titanic_best, as_tibble(titanic_split$test), decision.values = TRUE) %>%
  attributes

roc_line <- roc(as_tibble(titanic_split$test)$Survived, fitted$decision.values)
plot(roc_line)
auc(roc_line)
```

## Titanic - polynomial SVM {.scrollable}

```{r titanic-svm-poly}
titanic_poly_tune <- tune(svm, Survived ~ Age + Fare, data = as_tibble(titanic_split$train),
                     kernel = "polynomial",
                     range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(titanic_poly_tune)

titanic_poly_best <- titanic_poly_tune$best.model
summary(titanic_poly_best)

# get predictions for test set
fitted <- predict(titanic_poly_best, as_tibble(titanic_split$test), decision.values = TRUE) %>%
  attributes

roc_poly <- roc(as_tibble(titanic_split$test)$Survived, fitted$decision.values)
plot(roc_poly)
auc(roc_poly)
```

## Titanic - radial SVM {.scrollable}

```{r titanic-svm-radial}
titanic_rad_tune <- tune(svm, Survived ~ Age + Fare, data = as_tibble(titanic_split$train),
                     kernel = "radial",
                     range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(titanic_rad_tune)

titanic_rad_best <- titanic_rad_tune$best.model
summary(titanic_rad_best)

# get predictions for test set
fitted <- predict(titanic_rad_best, as_tibble(titanic_split$test), decision.values = TRUE) %>%
  attributes

roc_rad <- roc(as_tibble(titanic_split$test)$Survived, fitted$decision.values)
plot(roc_rad)
auc(roc_rad)
```

## Titanic - ROC curves

```{r titanic-roc-compare}
plot(roc_line, print.auc = TRUE, col = "blue")
plot(roc_poly, print.auc = TRUE, col = "red", print.auc.y = .4, add = TRUE)
plot(roc_rad, print.auc = TRUE, col = "orange", print.auc.y = .3, add = TRUE)
```

## Voter turnout {.scrollable}

```{r vote96}
(mh <- read_csv("../data/mental_health.csv") %>%
  mutate_each(funs(as.factor(.)), vote96, black, female, married) %>%
  na.omit)

mh_split <- resample_partition(mh, p = c("test" = .3, "train" = .7))
```

## Linear SVM {.scrollable}

```{r vote96-svm-line, dependson="vote96"}
mh_lin_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "linear",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_lin_tune)

mh_lin <- mh_lin_tune$best.model
summary(mh_lin)
fitted <- predict(mh_lin, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_line <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
plot(roc_line)
auc(roc_line)
```

## Polynomial SVM {.scrollable}

```{r vote96-svm-poly, dependson="vote96"}
mh_poly_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "polynomial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_poly_tune)

mh_poly <- mh_poly_tune$best.model
summary(mh_poly)
fitted <- predict(mh_poly, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_poly <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
plot(roc_poly)
auc(roc_poly)
```

## Radial SVM {.scrollable}

```{r vote96-svm-rad, dependson="vote96"}
mh_rad_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "radial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_rad_tune)

mh_rad <- mh_rad_tune$best.model
summary(mh_rad)
fitted <- predict(mh_rad, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_rad <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
plot(roc_rad)
auc(roc_rad)
```

## Compare SVMs {.scrollable}

```{r mh-roc-compare, dependson=c("vote96-svm-line","vote96-svm-poly","vote96-svm-rad")}
plot(roc_line, print.auc = TRUE, col = "blue")
plot(roc_poly, print.auc = TRUE, col = "red", print.auc.y = .4, add = TRUE)
plot(roc_rad, print.auc = TRUE, col = "orange", print.auc.y = .3, add = TRUE)
```

SVM kernel | CV training error rate
-----------|-----------------------
Linear     | `r mh_lin_tune$best.performance`
Polynomial | `r mh_poly_tune$best.performance`
Radial     | `r mh_rad_tune$best.performance`

## Logistic regression {.scrollable}

```{r vote96-logit}
mh_logit <- glm(vote96 ~ ., data = as_tibble(mh_split$train), family = binomial)
summary(mh_logit)

fitted <- predict(mh_logit, as_tibble(mh_split$test), type = "response")
logit_err <- mean(as_tibble(mh_split$test)$vote96 != round(fitted))

roc_logit <- roc(as_tibble(mh_split$test)$vote96, fitted)
plot(roc_logit)
auc(roc_logit)
```

The test error rate for the logistic regression model is `r logit_err`.

## Decision tree {.scrollable}

```{r vote96-tree}
mh_tree <- tree(vote96 ~ ., data = as_tibble(mh_split$train))
mh_tree

plot(mh_tree)
text(mh_tree, pretty = 0)

fitted <- predict(mh_tree, as_tibble(mh_split$test), type = "class")
tree_err <- mean(as_tibble(mh_split$test)$vote96 != fitted)

roc_tree <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
plot(roc_tree)
auc(roc_tree)
```

The test error rate for the decision tree model is `r tree_err`.

## Bagging {.scrollable}

```{r vote96-bag}
mh_bag <- randomForest(vote96 ~ ., data = as_tibble(mh_split$train),
                         mtry = 7)
mh_bag

varImpPlot(mh_bag)

fitted <- predict(mh_bag, as_tibble(mh_split$test), type = "prob")[,2]

roc_bag <- roc(as_tibble(mh_split$test)$vote96, fitted)
plot(roc_bag)
auc(roc_bag)
```

## Random forest {.scrollable}

```{r vote96-rf}
mh_rf <- randomForest(vote96 ~ ., data = as_tibble(mh_split$train))
mh_rf

varImpPlot(mh_rf)

fitted <- predict(mh_rf, as_tibble(mh_split$test), type = "prob")[,2]

roc_rf <- roc(as_tibble(mh_split$test)$vote96, fitted)
plot(roc_rf)
auc(roc_rf)
```

## Comparison {.scrollable}

```{r vote96-compare-roc}
plot(roc_poly, print.auc = TRUE, col = "blue", print.auc.x = .2)
plot(roc_logit, print.auc = TRUE, col = "red", print.auc.x = .2, print.auc.y = .4, add = TRUE)
plot(roc_tree, print.auc = TRUE, col = "orange", print.auc.x = .2, print.auc.y = .3, add = TRUE)
plot(roc_bag, print.auc = TRUE, col = "green", print.auc.x = .2, print.auc.y = .2, add = TRUE)
plot(roc_rf, print.auc = TRUE, col = "purple", print.auc.x = .2, print.auc.y = .1, add = TRUE)
```

* SVM (linear kernel)
* Logistic regression
* Decision tree
* Bagging ($n = 500$)
* Random forest ($n = 500, m = \sqrt{p}$)



