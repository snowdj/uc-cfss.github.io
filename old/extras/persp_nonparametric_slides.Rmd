---
title: "Statistical learning: non-parametric methods"
author: |
  | MACS 30100
  | University of Chicago
date: "March 6, 2017"
output: rcfss::cfss_slides
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE)

library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(stringr)
library(ISLR)
library(titanic)
library(rcfss)
library(pROC)
library(grid)
library(gridExtra)
library(FNN)
library(kknn)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal(base_size = 22))
```

## Parametric methods

1. First make an assumption about the functional form of $f$
1. After a model has been selected, **fit** or **train** the model using the actual data

## OLS

```{r get_ad, message = FALSE, warning = FALSE}
# get advertising data
advertising <- read_csv("../data/Advertising.csv") %>%
  tbl_df() %>%
  # remove id column
  select(-X1)
```

```{r plot_ad, dependson="get_ad"}
# plot separate facets for relationship between ad spending and sales
plot_ad <- advertising %>%
  gather(method, spend, -Sales) %>%
  ggplot(aes(spend, Sales)) +
  facet_wrap(~ method, scales = "free_x") +
  geom_point() +
  labs(x = "Spending (in thousands of dollars)")
```

```{r plot_parametric, dependson="get_ad"}
method_model <- function(df) {
  lm(Sales ~ spend, data = df)
}

ad_pred <- advertising %>%
  gather(method, spend, -Sales) %>%
  group_by(method) %>%
  nest() %>%
  mutate(model = map(data, method_model),
         pred = map(model, broom::augment)) %>%
  unnest(pred)

plot_ad +
  geom_smooth(method = "lm", se = FALSE) +
  geom_linerange(data = ad_pred,
                 aes(ymin = Sales, ymax = .fitted),
                 color = "blue",
                 alpha = .5) 
```

## Parametric methods

$$Y = \beta_0 + \beta_{1}X_1$$

* $Y =$ sales
* $X_{1} =$ advertising spending in a given medium
* $\beta_0 =$ intercept
* $\beta_1 =$ slope

## Non-parametric methods

* No/minimal assumptions about functional form
* Use data to estimate $f$ directly
    * Get close to data points
    * Avoid overcomplexity
* Requires large amount of observations

## Two types of non-parametric

1. Techniques that do not rely on data belonging to any particular distribution
1. Techniques that do not assume a global structure

## Non-parametric methods for description

* Data description

## Measures of central tendency

* Median
* Mode
* Arithmetic mean
    
    $$\bar{x} = \frac{1}{n} \sum_{i = 1}^n x_i$$
        
## Measures of dispersion

* Variance

    $$E[X] = \mu$$
        
    $$\text{Var}(X) \equiv \sigma^2 = E[X^2] - (E[X])^2$$

* Deviation
* Standard deviation
    
    $$\sigma = \sqrt{E[X^2] - (E[X])^2}$$
        
* Median absolute deviation

    $$MAD = \text{median}(|X_i - \text{median}(X)|)$$

## Histograms {.scrollable}

```{r infant-data}
infant <- read_csv("../data/infant.csv") %>%
  # remove non-countries
  filter(is.na(`Value Footnotes`) | `Value Footnotes` != 1) %>%
  select(`Country or Area`, Year, Value) %>%
  rename(country = `Country or Area`,
         year = Year,
         mortal = Value)
```

```{r infant-hist}
ggplot(infant, aes(mortal)) +
  geom_histogram(bins = 10, origin = 0) +
  labs(title = "Histogram of infant mortality rate for 195 nations",
       subtitle = "10 bins, origin = 0",
       x = "Infant mortality rate (per 1,000)",
       y = "Frequency")

ggplot(infant, aes(mortal)) +
  geom_histogram(bins = 10, origin = -5) +
  labs(title = "Histogram of infant mortality rate for 195 nations",
       subtitle = "10 bins, origin = -5",
       x = "Infant mortality rate (per 1,000)",
       y = "Frequency")
```

## Density estimation

* Nonparametric density estimation

    $$x_0 + 2(j - 1)h \leq X_i < x_0 + 2jh$$

    $$\hat{p}(x) = \frac{\#_{i = 1}^n [x_0 + 2(j - 1)h \leq X_i < x_0 + 2jh]}{2nh}$$

    $$\hat{p}(x) = \frac{\#_{i = 1}^n [x_0 + 2(j - 1)h \leq X_i < x_0 + 2jh]}{2nh}$$

    $$\hat{p}(x) = \frac{1}{nh} \sum_{i = 1}^n W \left( \frac{x - X_i}{h} \right)$$

    $$W(z) = \begin{cases} 
      \frac{1}{2} & \text{for } |z| < 1 \\
      0 & \text{otherwise} \\
   \end{cases}$$
   
    $$z = \frac{x - X_i}{h}$$

## Naive density estimation

```{r}
ggplot(infant, aes(mortal)) +
  geom_density(kernel = "rectangular") +
  labs(title = "Naive density estimator of infant mortality rate for 195 nations",
       x = "Infant mortality rate (per 1,000)",
       y = "Density")
```


## Density estimation

* Kernels

    $$\hat{x}(x) = \frac{1}{nh} \sum_{i = 1}^k K \left( \frac{x - X_i}{h} \right)$$

## Gaussian kernel {.scrollable}

$$K(z) = \frac{1}{\sqrt{2 \pi}}e^{-\frac{1}{2} z^2}$$

```{r gaussian}
x <- rnorm(1000)

qplot(x, geom = "blank") +
  stat_function(fun = dnorm) +
  labs(title = "Gaussian (normal) kernel",
       x = NULL,
       y = NULL)

ggplot(infant, aes(mortal)) +
  geom_density(kernel = "gaussian") +
  labs(title = "Gaussian density estimator of infant mortality rate for 195 nations",
       x = "Infant mortality rate (per 1,000)",
       y = "Density")
```

## Rectangular (uniform) kernel {.scrollable}

$$K(z) = \frac{1}{2} \mathbf{1}_{\{ |z| \leq 1 \} }$$

```{r uniform}
x <- runif(1000, -1.5, 1.5)
x_lines <- tribble(
  ~x, ~y, ~xend, ~yend,
  -1, 0, -1, .5,
  1, 0, 1, .5
)

qplot(x, geom = "blank") +
  stat_function(fun = dunif, args = list(min = -1), geom = "step") +
  # geom_segment(data = x_lines, aes(x = x, y = y, xend = xend, yend = yend)) +
  labs(title = "Rectangular kernel",
       x = NULL,
       y = NULL)

ggplot(infant, aes(mortal)) +
  geom_density(kernel = "rectangular") +
  labs(title = "Rectangular density estimator of infant mortality rate for 195 nations",
       x = "Infant mortality rate (per 1,000)",
       y = "Density")
```

## Triangular kernel {.scrollable}

$$K(z) = (1 - |z|) \mathbf{1}_{\{ |z| \leq 1 \} }$$

```{r triangular}
triangular <- function(x) {
  (1 - abs(x)) * ifelse(abs(x) <= 1, 1, 0)
}

qplot(x, geom = "blank") +
  stat_function(fun = triangular) +
  labs(title = "Triangular kernel",
       x = NULL,
       y = NULL)

ggplot(infant, aes(mortal)) +
  geom_density(kernel = "triangular") +
  labs(title = "Triangular density estimator of infant mortality rate for 195 nations",
       x = "Infant mortality rate (per 1,000)",
       y = "Density")
```

## Quartic (biweight) kernel {.scrollable}

$$K(z) = \frac{15}{16} (1 - z^2)^2 \mathbf{1}_{\{ |z| \leq 1 \} }$$

```{r biweight}
biweight <- function(x) {
  (15 / 16) * (1 - x^2)^2 * ifelse(abs(x) <= 1, 1, 0)
}

qplot(x, geom = "blank") +
  stat_function(fun = biweight) +
  labs(title = "Biweight kernel",
       x = NULL,
       y = NULL)

ggplot(infant, aes(mortal)) +
  geom_density(kernel = "biweight") +
  labs(title = "Biweight density estimator of infant mortality rate for 195 nations",
       x = "Infant mortality rate (per 1,000)",
       y = "Density")
```

## Epanechnikov kernel {.scrollable}

$$K(z) = \frac{3}{4} (1 - z^2) \mathbf{1}_{\{ |z| \leq 1 \} }$$

```{r epanechnikov}
epanechnikov <- function(x) {
  (15 / 16) * (1 - x^2)^2 * ifelse(abs(x) <= 1, 1, 0)
}

qplot(x, geom = "blank") +
  stat_function(fun = epanechnikov) +
  labs(title = "Epanechnikov kernel",
       x = NULL,
       y = NULL)

ggplot(infant, aes(mortal)) +
  geom_density(kernel = "epanechnikov") +
  labs(title = "Epanechnikov density estimator of infant mortality rate for 195 nations",
       x = "Infant mortality rate (per 1,000)",
       y = "Density")
```

## Comparison of kernels {.scrollable}

```{r kernels}
qplot(x, geom = "blank") +
  stat_function(aes(color = "Gaussian"), fun = dnorm) +
  stat_function(aes(color = "Epanechnikov"), fun = epanechnikov) +
  stat_function(aes(color = "Rectangular"), fun = dunif, args = list(min = -1), geom = "step") +
  stat_function(aes(color = "Triangular"), fun = triangular) +
  stat_function(aes(color = "Biweight"), fun = biweight) +
  labs(x = NULL,
       y = NULL,
       color = NULL) +
  theme(legend.position = c(0.04, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(fill = "white"))

ggplot(infant, aes(mortal)) +
  geom_density(aes(color = "Gaussian"), kernel = "gaussian") +
  geom_density(aes(color = "Epanechnikov"), kernel = "epanechnikov") +
  geom_density(aes(color = "Rectangular"), kernel = "rectangular") +
  geom_density(aes(color = "Triangular"), kernel = "triangular") +
  geom_density(aes(color = "Biweight"), kernel = "biweight") +
  labs(title = "Density estimators of infant mortality rate for 195 nations",
       x = "Infant mortality rate (per 1,000)",
       y = "Density",
       color = "Kernel") +
  theme(legend.position = c(0.96, 1),
        legend.justification = c(1, 1),
        legend.background = element_rect(fill = "white"))
```

## Selecting the bandwidth $h$

```{r gaussian-h}
ggplot(infant, aes(mortal)) +
  geom_density(kernel = "gaussian", adjust = 5) +
  geom_density(kernel = "gaussian", adjust = 1, linetype = 2) +
  geom_density(kernel = "gaussian", adjust = 1/5, linetype = 3) +
  labs(title = "Gaussian density estimators of infant mortality rate for 195 nations",
       subtitle = "Three different bandwidth parameters",
       x = "Infant mortality rate (per 1,000)",
       y = "Density")
```

## Selecting the bandwidth $h$

$$h = 0.9 \sigma n^{-1 / 5}$$

$$A = \min \left( S, \frac{IQR}{1.349} \right)$$

## Naive non-parametric regression

```{r np-data}
n <- 1000000
wage <- data_frame(educ = rpois(n, lambda = 12),
                   age = rpois(n, lambda = 40),
                   prestige = rpois(n, lambda = 3)) %>%
  mutate(educ = ifelse(educ > 25, 25, educ),
         wage = 10 + 2 * educ + .5 * age + 5 * prestige + rnorm(n, 0, 3))

ggplot(wage, aes(wage)) +
  geom_histogram(binwidth = 5) +
  labs(title = "Histogram of simulated income data",
       subtitle = "Binwidth = 5",
       x = "Income, in thousands of dollars",
       y = "Frequency count")
```

## Naive non-parametric regression {.scrollable}

$$\mu = E(\text{Income}|\text{Education}) = f(\text{Education})$$

```{r np-wage-cond}
wage %>%
  group_by(educ) %>%
  summarize(mean = mean(wage),
            sd = sd(wage)) %>%
  ggplot(aes(educ, mean, ymin = mean - sd, ymax = mean + sd)) +
  geom_errorbar() +
  geom_point() +
  labs(title = "Conditional income, by education level",
       subtitle = "Plus/minus SD",
       x = "Education level",
       y = "Income, in thousands of dollars")

wage %>%
  filter(educ == 12) %>%
  ggplot(aes(wage)) +
  geom_density() +
  geom_vline(xintercept = mean(wage$wage[wage$educ == 12]), linetype = 2) +
  labs(title = "Conditional distribution of income for education = 12",
       subtitle = str_c("Mean income = ", formatC(mean(wage$wage[wage$educ == 12]), digits = 3)),
       x = "Income, in thousands of dollars",
       y = "Frequency count")
```

## Naive non-parametric regression

$$\mu = E(Y|x) = f(x)$$

* Binning

## Naive non-parametric regression

```{r prestige}
# get data
prestige <- read_csv("../data/prestige.csv")
```

```{r prestige-5bins, dependson="prestige"}
# bin into 5 and get means
prestige_bin <- prestige %>%
  mutate(bin = cut_number(income, 6)) %>%
  group_by(bin) %>%
  summarize(prestige = mean(prestige),
            income = mean(income))

# get cutpoints
labs <- levels(prestige_bin$bin)
cutpoints <- c(as.numeric( sub("\\((.+),.*", "\\1", labs) ),
  as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", labs) )) %>%
  unique %>%
  sort %>%
  .[2:(length(.)-1)] %>%
  as_tibble

ggplot(prestige, aes(income, prestige)) +
  geom_point(shape = 1) +
  geom_line(data = prestige_bin) +
  geom_point(data = prestige_bin) +
  geom_vline(data = cutpoints, aes(xintercept = value), linetype = 2) +
  labs(title = "Naive nonparametric regression",
       subtitle = "Bins = 5",
       x = "Average income (in dollars)",
       y = "Occupational prestige")
```

## Naive non-parametric regression

```{r prestige-50bins, dependson="prestige"}
# bin into 50 and get means
prestige_bin <- prestige %>%
  mutate(bin = cut_number(income, 51)) %>%
  group_by(bin) %>%
  summarize(prestige = mean(prestige),
            income = mean(income))

# get cutpoints
labs <- levels(prestige_bin$bin)
cutpoints <- c(as.numeric( sub("\\((.+),.*", "\\1", labs) ),
  as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", labs) )) %>%
  unique %>%
  sort %>%
  .[2:(length(.)-1)] %>%
  as_tibble

ggplot(prestige, aes(income, prestige)) +
  geom_point(shape = 1) +
  geom_line(data = prestige_bin) +
  geom_point(data = prestige_bin) +
  geom_vline(data = cutpoints, aes(xintercept = value), linetype = 2, alpha = .25) +
  labs(title = "Naive nonparametric regression",
       subtitle = "Bins = 50",
       x = "Average income (in dollars)",
       y = "Occupational prestige")
```

## Naive non-parametric regression

$$X_1 \in \{1, 2, \dots ,10 \}$$
$$X_2 \in \{1, 2, \dots ,10 \}$$
$$X_3 \in \{1, 2, \dots ,10 \}$$

* $10^3 = 1000$ possible combinations of the explanatory variables and $1000$ conditional expectations of $Y$ given $X$:

$$\mu = E(Y|x_1, x_2, x_3) = f(x_1, x_2, x_3)$$

## Naive non-parametric regression {.scrollable}

```{r wage-sim-describe}
ggplot(wage, aes(educ)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of simulated wage data",
       x = "Education",
       y = "Frequency count")

ggplot(wage, aes(age)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of simulated wage data",
       x = "Age",
       y = "Frequency count")

ggplot(wage, aes(prestige)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of simulated wage data",
       x = "Job prestige",
       y = "Frequency count")
```

## Naive non-parametric regression

```{r wage-sim-np}
wage_np <- wage %>%
  group_by(educ, age, prestige) %>%
  summarize(wage_mean = mean(wage),
            wage_sd = sd(wage),
            n = n()) %>%
  ungroup %>%
  complete(educ, age, prestige, fill = list(wage_mean = NA,
                                          wage_sd = NA,
                                          n = 0))

# number of unique combos 
wage_unique <- nrow(wage_np)

# n for each unique combo
ggplot(wage_np, aes(n)) +
  geom_density() +
  labs(title = "Naive nonparametric regression of simulated wage data",
       x = "Number of observations for each unique combination",
       y = "Density")
```

## Naive non-parametric regression

```{r wage-sim-np-ten}
n <- 10000000
wage10 <- data_frame(educ = rpois(n, lambda = 12),
                   age = rpois(n, lambda = 40),
                   prestige = rpois(n, lambda = 3)) %>%
  mutate(educ = ifelse(educ > 25, 25, educ),
         wage = 10 + 2 * educ + .5 * age + 5 * prestige + rnorm(n, 0, 3))

wage10_np <- wage10 %>%
  group_by(educ, age, prestige) %>%
  summarize(wage_mean = mean(wage),
            wage_sd = sd(wage),
            n = n()) %>%
  ungroup %>%
  complete(educ, age, prestige, fill = list(wage_mean = NA,
                                          wage_sd = NA,
                                          n = 0))

# number of unique combos 
wage10_unique <- nrow(wage10_np)

# n for each unique combo
ggplot(wage10_np, aes(n)) +
  geom_density() +
  labs(title = "Naive nonparametric regression of simulated wage data",
       x = "Number of observations for each unique combination",
       y = "Density")
```

## $K$-nearest neighbors regression

$$\hat{f}(x_0) = \frac{1}{K} \sum_{x_i \in N_0} y_i$$

## $K$-nearest neighbors regression

```{r prestige-knn-1}
prestige_knn1 <- knn.reg(select(prestige, income), y = prestige$prestige,
                         test = select(prestige, income), k = 1)

prestige %>%
  mutate(pred = prestige_knn1$pred) %>%
  ggplot(aes(income, prestige)) +
  geom_point() +
  geom_step(aes(y = pred)) +
  labs(title = "1-nearest neighbor regression",
       x = "Income (in dollars)",
       y = "Occupational prestige")
```

## $K$-nearest neighbors regression

```{r prestige-knn-9}
prestige_knn9 <- knn.reg(select(prestige, income), y = prestige$prestige,
                         test = select(prestige, income), k = 9)

prestige %>%
  mutate(pred = prestige_knn9$pred) %>%
  ggplot(aes(income, prestige)) +
  geom_point() +
  geom_step(aes(y = pred)) +
  labs(title = "9-nearest neighbor regression",
       x = "Income (in dollars)",
       y = "Occupational prestige")
```

## $K$-nearest neighbors regression

```{r np-p-line}
sim <- data_frame(x = runif(100, -1,1),
                  y = 2 + x + rnorm(100, 0, .2))

sim_knn9 <- knn.reg(select(sim, x), y = sim$y,
                         test = select(sim, x), k = 9)

sim %>%
  mutate(pred = sim_knn9$pred) %>%
  ggplot(aes(x, y)) +
  geom_point(shape = 1) +
  geom_abline(aes(color = "True"), intercept = 2, slope = 1) +
  geom_smooth(aes(color = "LM"), method = "lm", se = FALSE) +
  geom_step(aes(y = pred, color = "KNN")) +
  labs(color = "Method")
```

## $K$-nearest neighbors regression

$$f(x) = 2 + x + \epsilon_i$$

```{r np-p-line2}
# estimate test MSE for LM and KNN models
sim_test <- data_frame(x = runif(100, -1,1),
                  y = 2 + x + rnorm(100, 0, .2))
mse_lm <- lm(y ~ x, data = sim) %>%
  mse(sim_test)

mse_knn <- data_frame(k = 1:10,
                      knn = map(k, ~ knn.reg(select(sim, x), y = sim$y,
                         test = select(sim_test, x), k = .)),
                      mse = map_dbl(knn, ~ mean((sim_test$y - .$pred)^2)))

ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

## $K$-nearest neighbors regression

$$f(x) = 2 + x + x^2 + x^3 + \epsilon_i$$

```{r np-p-cubic}
x_cube <- function(x) {
  2 + x + x^2 + x^3
}

sim <- data_frame(x = runif(100, -1,1),
                  y = x_cube(x) + rnorm(100, 0, .2))

sim_knn9 <- knn.reg(select(sim, x), y = sim$y,
                         test = select(sim, x), k = 9)

sim %>%
  mutate(pred = sim_knn9$pred) %>%
  ggplot(aes(x, y)) +
  geom_point(shape = 1) +
  stat_function(aes(color = "True"), fun = x_cube) +
  geom_smooth(aes(color = "LM"), method = "lm", se = FALSE) +
  geom_step(aes(y = pred, color = "KNN")) +
  labs(color = "Method")

# estimate test MSE for LM and KNN models
sim_test <- data_frame(x = runif(100, -1,1),
                  y = x_cube(x) + rnorm(100, 0, .2))

mse_lm <- lm(y ~ x, data = sim) %>%
  mse(sim_test)

mse_knn <- data_frame(k = 1:10,
                      knn = map(k, ~ knn.reg(select(sim, x), y = sim$y,
                         test = select(sim_test, x), k = .)),
                      mse = map_dbl(knn, ~ mean((sim_test$y - .$pred)^2)))

ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

## $K$-nearest neighbors regression

$$f(x) = 2 + x + x^2 + x^3 + \epsilon_i$$

```{r knn-nonrobust}
sim_nr <- data_frame(x1 = runif(100, -1,1),
                  y = x_cube(x1) + rnorm(100, 0, .2),
                  x2 = rnorm(100, 0, 1),
                  x3 = rnorm(100, 0, 1),
                  x4 = rnorm(100, 0, 1),
                  x5 = rnorm(100, 0, 1),
                  x6 = rnorm(100, 0, 1))
sim_nr_test <- data_frame(x1 = runif(100, -1,1),
                       y = x_cube(x1) + rnorm(100, 0, .2),
                       x2 = rnorm(100, 0, 1),
                       x3 = rnorm(100, 0, 1),
                       x4 = rnorm(100, 0, 1),
                       x5 = rnorm(100, 0, 1),
                       x6 = rnorm(100, 0, 1))

sim_pred_knn <- expand.grid(p = 1:6,
            k = 1:10) %>%
  as_tibble %>%
  mutate(lm = map(p, ~ lm(formula(str_c("y ~ ", str_c("x", seq.int(.), collapse = " + "))),
                          data = sim_nr)),
         mse_lm = map_dbl(lm, ~ mse(., sim_nr_test)),
         knn = map2(p, k, ~ knn.reg(select_(sim_nr, .dots = str_c("x", seq.int(.x))),
                                    y = sim_nr$y,
                                    test = select_(sim_nr_test, .dots = str_c("x", seq.int(.x))),
                                    k = .y)),
         mse_knn = map_dbl(knn, ~ mean((sim_nr_test$y - .$pred)^2)))

ggplot(sim_pred_knn, aes(k, mse_knn)) +
  facet_grid(. ~ p, labeller = labeller(p = label_both)) +
  geom_line() +
  geom_point() +
  geom_hline(aes(yintercept = mse_lm), linetype = 2) +
  labs(title = "Test MSE for linear regression vs. KNN",
       x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

## Weighted $K$-nearest neighbors regression

$$\text{Distance}(x_i, y_i) = \left( \sum_{i = 1}^n |x_i - y_i| ^p \right)^\frac{1}{p}$$

## Weighted $K$-nearest neighbors regression

```{r knn-weight}
sim <- data_frame(x = runif(100, -1,1),
                  y = x_cube(x) + rnorm(100, 0, .2))

sim_wknn <- kknn(y ~ x, train = sim, test = sim, k = 5)

sim %>%
  mutate(pred = sim_wknn[["fitted.values"]]) %>%
  ggplot(aes(x, y)) +
  geom_point(shape = 1) +
  stat_function(aes(color = "True"), fun = x_cube) +
  geom_smooth(aes(color = "LM"), method = "lm", se = FALSE) +
  geom_step(aes(y = pred, color = "KNN")) +
  labs(title = "5-nearest neighbor regression",
       subtitle = "Euclidean distance weighting",
       color = "Method")

# estimate test MSE for LM and KNN models
sim_test <- data_frame(x = runif(100, -1,1),
                  y = x_cube(x) + rnorm(100, 0, .2))

mse_lm <- lm(y ~ x, data = sim) %>%
  mse(sim_test)

mse_knn <- data_frame(k = 1:10,
                      knn = map(k, ~ kknn(y ~ x, train = sim, test = sim_test, k = .)),
                      mse = map_dbl(knn, ~ mean((sim_test$y - .$fitted.values)^2)))

ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

## Weighted $K$-nearest neighbors regression

```{r wknn-nonrobust}
sim_pred_wknn <- sim_pred_knn %>%
  mutate(wknn = map2(p, k, ~ kknn(formula(str_c("y ~ ",
                                                str_c("x", seq.int(.x), collapse = " + "))),
                          train = sim_nr, test = sim_nr_test, k = .y)),
         mse_wknn = map_dbl(wknn, ~ mean((sim_nr_test$y - .$fitted.values)^2)))
sim_pred_lm <- sim_pred_wknn %>%
  select(p, k, mse_lm) %>%
  distinct

sim_pred_wknn %>%
  select(p, k, contains("mse"), -mse_lm) %>%
  gather(method, mse, contains("mse")) %>%
  mutate(method = str_replace(method, "mse_", "")) %>%
  mutate(method = factor(method, levels = c("knn", "wknn"),
                         labels = c("KNN", "Weighted KNN"))) %>%
  ggplot(aes(k, mse, color = method)) +
  facet_grid(. ~ p, labeller = labeller(p = label_both)) +
  geom_line() +
  geom_point() +
  geom_hline(data = sim_pred_lm, aes(yintercept = mse_lm), linetype = 2) +
  labs(title = "Test MSE for linear regression vs. KNN",
       subtitle = "Traditional and weighted KNN",
       x = "K",
       y = "Test mean squared error",
       method = NULL) +
  expand_limits(y = 0) +
  theme(legend.position = "bottom")
```

## Estimating KNN on simulated wage data

```{r wage-sim-knn}
# split into train/test set
wage_split <- resample_partition(wage, p = c("test" = .5, "train" = .5))
wage_train <- as_tibble(wage_split$train)
wage_test <- as_tibble(wage_split$test)

# estimate test MSE for LM and KNN models
mse_lm <- lm(wage ~ educ + age + prestige, data = wage_train) %>%
  mse(wage_test)

mse_knn <- data_frame(k = c(1:10, seq(20, 100, by = 10)),
                      knn = map(k, ~ knn.reg(select(wage_train, -wage), y = wage_train$wage,
                         test = select(wage_test, -wage), k = .)),
                      mse = map_dbl(knn, ~ mean((wage_test$wage - .$pred)^2)))

ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(title = "KNN on simulated wage data",
       x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

## KNN on Biden

```{r biden-knn}
biden <- read_csv("../data/biden.csv")

# split into train/test set
biden_split <- resample_partition(biden, p = c("test" = .3, "train" = .7))
biden_train <- as_tibble(biden_split$train)
biden_test <- as_tibble(biden_split$test)

# estimate test MSE for LM and KNN models
mse_lm <- lm(biden ~ ., data = biden_train) %>%
  mse(biden_test)

mse_knn <- data_frame(k = c(1:10, seq(20, 100, by = 10)),
                      knn = map(k, ~ knn.reg(select(biden_train, -biden), y = biden_train$biden,
                         test = select(biden_test, -biden), k = .)),
                      mse = map_dbl(knn, ~ mean((biden_test$biden - .$pred)^2)))

ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(title = "KNN for Biden",
       x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

## Weighted KNN on Biden

```{r biden-wknn}
# estimate test MSE for LM and WKNN models
mse_lm <- lm(biden ~ ., data = biden_train) %>%
  mse(biden_test)

mse_knn <- data_frame(k = c(1:10, seq(20, 100, by = 10)),
                      knn = map(k, ~ kknn(biden ~ .,
                                          train = biden_train, test = biden_test, k = .)),
                      mse = map_dbl(knn, ~ mean((sim_test$y - .$fitted.values)^2)))

ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(title = "Weighted KNN for Biden",
       x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

## Non-linearity of linear models

### Parametric methods

* Linear regression
* Logistic regression
* Generalized linear models (GLMs)
* Polynomial regression
* Step functions

### Non-parametric methods

* Regression splines
* Smoothing splines
* Local regression
* Generalized additive models
* Decision trees
* Bagging/random forest/boosting
* Support vector machines

## Bayes decision rule

$$\Pr(Y = j | X = x_0)$$

## Bayes decision rule

```{r bayes-class}
bayes_rule <- function(x1, x2) {
  x1 + x1^2 + x2 + x2^2
}

bayes_grid <- expand.grid(x1 = seq(-1, 1, by = .05),
            x2 = seq(-1, 1, by = .05)) %>%
  as_tibble %>%
  mutate(logodds = bayes_rule(x1, x2),
         y = logodds > .5,
         prob = logit2prob(logodds))

bayes_bound <- bind_rows(mutate(bayes_grid,
                                prob = prob,
                                cls = TRUE,
                                prob_cls = ifelse(y == cls, 1, 0)),
                         mutate(bayes_grid,
                                prob = prob,
                                cls = FALSE,
                                prob_cls = ifelse(y == cls, 1, 0)))

sim_bayes <- data_frame(x1 = runif(200, -1, 1),
                        x2 = runif(200, -1, 1),
                        logodds = bayes_rule(x1, x2) + rnorm(200, 0, .5),
                        y = logodds > .5,
                        y_actual = bayes_rule(x1, x2) > .5)
sim_bayes_err <- mean(sim_bayes$y != sim_bayes$y_actual)

ggplot(bayes_bound, aes(x1, x2, color = y)) +
  geom_point(size = .5, alpha = .5) +
  geom_contour(aes(z = prob_cls, group = cls), bins = 1) +
  geom_point(data = sim_bayes) +
  theme(legend.position = "none")
```

## Bayes error rule

$$1 - E \left( \max_j \Pr(Y = j | X) \right)$$

## $K$-nearest neighbors classification

$Pr(Y = j| X = x_0) = \frac{1}{K} \sum_{i \in N_0} I(y_i = j)$$

## $K$-nearest neighbors classification

```{r knn-class1, dependson="bayes-class"}
knn1 <- class::knn(select(sim_bayes, x1, x2), test = select(bayes_grid, x1, x2),
                   cl = sim_bayes$y, k = 1, prob = TRUE)
prob1 <- attr(knn1, "prob")

bayes_bound1 <- bind_rows(mutate(bayes_grid,
                           prob = attr(knn1, "prob"),
                           y = as.logical(knn1),
                           cls = TRUE,
                           prob_cls = ifelse(y == cls,
                                           1, 0)),
                    mutate(bayes_grid,
                           prob = attr(knn1, "prob"),
                           y = as.logical(knn1),
                           cls = FALSE,
                           prob_cls = ifelse(y == cls,
                                           1, 0)))

ggplot(bayes_bound, aes(x1, x2, color = y)) +
  geom_contour(aes(z = prob_cls, group = cls, linetype = "True boundary"), bins = 1) +
  geom_contour(data = bayes_bound1, aes(z = prob_cls, group = cls, linetype = "KNN"), bins = 1) +
  geom_point(data = sim_bayes) +
  scale_color_discrete(guide = FALSE) +
  labs(title = "K nearest neighbor classifier",
       subtitle = expression(K==1),
       linetype = NULL) +
  theme(legend.position = "bottom")
```

## $K$-nearest neighbors classification

```{r knn-class5, dependson="bayes-class"}
knn5 <- class::knn(select(sim_bayes, x1, x2), test = select(bayes_grid, x1, x2),
                   cl = sim_bayes$y, k = 5, prob = TRUE)
prob5 <- attr(knn5, "prob")

bayes_bound5 <- bind_rows(mutate(bayes_grid,
                           prob = attr(knn5, "prob"),
                           y = as.logical(knn5),
                           cls = TRUE,
                           prob_cls = ifelse(y == cls,
                                           1, 0)),
                    mutate(bayes_grid,
                           prob = attr(knn5, "prob"),
                           y = as.logical(knn5),
                           cls = FALSE,
                           prob_cls = ifelse(y == cls,
                                           1, 0)))

ggplot(bayes_bound, aes(x1, x2, color = y)) +
  geom_contour(aes(z = prob_cls, group = cls, linetype = "True boundary"), bins = 1) +
  geom_contour(data = bayes_bound5, aes(z = prob_cls, group = cls, linetype = "KNN"), bins = 1) +
  geom_point(data = sim_bayes) +
  scale_color_discrete(guide = FALSE) +
  labs(title = "K nearest neighbor classifier",
       subtitle = expression(K==5),
       linetype = NULL) +
  theme(legend.position = "bottom")
```

## $K$-nearest neighbors classification

```{r knn-class10, dependson="bayes-class"}
knn10 <- class::knn(select(sim_bayes, x1, x2), test = select(bayes_grid, x1, x2),
                   cl = sim_bayes$y, k = 10, prob = TRUE)
prob10 <- attr(knn10, "prob")

bayes_bound10 <- bind_rows(mutate(bayes_grid,
                           prob = attr(knn10, "prob"),
                           y = as.logical(knn5),
                           cls = TRUE,
                           prob_cls = ifelse(y == cls,
                                           1, 0)),
                    mutate(bayes_grid,
                           prob = attr(knn10, "prob"),
                           y = as.logical(knn5),
                           cls = FALSE,
                           prob_cls = ifelse(y == cls,
                                           1, 0)))

ggplot(bayes_bound, aes(x1, x2, color = y)) +
  geom_contour(aes(z = prob_cls, group = cls, linetype = "True boundary"), bins = 1) +
  geom_contour(data = bayes_bound10, aes(z = prob_cls, group = cls, linetype = "KNN"), bins = 1) +
  geom_point(data = sim_bayes) +
  scale_color_discrete(guide = FALSE) +
  labs(title = "K nearest neighbor classifier",
       subtitle = expression(K==10),
       linetype = NULL) +
  theme(legend.position = "bottom")
```

## $K$-nearest neighbors classification

```{r knn-class-compare, dependson="bayes-class"}
# estimate test MSE for KNN models
sim_test <- data_frame(x1 = runif(5000, -1, 1),
                       x2 = runif(5000, -1, 1),
                       logodds = bayes_rule(x1, x2) + rnorm(5000, 0, .5),
                       y = logodds > .5)

mse_knn <- data_frame(k = 1:100,
                      knn_train = map(k, ~ class::knn(select(sim_bayes, x1, x2),
                                                test = select(sim_bayes, x1, x2),
                                                cl = sim_bayes$y, k = .)),
                      knn_test = map(k, ~ class::knn(select(sim_bayes, x1, x2),
                                                test = select(sim_test, x1, x2),
                                                cl = sim_bayes$y, k = .)),
                      mse_train = map_dbl(knn_train, ~ mean(sim_bayes$y != as.logical(.))),
                      mse_test = map_dbl(knn_test, ~ mean(sim_test$y != as.logical(.))))

ggplot(mse_knn, aes(k, mse_test)) +
  geom_line() +
  geom_hline(yintercept = sim_bayes_err, linetype = 2) +
  labs(x = "K",
       y = "Test error rate") +
  expand_limits(y = 0)
```

## Applying KNN to Titanic

```{r titanic-data}
titanic <- titanic::titanic_train %>%
  as_tibble %>%
  select(-Name, -Ticket, -Cabin, -PassengerId, -Embarked) %>%
  mutate(Female = ifelse(Sex == "female", 1, 0)) %>%
  select(-Sex) %>%
  na.omit

titanic_split <- resample_partition(titanic, p = c("test" = .3, "train" = .7))
titanic_train <- as_tibble(titanic_split$train)
titanic_test <- as_tibble(titanic_split$test)
```

```{r titanic-logit}
titanic_logit <- glm(Survived ~ ., data = titanic_train, family = binomial)
titanic_logit_mse <- mse.glm(titanic_logit, titanic_test)
```

```{r titanic-knn-compare, dependson="bayes-class"}
# estimate test MSE for KNN models
mse_knn <- data_frame(k = 1:100,
                      knn_train = map(k, ~ class::knn(select(titanic_train, -Survived),
                                                test = select(titanic_train, -Survived),
                                                cl = titanic_train$Survived, k = .)),
                      knn_test = map(k, ~ class::knn(select(titanic_train, -Survived),
                                                test = select(titanic_test, -Survived),
                                                cl = titanic_train$Survived, k = .)),
                      mse_train = map_dbl(knn_train, ~ mean(titanic_test$Survived != .)),
                      mse_test = map_dbl(knn_test, ~ mean(titanic_test$Survived != .)))

ggplot(mse_knn, aes(k, mse_test)) +
  geom_line() +
  geom_hline(yintercept = titanic_logit_mse, linetype = 2) +
  labs(x = "K",
       y = "Test error rate") +
  expand_limits(y = 0)
```


