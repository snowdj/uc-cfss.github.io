---
title: "Multilevel data"
author: |
  | MACS 30200
  | University of Chicago
date: "May 15, 2017"
output: rcfss::cfss_slides
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE)

library(tidyverse)
library(broom)
library(forcats)
library(modelr)
library(stringr)
library(car)
library(rcfss)
library(coefplot)
library(RColorBrewer)
library(lme4)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal(base_size = 22))
```

## What influences partisan defection in voters?

* Partisan defection
* Potential explanations
    * Partisan intensity
    * Relative favorability
    * Correct is defect
    * Incumbency

## Pooling

* Pooling
* Requirements to pool
* Reasons for pooling
    * Pooling adds data
    * Generalizability
    * Readability

## Issues with pooling

$$
\begin{align}
 Pr(Y_{ei} = 1) &= \text{logit}^{-1}[\alpha + \beta_{1}\text{PID Intensity}_{ei} + \beta_{2}{\text{Relative Favorability}_{ei}} \nonumber \\
 &\: + \beta_{3}\text{Defect is Correct}_{ei}  + \beta_{4}\text{Incumbent Candidate}_{ei}] \nonumber \\
  \text{logit}^{-1}[x] &= \frac{e^x}{1+e^x}
\end{align}
$$

$$\mathbf{E} \in \{1972 ,\dotsc, 2008 \}$$

* Exchangability
    * that the process governing the relationship between $X$ and $Y$ is exactly the same for each $e$,
    * that the process governing the relationship between $X$ and $Y$ is the same for all $i$,
    * that the process governing the $u$s is the same $\forall \: e$ and $i$ as well.
* Violating exchangability assumption
    * Biased estimator

## The error term

$$u_{ij} \sim i.i.d.N(0, \sigma^2), \: \forall\: i,j$$

$$
\begin{align}
Var(u_{ab}) &= Var(u_{cb}) \: \forall \: a \neq b  \text{ (i.e., no cross-unit heteroscedasticity)},  \\
Var(u_{ab}) &= Var(u_{ac}) \: \forall \: b \neq c \text{ (i.e., no temporal heteroscedasticity)},  \\
Cov(u_{ab},u_{cd}) &= 0 \: \forall \: a \neq c,\: \forall \: b \neq d \text{ (i.e., no auto- or spatial correlation)}
\end{align}
$$

* Assumption violations
    * Cross-unit differences
    * Time effects
    * Omitted variables

## Simulation of bad pooling {.scrollable}

$$
\begin{align}
y_{i} &\sim N(\alpha_{j[i]} + \beta x_{i},\sigma^2_y), &\text{for } i=1,\dotsc,n \nonumber \\
\alpha_j &\sim N(\mu_{\alpha},\sigma^2_{\alpha}), &\text{for } j=1,\dotsc,J
\end{align}
$$

```{r sim-pool}
obs <- 60
group.obs <- 6
groups <- rep(1:group.obs, times = obs / group.obs)

#varying intercept
beta <- 2
alpha <- rnorm(group.obs,2,2)

vary_int <- data_frame(x = runif(obs,0,3),
                       y = alpha + beta * x,
                       groups = factor(groups))

p <- ggplot(vary_int, aes(x, y)) +
  geom_point(aes(color = groups, shape = groups)) +
  labs(title = "Simulated data with varying intercepts",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")
p

p +
  geom_smooth(method = "lm") +
  labs(subtitle = "OLS")
```

## Partisan defection and pooling

```{r anes-data}
library(haven)

# read in data
anes <- read_dta("../data/anes_pres.dta")

# generate variables
anes <- anes %>%
  #generate binary party measures
  mutate(rep = ifelse(pid < 4, 0,
                      ifelse(pid > 4, 1, NA))) %>%
  #generate measure of defection and whether or not respondent actually voted correctly
  mutate(defect = NA,
    defect = replace(defect, which((pres_vote_r == 0) &
                                     rep == 0), 0),
    defect = replace(defect, which((pres_vote_r == 1) &
                                     rep == 1), 0),
    defect = replace(defect, which((pres_vote_r == 1 | pres_vote_r == 6) &
                                     rep == 0), 1),
    defect = replace(defect, which((pres_vote_r == 0 | pres_vote_r == 6) &
                                     rep == 1), 1),
    vote_cor_actual = ifelse(pres_vote_r == vote_cor, 1, 0)) %>%
  # does the correct vote require one to defect?
  mutate(defect_cor = ifelse(vote_cor != rep, 1, 0)) %>%
  # generate index of PID intensity
  mutate(pid_abs = abs(pid - 4) - 1) %>%
  # remove 1948
  filter(year != 1948) %>%
  # does feeling thermometer towards candidates explain defection?
  mutate(feel_def = ifelse(rep == 1, (dem_feel - rep_feel) / 2,
                           ifelse(rep == 0, (rep_feel - dem_feel) / 2, NA))) %>%
  #does incumbency influence partisan defection?
  mutate(inc = ifelse(year %in% c(1956, 1964, 1972, 1976,
                                  1980, 1984, 1992, 1996, 2004), 1, 0),
         inc_opp = ifelse((rep == 1 & year %in% c(1964, 1980, 1996)) |
                            (rep == 0 & year %in% c(1956, 1972, 1976,
                                                    1984, 1992, 2004)), 1, 0))
```

```{r prop-defect}
#generate a plot of the percentage of partisan defectors in each election
##get weighted proportions for all, dems only, and reps only
anes %>%
  group_by(year) %>%
  summarize(defect = weighted.mean(defect, std_wt, na.rm = TRUE)) %>%
  ggplot(aes(year, defect)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Percentage of partisan defectors",
       x = "Presidential election year",
       y = "Percentage of partisan defectors")
```

## Multilevel data structures

$$y_{i} \sim N(\alpha + \beta x_{i}, \sigma^2_{y}), \: \text{for}\: i=1,\dotsc,n$$

$$y_{ij} \sim N(\alpha + \beta x_{ij}, \sigma^2_{y}), \: \text{for}\: i=1,\dotsc,n; j = 1, \dotsc,k$$

* Assumes
    * Constant term is constant across different $i$s
    * Effect of any given variable $X$ on $Y$ is constant across observations

## Variable intercepts

$$y_{ij} \sim N(\alpha_i + \beta x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n$$

$$y_{ij} \sim N(\alpha_t + \beta x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n$$

## Variable slopes

$$y_{it} \sim N(\alpha + \beta_{i} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n$$

## Variable slopes and intercepts

$$y_{ij} \sim N(\alpha_{i} + \beta_{i} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n$$

$$y_{ij} \sim N(\alpha_{t} + \beta_{j} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n$$

$$y_{ij} \sim N(\alpha_{it} + \beta_{it} x_{ij}, \sigma^2_{y}), \: for\: i=1,\dotsc,n$$

## Multilevel modeling

* Random effects
* Fixed effects

## Fixed effects

* $J-1$ dummy variables
* When to use it

## Fixed effects

```{r fixed-eff}
basic.fix <- glm(defect ~ pid_abs + feel_def + defect_cor + factor(year),
             data = anes, family = binomial(link = "logit"))
coefplot(basic.fix,
         title = "Fixed effects GLM of partisan defection",
         newNames = c("pid_abs" = "PID intensity",
                      "feel_def" = "Relative favorability",
                      "defect_cor" = "Defect is correct",
                      "inc" = "Incumbent candidate",
                      "factor(year)1976" = "1976",
                      "factor(year)1980" = "1980",
                      "factor(year)1984" = "1984",
                      "factor(year)1988" = "1988",
                      "factor(year)1992" = "1992",
                      "factor(year)1996" = "1996",
                      "factor(year)2000" = "2000",
                      "factor(year)2004" = "2004",
                      "factor(year)2008" = "2008"),
         decreasing = TRUE)
```

## Fixed effects fail {.scrollable}

```{r fixef-collin}
summary(glm(defect ~ pid_abs + feel_def + defect_cor + inc + factor(year),
            data = anes, family = binomial(link = "logit")))
```

```{r year-inc}
ggplot(anes, aes(year, inc)) +
  geom_point() +
  labs(title = "Perfect prediction",
       x = "Election year",
       y = "Incumbent in election")

# correlation coefficient, by year
anes %>%
  group_by(year) %>%
  summarize(cor = cor(year, inc))
```

## Random effects

* Complete pooling
* No pooling
* Partial pooling
* Soft constraint
    * Probability distribution
    
## Varying intercepts

$$
\begin{align}
y_i &\sim N(\alpha_{j[i]} + \beta x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
\alpha_{j} &\sim N(\mu_{\alpha},\sigma^2_{\alpha}), \: \text{for} \: j=1,\dotsc,J,
\end{align}
$$

* Partial-pooling
    * $\sigma_{\alpha} \to \infty$
    * $\sigma_{\alpha} \to 0$
* Reduces number of parameters

----

```{r sim-vary-int}
#draw x and c from random normal distribution, adjusting for group differences
x <- runif(obs, 0, 3) + groups
c <- rep(rnorm(group.obs, 2, 1), times = obs / group.obs) * groups
beta <- 2		#set beta
y <- beta*(x) - c #+ rnorm(obs,0,1)		#generate y from x, c, beta, and random noise

sim_data_mlm <- data_frame(x, y, groups)

# plot data
ggplot(sim_data_mlm, aes(x, y)) +
  geom_point(aes(color = factor(groups), shape = factor(groups))) +
  geom_smooth(aes(color = factor(groups)), method = "lm", se = FALSE,
              fullrange = TRUE, size = .5) +
  geom_smooth(method = "lm") +
  labs(title = "Simulated data with varying intercepts",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")
```

## Adding group-level predictors

$$
\begin{align}
y_i &\sim N(\alpha_{j[i]} + \beta x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
\alpha_{j} &\sim N(\gamma_{0} + \gamma_{1}u_{j},\sigma^2_{\alpha}), \: \text{for} \: j=1,\dotsc,J,
\end{align}
$$

* Interpreting $\gamma$s

## Varying slopes {.scrollable}

$$
\begin{align}
y_i &\sim N(\alpha_{j[i]} + \beta_{j[i]} x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
 \begin{pmatrix}
 \alpha_{j} \\
 \beta_{j} \\
 \end{pmatrix} &\sim N\left(\begin{pmatrix}
 \mu_{\alpha} \\
 \mu_{\beta} \\
 \end{pmatrix},\begin{pmatrix}
 \sigma^2_{\alpha} & \rho \sigma_{\alpha} \sigma_{\beta} \\
 \rho \sigma_{\alpha} \sigma_{\beta} & \sigma^{2}_{\beta} & \\
 \end{pmatrix}\right), \: \text{for} \: j=1,\dotsc,J,
\end{align}
$$

```{r sim-vary-slope}
# constant intercept
alpha <- 0
beta <- rnorm(group.obs, 2, 1)
y <- alpha + beta * x

sim_data_mlm <- data_frame(x, y, groups)

# plot data
ggplot(sim_data_mlm, aes(x, y)) +
  geom_point(aes(color = factor(groups), shape = factor(groups))) +
  geom_smooth(aes(color = factor(groups)), method = "lm", se = FALSE,
              fullrange = TRUE, size = .5) +
  geom_smooth(method = "lm") +
  labs(title = "Simulated data with varying slopes",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")

# varying intercept
alpha <- rnorm(group.obs,2,2)
y <- alpha + beta * x

sim_data_mlm <- data_frame(x, y, groups)

# plot data
ggplot(sim_data_mlm, aes(x, y)) +
  geom_point(aes(color = factor(groups), shape = factor(groups))) +
  geom_smooth(aes(color = factor(groups)), method = "lm", se = FALSE,
              fullrange = TRUE, size = .5) +
  geom_smooth(method = "lm") +
  labs(title = "Simulated data with varying intercepts and slopes",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")
```

## Varying slopes with group-level predictors

$$
\begin{align}
y_i &\sim N(\alpha_{j[i]} + \beta_{j[i]} x_{i}, \sigma^2_{y}), \: \text{for} \: i=1,\dotsc,n, \nonumber \\
 \begin{pmatrix}
 \alpha_{j} \\
 \beta_{j} \\
 \end{pmatrix} &\sim N\left(\begin{pmatrix}
 \gamma^{\alpha}_{0} + \gamma^{\alpha}_{1} u_{j} \\
 \gamma^{\beta}_{0} + \gamma^{\beta}_{1} u_{j} \\
 \end{pmatrix},\begin{pmatrix}
 \sigma^2_{\alpha} & \rho \sigma_{\alpha} \sigma_{\beta} \\
 \rho \sigma_{\alpha} \sigma_{\beta} & \sigma^{2}_{\beta} & \\
 \end{pmatrix}\right), \: \text{for} \: j=1,\dotsc,J,
\end{align}
$$

## Other specifications of MLM

* Three or more levels of data
* Repeated measures (i.e. panel data)
* Time-series cross sections
* Non-nested structures
* Varying slopes without varying intercepts
* Cross-level interactions
* Discrete and non-normally distributed outcomes
* Bayesian estimation procedures
* Measuring sample size and power calculations
* Multilevel regression, imputation, and post-stratification (MRP)

## Partisan defection

$$
\begin{align}
 Pr(Y_{ei} = 1) &= \text{logit}^{-1}[\alpha + \beta_{1}\text{PID Intensity}_{ei} + \beta_{2}{\text{Relative Favorability}_{ei}} \nonumber \\
 &\: + \beta_{3}\text{Defect is Correct}_{ei}  + \beta_{4}\text{Incumbent Candidate}_{e}] \nonumber \\
  \text{logit}^{-1}[x] &= \frac{e^x}{1+e^x}
\end{align}
$$

## Classical logistic regression

```{r anes-logit}
basic <- glm(defect ~ pid_abs + feel_def + defect_cor + inc,
             data = anes, family = binomial(link = "logit"))
coefplot(basic,
         title = "Classical GLM of partisan defection",
         newNames = c("pid_abs" = "PID intensity",
                      "feel_def" = "Relative favorability",
                      "defect_cor" = "Defect is correct",
                      "inc" = "Incumbent candidate"),
         decreasing = TRUE)
```

## Varying intercept

$$
\begin{align}
 Pr(Y_{ei} = 1) &= \text{logit}^{-1}[\alpha_{e[i]} + \beta_{1}\text{PID Intensity}_{ei} \nonumber \\
 &\quad + \beta_{2}{\text{Relative Favorability}_{ei}} + \beta_{3}\text{Defect is Correct}_{ei}]  \nonumber \\
 \alpha_{e} &\sim N(\gamma_{0}^{\alpha} + \gamma_{1}^{\alpha}\text{Incumbent},\sigma^2_{\alpha}), \: \text{for} \: e=1976,\dotsc,2008, \nonumber \\
 \text{logit}^{-1}[x] &= \frac{e^x}{1+e^x}
\end{align}
$$

## Varying intercept {.scrollable}

```{r anes-vary-int}
vary.int <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc + (1|year),
                  data = anes,
                  family = binomial(link = "logit"))
tidy(vary.int)
summary(vary.int)

multiplot(basic, vary.int,
         title = "Regression models of partisan defection",
         newNames = c("pid_abs" = "PID intensity",
                      "feel_def" = "Relative favorability",
                      "defect_cor" = "Defect is correct",
                      "inc" = "Incumbent candidate"),
         names = c("Classical GLM", "Varying intercept"),
         decreasing = TRUE) +
  theme(legend.position = "bottom")
```

## Group-specific parameters

```{r vary-int-coef}
coef(vary.int)
```

## Varying slope(s) and intercept

* When to use varying slopes
* Log-likelihood (ANOVA) test

```{r lrtest}
defect <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc + (1|year),
                data = anes, family = binomial(link = "logit"))
defect.pid3.rc1 <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc +
                           (1|year) + (pid_abs|year),
                         data = anes, family = binomial(link = "logit"))
defect.pid3.test <- anova(defect, defect.pid3.rc1)
tidy(defect.pid3.test)
```

```{r anova-tests, eval = FALSE}
defect <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc + (1|year),
                data = anes, family = binomial(link = "logit"))
defect.xfeel <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc +
                        pid_abs * feel_def + (1|year),
                      data = anes, family = binomial(link = "logit"))
defect.xdef <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc +
                       pid_abs * defect_cor + (1|year),
                     data = anes, family = binomial(link = "logit"))
defect.xinc <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc +
                       pid_abs * inc_opp + (1|year),
                     data = anes, family = binomial(link = "logit"))
defect.cor.rc1 <- glmer(defect ~ pid_abs + feel_def + defect_cor +
                          inc + (1|year) + (defect_cor|year),
                        data = anes, family = binomial(link = "logit"))
defect.pid3.rc1 <- glmer(defect ~ pid_abs + feel_def + defect_cor +
                           inc + (1|year) + (pid_abs|year),
                         data = anes, family = binomial(link = "logit"))
defect.feel.rc1 <- glmer(defect ~ pid_abs + feel_def + defect_cor +
                           inc + (1|year) + (feel_def|year),
                         data = anes, family = binomial(link = "logit"))
defect.inc.rc1 <- glmer(defect ~ pid_abs + feel_def + defect_cor +
                          inc + (1|year) + (inc|year),
                        data = anes, family = binomial(link = "logit"))
  
# likelihood ratio tests to compare model fit
anova(defect, defect.xfeel)
anova(defect, defect.xdef)
anova(defect, defect.xinc)
anova(defect, defect.cor.rc1)
anova(defect, defect.pid3.rc1)
anova(defect, defect.feel.rc1)
anova(defect, defect.inc.rc1)
```

## Final model {.scrollable}

$$
\begin{align}
 Pr(Y_{ei} = 1) &= \text{logit}^{-1}[\alpha_{e[i]} + \beta_{1}\text{PID Intensity}_{ei} \nonumber \\
 &\quad + \beta_{2e[i]}{\text{Relative Favorability}_{ei}} + \beta_{3}\text{Defect is Correct}_{ei}]  \nonumber \\
 \begin{pmatrix}
 \alpha_{e} \\
 \beta_{2e} \\
 \end{pmatrix} &\sim N\left(\begin{pmatrix}
 \gamma_{0}^{\alpha} + \gamma_{1}^{\alpha}\text{Incumbent} \\
 \gamma_{0}^{\beta_{2}} \\
 \end{pmatrix},\begin{pmatrix}
 \sigma^2_{\alpha} & \rho\sigma_{\alpha}\sigma_{\beta_2}  \\
 \rho\sigma_{\alpha}\sigma_{\beta_2} & \sigma^{2}_{\beta_2}  \\
 \end{pmatrix}\right) \nonumber \\
  \text{logit}^{-1}[x] &= \frac{e^x}{1+e^x}
\end{align}
$$

```{r final}
final <- glmer(defect ~ pid_abs + feel_def + defect_cor + inc +
                 (1|year) + (feel_def|year),
               data = anes, family = binomial(link = "logit"))
tidy(final)

multiplot(basic, vary.int, final,
         title = "Regression models of partisan defection",
         newNames = c("pid_abs" = "PID intensity",
                      "feel_def" = "Relative favorability",
                      "defect_cor" = "Defect is correct",
                      "inc" = "Incumbent candidate"),
         names = c("Classical GLM", "Varying intercept", "Varying slopes"),
         decreasing = TRUE) +
  theme(legend.position = "bottom")

coef(final)
```

## Varying slope uncertainty {.scrollable}

```{r final-sim-se}
# generate posterior simulations of beta to estimate group-specific standard errors
sim.final <- arm::sim(final, n.sims = 1000)

se.fix <- apply(sim.final@fixef, 2, function(x) sd(x, na.rm = TRUE))
se.fix <- matrix(rep(se.fix,times=10),nrow=nrow(coef(final)$year),ncol=length(se.fix),byrow=TRUE)	#expand to matrix for addition with random se

se.ran <- matrix(NA, nrow = nrow(coef(final)$year), ncol = ncol(ranef(final)$year))
se.ran[, 1] <- apply(sim.final@ranef[[1]], 2, function(x) sd(x, na.rm = TRUE))
se.ran[, 2] <- apply(sim.final@ranef[[2]][,,1], 2, function(x) sd(x, na.rm = TRUE))
se.ran[, 3] <- apply(sim.final@ranef[[2]][,,2], 2, function(x) sd(x, na.rm = TRUE))

se.ran <- cbind(se.ran[, 1] + se.ran[, 2], 0, se.ran[, 3], 0, 0)
se.final <- se.fix + se.ran
```

```{r final-by-year}
final_year <- anes %>%
  filter(year >= 1972) %>%
  group_by(year) %>%
  nest() %>%
  mutate(model = map(data, ~ glm(defect ~ pid_abs + feel_def + defect_cor + inc,
               data = .x, family = binomial(link = "logit"))),
         coef = map(model, tidy))

# get intercept and standard errors for MLM into a tidy data frame
mlm_int <- data_frame(year = rownames(coef(final)$year),
           term = "(Intercept)",
           estimate = coef(final)$year[["(Intercept)"]],
           std.error = se.final[, 1],
           method = "MLM")

# extract intercepts and standard errors
glm_int <- final_year %>%
  unnest(coef) %>%
  mutate(method = "Separate GLM",
         year = as.character(year)) %>%
  filter(term == "(Intercept)")

# join together
bind_rows(glm_int,
          mlm_int,
          tidy(basic) %>%
            mutate(year = "Overall",
                   method = "Separate GLM"),
          tidy(final) %>%
            mutate(year = "Overall",
                   method = "MLM")) %>%
  filter(term == "(Intercept)") %>%
  ggplot(aes(fct_rev(year), estimate, color = method)) +
  geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                      ymax = estimate + 1.96 * std.error),
                  position = position_dodge(.75)) +
  geom_linerange(aes(ymin = estimate - .67 * std.error,
                     ymax = estimate + .67 * std.error),
                 position = position_dodge(.75),
                 size = 1) +
  geom_hline(yintercept = 0) +
  coord_flip() +
  labs(title = "Election-specific estimates of the intercept",
       y = "Estimated coefficient with 50% and 95% CI",
       x = NULL,
       color = NULL) +
  theme(legend.position = "bottom")

# feel_def plot now
# get intercept and standard errors for MLM into a tidy data frame
mlm_feel_def <- data_frame(year = rownames(coef(final)$year),
                           term = "feel_def",
                           estimate = coef(final)$year[["feel_def"]],
                           std.error = se.final[, 2],
                           method = "MLM")

glm_feel_def <- final_year %>%
  unnest(coef) %>%
  mutate(method = "Separate GLM",
         year = as.character(year)) %>%
  filter(term == "feel_def")

bind_rows(glm_feel_def,
          mlm_feel_def,
          tidy(basic) %>%
            mutate(year = "Overall",
                   method = "Separate GLM"),
          tidy(final) %>%
            mutate(year = "Overall",
                   method = "MLM")) %>%
  filter(term == "feel_def") %>%
  ggplot(aes(fct_rev(year), estimate, color = method)) +
  geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                      ymax = estimate + 1.96 * std.error),
                  position = position_dodge(.75)) +
  geom_linerange(aes(ymin = estimate - .67 * std.error,
                     ymax = estimate + .67 * std.error),
                 position = position_dodge(.75),
                 size = 1) +
  geom_hline(yintercept = 0) +
  coord_flip() +
  labs(title = "Election-specific estimates of relative favorability",
       y = "Estimated coefficient with 50% and 95% CI",
       x = NULL,
       color = NULL) +
  theme(legend.position = "bottom")
```

## Limitations of MLM

* Likelihood maximization (MLE)
    * Linear vs. non-linear model
    * Restricted estimation of maximum likelihood
* Bayesian estimation procedures
* Determining which coefficients should receive varying slopes
