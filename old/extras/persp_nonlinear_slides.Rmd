---
title: "Statistical learning: moving beyond linearity"
author: |
  | MACS 30100
  | University of Chicago
date: "February 22, 2017"
output: rcfss::cfss_slides
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE)

library(tidyverse)
library(modelr)
library(broom)
library(rcfss)
library(titanic)
library(knitr)
library(splines)
library(ISLR)
library(lattice)
library(gam)

set.seed(1234)
options(digits = 3)
theme_set(theme_minimal(base_size = 18))
```

## Linearity in linear models

* Linear assumption
* Why this is wrong
* When to relax the assumption

## Linearity of the data

$$Y = 2 + 3X + \epsilon, \epsilon \sim N(0,3)$$

```{r sim-linear}
sim_linear <- data_frame(x = runif(100, 0, 10),
                         y = 2 + 3 * x + rnorm(100, 0, 3))
sim_linear_mod <- glm(y ~ x, data = sim_linear)

ggplot(sim_linear, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Linear model for a linear relationship")
```

## Linearity of the data

```{r sim-linear-resid, dependson = "sim-linear"}
sim_linear_pred <- sim_linear %>%
  add_predictions(sim_linear_mod) %>%
  add_residuals(sim_linear_mod)

# distribution of residuals
ggplot(sim_linear_pred, aes(resid)) +
  geom_histogram(aes(y = stat(density))) +
  stat_function(fun = dnorm,
                args = list(mean = mean(sim_linear_pred$resid),
                            sd = sd(sim_linear_pred$resid))) +
  labs(title = "Linear model for a linear relationship",
       x = "Residuals")
```

## Linearity of the data

```{r}
# predicted vs. residuals
ggplot(sim_linear_pred, aes(pred, resid)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(title = "Linear model for a linear relationship",
       x = "Predicted values",
       y = "Residuals")
```

## Non-linearity

$$Y = 2 + 3X + 2X^2 + \epsilon$$

```{r sim-nonlinear}
sim_nonlinear <- data_frame(x = runif(100, 0, 10),
                         y = 2 + 3 * x + 2 * x^2 + rnorm(100, 0, 3))
sim_nonlinear_mod <- glm(y ~ x, data = sim_nonlinear)

ggplot(sim_nonlinear, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Linear model for a non-linear relationship")
```

## Non-linearity

```{r sim-nonlinear-resid, dependson = "sim-nonlinear"}
sim_nonlinear_pred <- sim_nonlinear %>%
  add_predictions(sim_nonlinear_mod) %>%
  add_residuals(sim_nonlinear_mod)

# distribution of residuals
ggplot(sim_nonlinear_pred, aes(resid)) +
  geom_histogram(aes(y = stat(density))) +
  stat_function(fun = dnorm,
                args = list(mean = mean(sim_nonlinear_pred$resid),
                            sd = sd(sim_nonlinear_pred$resid))) +
  labs(title = "Linear model for a non-linear relationship",
       x = "Residuals")
```

## Non-linearity

```{r}
# predicted vs. residuals
ggplot(sim_nonlinear_pred, aes(pred, resid)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(title = "Linear model for a non-linear relationship",
       x = "Predicted values",
       y = "Residuals")
```

## Non-constant variance of the error terms

$$\epsilon_i = \sigma^2$$

* Homoscedasticity
* Heteroscedasticity

## Homoscedastic

$$Y = 2 + 3X + \epsilon, \epsilon \sim N(0,1)$$

```{r sim-homo}
sim_homo <- data_frame(x = runif(1000, 0, 10),
                       y = 2 + 3 * x + rnorm(1000, 0, 1))
sim_homo_mod <- glm(y ~ x, data = sim_homo)

sim_homo %>%
  add_predictions(sim_homo_mod) %>%
  add_residuals(sim_homo_mod) %>%
  ggplot(aes(pred, resid)) +
  geom_point(alpha = .2) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_quantile(method = "rqss", lambda = 5, quantiles = c(.05, .95)) +
  labs(title = "Homoscedastic variance of error terms",
       x = "Predicted values",
       y = "Residuals")
```

## Heteroscedastic

$$Y = 2 + 3X + \epsilon, \epsilon \sim N(0,\frac{X}{2})$$

```{r sim-hetero}
sim_hetero <- data_frame(x = runif(1000, 0, 10),
                       y = 2 + 3 * x + rnorm(1000, 0, (x / 2)))
sim_hetero_mod <- glm(y ~ x, data = sim_hetero)

sim_hetero %>%
  add_predictions(sim_hetero_mod) %>%
  add_residuals(sim_hetero_mod) %>%
  ggplot(aes(pred, resid)) +
  geom_point(alpha = .2) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_quantile(method = "rqss", lambda = 5, quantiles = c(.05, .95)) +
  labs(title = "Heteroscedastic variance of error terms",
       x = "Predicted values",
       y = "Residuals")
```


## How to relax the assumption

* Monotonic transformations
* Step functions
* Splines
* Local regression
* Generalized additive models (GAMs)

## Ladder of powers

Transformation | Power | $f(X)$
---------------|-------|--------
Cube | 3 | $X^3$
Square | 2 | $X^2$
Identity | 1 | $X$
Square root | $\frac{1}{2}$ | $\sqrt{X}$
Cube root | $\frac{1}{3}$ | $\sqrt[3]{X}$
Log | 0 (sort of) | $\ln(X)$

## Ladder of powers

```{r power-ladder}
data_frame(x = runif(1000, 0, 10),
           cube = x^3,
           square = x^2,
           identity = x,
           sqrt = sqrt(x),
           cubert = x ^ (1/3),
           log = log(x)) %>%
  gather(transform, value, -x) %>%
  mutate(transform = factor(transform,
                            levels = c("cube", "square", "identity", "sqrt", "cubert", "log"),
                            labels = c("X^3", "X^2", "X", "sqrt(X)", "sqrt(X, 3)", "ln(X)"))) %>%
  ggplot(aes(x, value)) +
  geom_line() +
  facet_wrap( ~ transform, scales = "free_y", labeller = label_parsed) +
  labs(title = "Ladder of powers transformations",
       x = "X",
       y = "Transformed X")
```

## The Bulging Rule

```{r bulge-rule, fig.asp = 1}
# from http://freakonometrics.hypotheses.org/14967
fakedataMT <- function(p = 1, q = 1, n = 500, s = .1) {
  X <- seq(1 / (n + 1), 1 - 1 / (n + 1), length = n)
  Y <- (5 + 2 * X ^ p + rnorm(n, sd = s)) ^ (1 / q)
  return(data_frame(x = X, y = Y))
}

bind_rows(`1` = fakedataMT(p = .5, q = 2),
          `2` = fakedataMT(p = 3, q = -5),
          `3` = fakedataMT(p = .5, q = -1),
          `4` = fakedataMT(p = 3, q = 5),
          .id = "id") %>%
  mutate(id = factor(id, levels = 1:4,
                     labels = c("Log X or Square Y", "Square X or Y",
                                "Log X or Y", "Square X or Log Y"))) %>%
  ggplot(aes(x, y)) +
  geom_point() +
  facet_wrap(~ id, scales = "free_y") +
  labs(title = 'Tukey and Mosteller\'s "Bulging Rule" for monotone transformations to linearity',
       x = "X",
       y = "Y") +
  theme_bw() +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank(),
        panel.grid = element_blank())
```

## Log transformations

* One-sided transformation of $Y$

    $$\ln(Y_i) = \beta_0 + \beta_{1}X_i + \epsilon_i$$

    $$E(Y) = e^{\beta_0 + \beta_{1}X_i}$$

    $$\frac{\vartheta E(Y)}{\vartheta X} = e^{\beta_1}$$

* One-sided transformation of $X$

    $$Y_i = \beta_0 + \beta_{1} \ln(X_i) + \epsilon_i$$

## Log-log regressions

$$\ln(Y_i) = \beta_0 + \beta_{1} \ln(X_i) + \dots + \epsilon_i$$

* Elasticity

    $$\text{Elasticity}_{YX} = \frac{\% \Delta Y}{\% \Delta X}$$

* A direct means of interpreting a nonlinear effect
* A double multiplicative relationship

## Polynomial regressions

$$y_i = \beta_0 + \beta_{1}x_{i} + \epsilon_{i}$$

$$y_i = \beta_0 + \beta_{1}x_{i} + \beta_{2}x_i^2 + \beta_{3}x_i^3 + \dots + \beta_{d}x_i^d + \epsilon_i$$

## Biden and age

$$\text{Biden}_i = \beta_0 + \beta_1 \text{Age} + \beta_2 \text{Age}^2 + \beta_3 \text{Age}^3 + \beta_4 \text{Age}^4$$

```{r biden-age}
# get data
biden <- read_csv("../data/biden.csv")

# estimate model
biden_age <- glm(biden ~ I(age^1) + I(age^2) + I(age^3) + I(age^4), data = biden)

# estimate the predicted values and confidence interval
biden_pred <- augment(biden_age, newdata = data_grid(biden, age)) %>%
  mutate(pred_low = .fitted - 1.96 * .se.fit,
         pred_high = .fitted + 1.96 * .se.fit)

# plot the curve
ggplot(biden_pred, aes(age)) +
  geom_point(data = biden, aes(age, biden), alpha = .05) +
  geom_line(aes(y = .fitted)) +
  geom_line(aes(y = pred_low), linetype = 2) +
  geom_line(aes(y = pred_high), linetype = 2) +
  labs(title = "Polynomial regression of Biden feeling thermometer",
       subtitle = "With 95% confidence interval",
       x = "Age",
       y = "Predicted Biden thermometer rating")
```

## Variance-covariance matrix

```{r biden-matrix}
vcov(biden_age) %>%
  kable(caption = "Variance-covariance matrix of Biden polynomial regression",
        digits = 5)
```

## Pointwise standard errors

$$\hat{f}(x_0) = \hat{\beta}_0 + \hat{\beta}_1 x_{0} + \hat{\beta}_2 x_{0}^2 + \hat{\beta}_3 x_{0}^3 + \hat{\beta}_4 x_{0}^4$$

$$\text{Var}(\hat{f}(x_o))$$

## Voter turnout and mental health

$$\Pr(\text{Voter turnout} = \text{Yes} | \text{mhealth}) = \frac{\exp[\beta_0 + \beta_1 \text{mhealth} + \beta_2 \text{mhealth}^2 + \beta_3 \text{mhealth}^3 + \beta_4 \text{mhealth}^4]}{1 + \exp[\beta_0 + \beta_1 \text{mhealth} + \beta_2 \text{mhealth}^2 + \beta_3 \text{mhealth}^3 + \beta_4 \text{mhealth}^4]}$$

```{r mhealth}
# load data
mh <- read_csv("../data/mental_health.csv")

# estimate model
mh_mod <- glm(vote96 ~ poly(mhealth_sum, 4, raw = TRUE), data = mh,
              family = binomial)

# estimate the predicted values and confidence interval
mh_pred <- augment(mh_mod, newdata = data_grid(mh, mhealth_sum)) %>%
  rename(pred = .fitted) %>%
  mutate(pred_low = pred - 1.96 * .se.fit,
         pred_high = pred + 1.96 * .se.fit) 

# plot the log-odds curve
ggplot(mh_pred, aes(mhealth_sum, pred, ymin = pred_low, ymax = pred_high)) +
  geom_point() +
  geom_errorbar() +
  labs(title = "Polynomial regression of voter turnout",
       subtitle = "With 95% confidence interval",
       x = "Mental health score",
       y = "Predicted log-odds of voter turnout")
```

## Voter turnout and mental health

```{r}
# plot the probability curve
mh_pred %>%
  mutate_each(funs(logit2prob), starts_with("pred")) %>%
  ggplot(aes(mhealth_sum, pred, ymin = pred_low, ymax = pred_high)) +
  geom_point() +
  geom_errorbar() +
  labs(title = "Polynomial regression of voter turnout",
       subtitle = "With 95% confidence interval",
       x = "Mental health score",
       y = "Predicted probability of voter turnout")
```

## Step functions

* Global structure
* Local structure
* Step functions
* Binning

    $$y_i = \beta_0 + \beta_1 C_1 (x_i) + \beta_2 C_2 (x_i) + \dots + \beta_K C_K (x_i) + \epsilon_i$$

## Biden and age

```{r biden-step}
# estimate model
biden_step <- glm(biden ~ cut(age, 5), data = biden)

# estimate the predicted values and confidence interval
biden_step_pred <- augment(biden_step, newdata = data_grid(biden, age)) %>%
  mutate(pred_low = .fitted - 1.96 * .se.fit,
         pred_high = .fitted + 1.96 * .se.fit)

# plot the curve
ggplot(biden_step_pred, aes(age)) +
  geom_point(data = biden, aes(age, biden), alpha = .05) +
  geom_line(aes(y = .fitted)) +
  geom_line(aes(y = pred_low), linetype = 2) +
  geom_line(aes(y = pred_high), linetype = 2) +
  labs(title = "Piecewise constant regression of Biden feeling thermometer",
       subtitle = "With 95% confidence interval",
       x = "Age",
       y = "Predicted Biden thermometer rating")
```

## Regression splines

* Extend monotonic transformations and piecewise constant regression by fitting separate polynomial functions over different regions of $X$

## Piecewise polynomials

$$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$$

* Piecewise cubic polynomial with 0 knots

    $$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$$

* Piecewise constant polynomial (degree $0$)
* Piecewise cubic polynomial with 1 knot

    $$y_i = \begin{cases} 
      \beta_{01} + \beta_{11}x_i^2 + \beta_{21}x_i^2 + \beta_{31}x_i^3 + \epsilon_i & \text{if } x_i < c \\
      \beta_{02} + \beta_{12}x_i^2 + \beta_{22}x_i^2 + \beta_{32}x_i^3 + \epsilon_i & \text{if } x_i \geq c
   \end{cases}$$
   
## Piecewise polynomials

```{r sim-piecewise}
# simulate data
sim_piece <- data_frame(x = runif(100, 0, 10),
                        y = ifelse(x < 5,
                                   .05 * x + .05 * x^2 + .05 * x^3 + rnorm(100, 0, 3),
                                   .1 * x + .1 * x^2 - .05 * x^3 + rnorm(100, 0, 3)))

# estimate models
sim_piece_mod1 <- glm(y ~ poly(x, 3, raw = TRUE), data = sim_piece, subset = x < 5)
sim_piece_mod2 <- glm(y ~ poly(x, 3, raw = TRUE), data = sim_piece, subset = x >= 5)

# draw the plot
sim_piece %>%
  data_grid(x = seq(0, 10, by = .001)) %>%
  gather_predictions(sim_piece_mod1, sim_piece_mod2) %>%
  filter((x < 5 & model == "sim_piece_mod1") |
           (x >=5 & model == "sim_piece_mod2")) %>%
  ggplot(aes(x, y)) +
  geom_point(data = sim_piece) +
  geom_line(aes(y = pred, color = model), size = 1) +
  geom_vline(xintercept = 5, linetype = 2, color = "grey") +
  labs(title = "Piecewise cubic regression",
       x = "X",
       y = "Y") +
  theme(legend.position = "none")
```

## Constraints and splines

```{r sim-spline}
###### very hackish implementation
# simulate data
sim_piece_cont <- sim_piece %>%
  mutate(y = ifelse(x < 5,
                    15 + .05 * x - .5 * x^2 - .05 * x^3,
                    .05 * x + .1 * x^2 - .05 * x^3))

# estimate models
sim_piece_cont_mod1 <- glm(y ~ poly(x, 3, raw = TRUE), data = sim_piece_cont, subset = x < 5)
sim_piece_cont_mod2 <- glm(y ~ poly(x, 3, raw = TRUE), data = sim_piece_cont, subset = x >= 5)

# draw the plot
sim_piece_cont %>%
  data_grid(x = seq(0, 10, by = .001)) %>%
  gather_predictions(sim_piece_cont_mod1, sim_piece_cont_mod2) %>%
  filter((x < 5 & model == "sim_piece_cont_mod1") |
           (x >=5 & model == "sim_piece_cont_mod2")) %>%
  ggplot() +
  geom_point(data = sim_piece %>%
               mutate(y = ifelse(x < 5,
                                 15 + .05 * x - .5 * x^2 - .05 * x^3 + rnorm(100, 0, 3),
                                 .05 * x + .1 * x^2 - .05 * x^3) + rnorm(100, 0, 3)), aes(x, y)) +
  geom_line(aes(x, pred, color = model), size = 1) +
  geom_vline(xintercept = 5, linetype = 2, color = "grey") +
  labs(title = "Continuous piecewise cubic regression",
       x = "X",
       y = "Y") +
  theme(legend.position = "none")
```

## Constraints and splines

```{r sim-spline-smooth}
# estimate models
sim_piece_smooth <- glm(y ~ bs(x, knots = c(5)), data = sim_piece)

# draw the plot
sim_piece %>%
  add_predictions(sim_piece_smooth) %>%
  ggplot(aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), size = 1) +
  geom_vline(xintercept = 5, linetype = 2, color = "grey") +
  labs(title = "Cubic spline",
       x = "X",
       y = "Y") +
  theme(legend.position = "none")
```

## Constraints and splines

```{r sim-spline-smooth-5}
data_frame(terms = c(1, 5, 10),
           models = map(terms, ~ glm(y ~ bs(x, df = . + 3), data = sim_piece)),
           pred = map(models, ~ add_predictions(sim_piece, .))) %>%
  unnest(pred) %>%
  ggplot(aes(x, y)) +
  geom_point(data = sim_piece, alpha = .2) +
  geom_line(aes(y = pred, color = factor(terms))) +
  labs(title = "Cubic spline",
       x = "X",
       y = "Y",
       color = "Knots")
```

## Choosing the number and location of knots

```{r biden-spline}
# estimate model
biden_spline <- glm(biden ~ bs(age, df = 8), data = biden)

# estimate the predicted values and confidence interval
biden_spline_pred <- augment(biden_spline, newdata = data_grid(biden, age)) %>%
  mutate(pred_low = .fitted - 1.96 * .se.fit,
         pred_high = .fitted + 1.96 * .se.fit)

# plot the curve
ggplot(biden_spline_pred, aes(age)) +
  # geom_point(data = biden, aes(age, biden), alpha = .05) +
  geom_line(aes(y = .fitted)) +
  geom_line(aes(y = pred_low), linetype = 2) +
  geom_line(aes(y = pred_high), linetype = 2) +
  geom_vline(xintercept = attr(bs(biden$age, df = 8), "knots"),
             linetype = 2, color = "blue") +
  labs(title = "Cubic spline regression of Biden feeling thermometer",
       subtitle = "5 knots, with 95% confidence interval",
       x = "Age",
       y = "Predicted Biden thermometer rating")
```

## Choosing the number and location of knots

```{r wage}
wage <- as_tibble(Wage)

ggplot(wage, aes(age, wage)) +
  geom_point(alpha = .2) +
  labs(x = "Age",
       y = "Wage")
```

## $10$-fold CV MSE

```{r wage-cv}
# function to simplify things
wage_spline_cv <- function(data, degree = 3, df = NULL){
  # estimate the model on each fold
  models <- map(data$train, ~ glm(wage ~ bs(age, df = df, degree = degree),
                                  data = .))
  
  # calculate mse for each test fold
  models_mse <- map2_dbl(models, data$test, mse)
  
  return(mean(models_mse, na.rm = TRUE))
}

# fold the data
wage_kfold <- crossv_kfold(wage, k = 10)

# estimate mse for polynomial degrees in 1:10
wage_degree_mse <- data_frame(degrees = 1:10,
                              mse = map_dbl(degrees, ~ wage_spline_cv(wage_kfold, degree = .,
                                                                      df = 3 + .)))

# estimate mse for degrees of freedom (aka knots)
wage_df_mse <- data_frame(df = 1:10,
                          mse = map_dbl(df, ~ wage_spline_cv(wage_kfold, df = 3 + .)))

# graph the results
ggplot(wage_degree_mse, aes(degrees, mse)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 1:10) +
  labs(title = "Optimal number of degrees for wage spline regression",
       subtitle = "Knots = 3",
       x = "Highest-order polynomial",
       y = "10-fold CV MSE")
```

## $10$-fold CV MSE

```{r}
ggplot(wage_df_mse, aes(df, mse)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 1:10) +
  labs(title = "Optimal number of knots for wage spline regression",
       subtitle = "Highest-order polynomial = 3",
       x = "Knots",
       y = "10-fold CV MSE")
```

## Optimal model

```{r wage-optimal-mod}
wage_optim <- glm(wage ~ bs(age, df = 8, degree = 6), data = wage)

augment(wage_optim, newdata = data_grid(wage, age)) %>%
  mutate(.fitted_low = .fitted - 1.96 * .se.fit,
         .fitted_high = .fitted + 1.96 * .se.fit) %>%
  ggplot(aes(age, .fitted)) +
  geom_point(data = wage, aes(y = wage), alpha = .05) +
  geom_line() +
  geom_line(aes(y = .fitted_low), linetype = 2) +
  geom_line(aes(y = .fitted_high), linetype = 2) +
  geom_vline(xintercept = attr(bs(wage$age, df = 8, degree = 6), "knots"),
             linetype = 2, color = "blue") +
  labs(title = "Sixth-order polynomial spline of wage",
       subtitle = "Knots = 2",
       x = "Age",
       y = "Wage")
```

## Local regression

1. Gather the fraction $s = \frac{k}{n}$ of training points whose $x_i$ are closest to $x_0$.
1. Assign a weight $K_{i0} = K(x_i, x_0)$ to each point in the neighborhood, so that the point furthest from $x_0$ has a weight of 0, and the closest has the highest weight. All but these $k$ nearest neighbors get a weight of 0.
1. Fit a **weighted least squares regression** of the $y_i$ on the $x_i$ using the aformentioned weights.
1. The fitted value at $x_0$ is given by $\hat{f}(x_0) = \hat{\beta}_0 + \hat{\beta}_1 x_0$

## Local regression

```{r loess, echo = FALSE, warning = FALSE, message = FALSE}
library(lattice)

mod <- loess(NOx ~ E, ethanol, degree = 1, span = .75)
fit <- augment(mod)

mod0 <- loess(NOx ~ E, ethanol, degree = 0, span = .75)
mod1 <- loess(NOx ~ E, ethanol, degree = 1, span = .75)
mod2 <- loess(NOx ~ E, ethanol, degree = 2, span = .75)

fit_all <- ethanol %>%
  gather_predictions(mod0, mod1, mod2) %>%
  mutate(model = factor(model, levels = c("mod0", "mod1", "mod2"),
                        labels = c("Constant", "Linear", "Quadratic")))

ggplot(fit_all, aes(E, NOx)) +
  geom_point() +
  geom_line(aes(y = pred, color = model)) +
  labs(title = "Local linear regression",
       x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J",
       color = "Regression")
```

## Local regression

```{r loess_buildup, dependson="loess", fig.show = "animate", echo = FALSE, warning = FALSE, message = FALSE}
dat <- ethanol %>%
  inflate(center = unique(ethanol$E)) %>%
  mutate(dist = abs(E - center)) %>%
  filter(rank(dist) / n() <= .75) %>%
  mutate(weight = (1 - (dist / max(dist)) ^ 3) ^ 3)

library(gganimate)

p <- ggplot(dat, aes(E, NOx)) +
  geom_point(aes(alpha = weight, frame = center)) +
  geom_smooth(aes(group = center, frame = center, weight = weight), method = "lm", se = FALSE) +
  geom_vline(aes(xintercept = center, frame = center), lty = 2) +
  geom_line(aes(y = .fitted), data = fit, color = "red") +
  labs(x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J")
gg_animate(p)
```

## Local regression

```{r loess_span, dependson="loess", fig.show = "animate", echo = FALSE, warning = FALSE, message = FALSE}
spans <- c(.25, .5, .75, 1)

# create loess fits, one for each span
fits <- data_frame(span = spans) %>%
  group_by(span) %>%
  do(augment(loess(NOx ~ E, ethanol, degree = 1, span = .$span)))

# calculate weights to reproduce this with local weighted fits
dat <- ethanol %>%
  inflate(span = spans, center = unique(ethanol$E)) %>%
  mutate(dist = abs(E - center)) %>%
  filter(rank(dist) / n() <= span) %>%
  mutate(weight = (1 - (dist / max(dist)) ^ 3) ^ 3)

# create faceted plot with changing points, local linear fits, and vertical lines,
# and constant hollow points and loess fit
p <- ggplot(dat, aes(E, NOx)) +
  geom_point(aes(alpha = weight, frame = center)) +
  geom_smooth(aes(group = center, frame = center, weight = weight), method = "lm", se = FALSE) +
  geom_vline(aes(xintercept = center, frame = center), lty = 2) +
  geom_point(shape = 1, data = ethanol, alpha = .25) +
  geom_line(aes(y = .fitted, frame = E, cumulative = TRUE), data = fits, color = "red") +
  facet_wrap(~span) +
  ylim(0, 5) +
  ggtitle("x0 = ") +
  labs(x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J")

gg_animate(p)
```

## Generalized additive models

* Combine multiple predictors
* Maintain additive assumption

## GAMs for regression problems

$$y_i = \beta_0 + \beta_{1} X_{i1} + \beta_{2} X_{i2} + \dots + \beta_{p} X_{ip} + \epsilon_i$$

$$y_i = \beta_0 + \sum_{j = 1}^p f_j(x_{ij}) + \epsilon_i$$

$$y_i = \beta_0 + f_1(x_{i1}) + \beta_{2} f_2(x_{i2}) + \dots + f_p(x_{ip}) + \epsilon_i$$

## GAM for Biden {.scrollable}

$$\text{Biden} = \beta_0 + f_1(\text{Age}) + f_2(\text{Education}) + f_3(\text{Gender}) + \epsilon$$

```{r biden-gam}
library(gam)

# estimate model for splines on age and education plus dichotomous female
biden_gam <- gam(biden ~ bs(age, df = 5) + bs(educ, df = 5) + female, data = biden)

# get graphs of each term
biden_gam_terms <- preplot(biden_gam, se = TRUE, rug = FALSE)

## age
data_frame(x = biden_gam_terms$`bs(age, df = 5)`$x,
           y = biden_gam_terms$`bs(age, df = 5)`$y,
           se.fit = biden_gam_terms$`bs(age, df = 5)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of Biden feeling thermometer",
       subtitle = "Cubic spline",
       x = "Age",
       y = expression(f[1](age)))

## education
data_frame(x = biden_gam_terms$`bs(educ, df = 5)`$x,
           y = biden_gam_terms$`bs(educ, df = 5)`$y,
           se.fit = biden_gam_terms$`bs(educ, df = 5)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of Biden feeling thermometer",
       subtitle = "Cubic spline",
       x = "Education",
       y = expression(f[2](education)))

## gender
data_frame(x = biden_gam_terms$female$x,
           y = biden_gam_terms$female$y,
           se.fit = biden_gam_terms$female$se.y) %>%
  unique %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit,
         x = factor(x, levels = 0:1, labels = c("Male", "Female"))) %>%
  ggplot(aes(x, y, ymin = y_low, ymax = y_high)) +
  geom_errorbar() +
  geom_point() +
  labs(title = "GAM of Biden feeling thermometer",
       x = NULL,
       y = expression(f[3](gender)))
```

## GAM for Biden {.scrollable}

```{r biden-gam-local}
# estimate model for splines on age and education plus dichotomous female
biden_gam_local <- gam(biden ~ lo(age) + lo(educ) + female, data = biden)

# get graphs of each term
biden_gam_local_terms <- preplot(biden_gam_local, se = TRUE, rug = FALSE)

## age
data_frame(x = biden_gam_local_terms$`lo(age)`$x,
           y = biden_gam_local_terms$`lo(age)`$y,
           se.fit = biden_gam_local_terms$`lo(age)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of Biden feeling thermometer",
       subtitle = "Local regression",
       x = "Age",
       y = expression(f[1](age)))

## education
data_frame(x = biden_gam_local_terms$`lo(educ)`$x,
           y = biden_gam_local_terms$`lo(educ)`$y,
           se.fit = biden_gam_local_terms$`lo(educ)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of Biden feeling thermometer",
       subtitle = "Local regression",
       x = "Education",
       y = expression(f[2](education)))

## gender
data_frame(x = biden_gam_local_terms$female$x,
           y = biden_gam_local_terms$female$y,
           se.fit = biden_gam_local_terms$female$se.y) %>%
  unique %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit,
         x = factor(x, levels = 0:1, labels = c("Male", "Female"))) %>%
  ggplot(aes(x, y, ymin = y_low, ymax = y_high)) +
  geom_errorbar() +
  geom_point() +
  labs(title = "GAM of Biden feeling thermometer",
       x = NULL,
       y = expression(f[3](gender)))
```

## GAM for Titanic {.scrollable}

```{r titanic-gam}
library(titanic)

# estimate model for splines on age and education plus dichotomous female
titanic_gam <- gam(Survived ~ bs(Age, df = 5) + bs(Fare, df = 5) + Sex, data = titanic_train,
                   family = binomial)

# get graphs of each term
titanic_gam_terms <- preplot(titanic_gam, se = TRUE, rug = FALSE)

## age
data_frame(x = titanic_gam_terms$`bs(Age, df = 5)`$x,
           y = titanic_gam_terms$`bs(Age, df = 5)`$y,
           se.fit = titanic_gam_terms$`bs(Age, df = 5)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of Titanic survival",
       subtitle = "Cubic spline",
       x = "Age",
       y = expression(f[1](age)))

## fare
data_frame(x = titanic_gam_terms$`bs(Fare, df = 5)`$x,
           y = titanic_gam_terms$`bs(Fare, df = 5)`$y,
           se.fit = titanic_gam_terms$`bs(Fare, df = 5)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of Titanic survival",
       subtitle = "Cubic spline",
       x = "Fare",
       y = expression(f[2](fare)))

## gender
data_frame(x = titanic_gam_terms$Sex$x,
           y = titanic_gam_terms$Sex$y,
           se.fit = titanic_gam_terms$Sex$se.y) %>%
  unique %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit,
         x = factor(x, levels = c("male", "female"), labels = c("Male", "Female"))) %>%
  ggplot(aes(x, y, ymin = y_low, ymax = y_high)) +
  geom_errorbar() +
  geom_point() +
  labs(title = "GAM of Titanic survival",
       x = NULL,
       y = expression(f[3](gender)))
```



