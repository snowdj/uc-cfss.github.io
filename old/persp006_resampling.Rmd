---
title: "Statistical learning: resampling methods"
author: "MACS 30100 - Perspectives on Computational Modeling"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Objectives

* Define common metrics for assessing model accuracy
* Explain the difference between the training and test MSE and the need for a test set of data
* Define resampling methods
* Compare and contrast the validation set approach with leave-one-out and $k$-fold cross-validation
* Define bootstrapping and explain when it can be used in research
* Demonstrate how to conduct cross-validation and bootstrapping using `modelr`

```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(modelr)
library(broom)
library(rcfss)
library(titanic)
set.seed(1234)

options(digits = 3)

theme_set(theme_minimal())
```

# Assessing model accuracy

Some statistical learning methods will perform better on specific datasets than other methods. Even within methods, various model specifications will perform differentially. We need metrics to determine which method/model works better on any given set of data.

Model accuracy (also called **goodness of fit**) will identify how well a given model fits a set of observations. However, it is important to realize that there are some things model accuracy does not do. It does not identify whether:

* Omitted-variable bias exists
* The correct statistical learning method was used
* The most appropriate set of independent variables has been chosen
* **The independent variables are a cause of the changes in the dependent variable**

Most importantly, it does not tell you the importance of specific variables in the model. Model accuracy describes the overall model's performance, not individual variables. To conduct inference, not only do you need a well-specified (and good fitting) model, but the variables themselves also need to be **statistically** and **substantively significant**. In order to assess that in a regression context, we typically examine the size of the estimated parameters relative to their standard errors by estimating a [$t$-statistic](https://en.wikipedia.org/wiki/T-statistic) and p-value, and comparing the magnitude of the relationships in a multiple variable model to determine which relationship is the strongest/largest.

Here, I'll briefly introduce some major approaches for regression and classification problems.

## Coefficient of determination

The **coefficient of determination** is a number that identifies the proportional reduction in squared error in a linear regression model, and is the ratio of the **regression sum of squares** and the **residual sum of squares**:

$$R^2 = \frac{\text{RegSS}}{\text{TSS}}$$

$$R^2 = \frac{\text{TSS} - \text{RSS}}{\text{TSS}}$$

where TSS is the **total sum of squares** for $Y$ and RSS is the **residual sum of squares**.

$$R^2 = \frac{\sum_{i = 1}^{N} (Y_i - \bar{Y}_i)^2 - \sum_{i = 1}^{N} (Y_i - \hat{Y}_i)^2}{\sum_{i = 1}^{N} (Y_i - \bar{Y}_i)^2}$$

$$R^2 = \frac{\sum_{i = 1}^{N} (\hat{Y}_i - \bar{Y}_i)^2}{\sum_{i = 1}^{N} (Y_i - \bar{Y}_i)^2}$$

```{r r-squared}
dist1 <- sim1 %>% 
  mutate(
    dodge = rep(c(-1, 0, 1) / 20, 10),
    x1 = x + dodge,
    pred = 7 + x1 * 1.5
  )

ggplot(dist1, aes(x1, y)) + 
  geom_abline(aes(color = "TSS"), intercept = 7, slope = 1.5) +
  geom_hline(aes(color = "RegSS"), yintercept = mean(dist1$y), color = "grey40", linetype = 2) +
  geom_point(color = "grey40") +
  geom_linerange(aes(ymin = y, ymax = pred, color = "TSS")) +
  geom_linerange(aes(ymin = pred, ymax = mean(y), color = "RegSS"), linetype = 2) +
  labs(title = expression(R^2),
       color = NULL)
```

$R^2$ ranges on a 0-1 scale and can be interpreted as the proportion of the variation in the outcome $Y$ explained by the independent variables $X$ in the linear regression model. It is a **relative measure of fit** because it explains the relative amount of error/accuracy in the model and is frequently used when conducting inference.

However a downside to $R^2$ is that it will only increase as the number of predictors in the regression model increases. Adjustments to $R^2$ can be made to counter this inflationary effect, most commonly employing **adjusted $R^2$**:

$\text{adj-}R^2 = 1 - (1 - R^2) \frac{n - 1}{n - p - 1}$

where $n$ is the number of observations in the data and $p$ is the number of predictors (not including the constant term).

Statisticians and researchers [debate the merits of using $R^2$ to compare models.](http://andrewgelman.com/2007/08/29/rsquared_useful/) For one, because $R^2$ increases with the number of predictors, comparing a simple single variable model to a multiple variable model with 50 predictors could yield the determination that the 50 variable model is better, **merely because it has 50 predictors**. Because of properties of $R^2$ we will not go into here, it is not the best approach for comparing the accuracy of different models using the same dataset. Instead, we want an **absolute measure of fit**.

## Mean squared error

**Mean Squared Error** (MSE) is defined as

$$MSE = \frac{1}{n} \sum_{i = 1}^{n}{(y_i - \hat{f}(x_i))^2}$$

where:

* $y_i =$ the observed response value for the $i$th observation
* $\hat{f}(x_i) =$ the predicted response value for the $i$th observation given by $\hat{f}$
* $n =$ the total number of observations

The MSE is an absolute measure of fit because its value depends on the measurement units of the response variable $Y$. It will be small if predicted values $\hat{Y}$ are close to the actual values $Y$, and will be large if for some/all observations the predicted and actual values differ significantly.

> **Root mean squared error** (RMSE) is the square root of MSE. It is also commonly seen in statistical packages as a model accuracy metric. You can use that as well, however we will use MSE because it is associated with **variance** ($\sigma^2$), whereas RMSE is associated with **standard deviation** ($\sigma$). Variance has certain statistical properties that are missing from standard deviation, so we will use MSE.

Consider the relationship between horsepower and car mileage in the `Auto` dataset (found in `library(ISLR)`):

```{r auto}
library(ISLR)

Auto <- Auto %>%
  tbl_df()
Auto
```

The relationship does not appear to be strictly linear:

```{r auto_plot, dependson="auto"}
ggplot(Auto, aes(horsepower, mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

Instead, perhaps the relationship is quadratic:

```{r auto_plot_lm, dependson="auto"}
# linear model
auto_lm <- lm(mpg ~ horsepower, data = Auto)

# quadratic model
auto_lm2 <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)

# generate predicted values plot
Auto %>%
  gather_predictions(auto_lm, auto_lm2) %>%
  mutate(model = factor(model, levels = c("auto_lm", "auto_lm2"),
                        labels = c("horsepower", "horsepower^2"))) %>%
  ggplot(aes(horsepower)) +
  geom_point(aes(y = mpg), alpha = .5) +
  geom_line(aes(y = pred, color = model), size = 1) +
  labs(x = "Horsepower",
       y = "MPG",
       color = "Model")
```

By eyeballing the graph it appears the quadratic model performs better and generates more accurate predicted values, but with MSE we can quantify that difference:

```{r mse-function}
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}
```

```{r mse, dependson="auto_split"}
mse(auto_lm, Auto)    # linear model
mse(auto_lm2, Auto)   # quadratic model
```

The MSE for the linear model is approximately `r mse(auto_lm, Auto) - mse(auto_lm2, Auto)` points larger than the quadratic model. On this basis, we could conclude that the quadratic model fits the data better than the linear model.

## Error rate

For classification problems, we instead consider the **error rate**, or the proportion of mistakest that are made if we apply our estimate $\hat{f}$ to the observations:

$$\frac{1}{n} \sum_{n = 1}^{n} I(y_i \neq \hat{y}_i)$$

where $\hat{y}_i$ is the predicted classification label for the $i$th observation using some estimated function $\hat{f}$, and $I(y_i \neq \hat{y}_i)$ is an **indicator** function that equals 1 if $y_i \neq \hat{y}_i$ and 0 if $y_i = \hat{y}_i$ (i.e. if the observation was correctly classified).

Recall how we used this approach to evaluate the accuracy of our [interactive model predicting survival during the sinking of the Titanic](persp004_logistic_regression.html#accuracy_of_predictions).^[Albeit then we calculated the **accuracy rate**, which is merely $1 - \frac{1}{n} \sum_{n = 1}^{n} I(y_i \neq \hat{y}_i)$ or $\frac{1}{n} \sum_{n = 1}^{n} I(y_i = \hat{y}_i)$.]

```{r titanic_data, message = FALSE}
library(titanic)
titanic <- titanic_train %>%
  as_tibble()
```

```{r age_woman_cross}
survive_age_woman_x <- glm(Survived ~ Age * Sex, data = titanic,
                           family = binomial)
summary(survive_age_woman_x)
```

```{r accuracy_age_gender_x_test_set, dependson="age_woman_cross", message = FALSE}
x_accuracy <- titanic %>%
  add_predictions(survive_age_woman_x) %>%
  mutate(pred = logit2prob(pred),
         prob = pred,
         pred = as.numeric(pred > .5))

mean(x_accuracy$Survived != x_accuracy$pred, na.rm = TRUE)
```

As we discussed [last week](persp004_logistic_regression.html), there are more advanced metrics for classification problems such as **proportional reduction in error** and **area under the curve** (AUC), but ultimately they are derived from this basic error rate.

# Training vs. test data

Until now we have used the same data to both train/estimate our models and evaluate model fit. **We must stop doing this**. We don't care how the method works on our training data, because we already know the results for our training data. That is, we know the outcomes for those observations. Instead, we want to predict or explain new observations that may occur in the future. Even if you don't care about prediction, for the purposes of inference we also need to use new observations to correctly and accurately assess model fit.

If we estimate a statistical model $\hat{f}$ from a set of observations $\{ (x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}$, we want to estimate the how close $\hat{f}(x_0)$ is to $y_0$ where $(x_0, y_0)$ is a completely new, never before seen **test observation**. If we have a large test set, then the **test MSE** will be:

$$MSE_{\text{Test}} = \frac{1}{n} \sum_{j = 1}^{n}{(y_j - \hat{f}(x_j))^2}$$

for the test set $j$. The optimal model should then be one that minimizes this quantity, rather than the training MSE.

But shouldn't these two quantities be the same thing? Not necessarily. There is no guarantee that the method produced by minimizing the training MSE will be the same as the method produced by minimizing the test MSE. This is because we can build overly complicated models that fit the training data very well, but don't generalize to observations outside the training data.

Consider the simulated example below:

```{r sim-mse}
sim_mse <- data_frame(X = runif(50, 0, 10),
                      Y = 3 + .05 * X + .05 * X^2 + .05 * X^3 + rnorm(50, 0, 4))

# linear model
sim_mse_1 <- lm(Y ~ poly(X, 1), data = sim_mse)

# third-order poly
sim_mse_3 <- lm(Y ~ poly(X, 3), data = sim_mse)

# tenth-order poly
sim_mse_10 <- lm(Y ~ poly(X, 10), data = sim_mse)

# add predictions and plot
sim_mse %>%
  gather_predictions(sim_mse_1, sim_mse_3, sim_mse_10) %>%
  mutate(model = factor(model, levels = c("sim_mse_1", "sim_mse_3", "sim_mse_10"),
                        labels = c("X^1", "X^3", "X^10"))) %>%
  ggplot(aes(X)) +
  geom_point(aes(y = Y)) +
  geom_line(aes(y = pred, color = model)) +
  scale_color_discrete(labels = c(expression(X^1), expression(X^3), expression(X^10))) +
  labs(color = "Model") +
  theme(legend.text.align = 0)
```

Clearly the fit is not strictly linear, but what higher-order polynomial is best? If we calculate the MSE of each model using the training data, we get:

```{r sim-mse-train}
data_frame(terms = c(1, 3, 20),
                           model = list(sim_mse_1, sim_mse_3, sim_mse_10)) %>%
  mutate(mse = map_dbl(model, mse, data = sim_mse)) %>%
  select(-model) %>%
  knitr::kable(caption = "Training MSEs",
               col.names = c("Highest-order term", "MSE"))
```

The tenth-order polynomial provides the lowest mean squared error, so we should use that model, right? But what if we sample another 50 observations from the same population and estimate the MSE for those observations **using the original statistical learning models**?

```{r sim-mse-test}
# simulate test data
sim_mse_test <- data_frame(X = runif(50, 0, 10),
                      Y = 3 + .05 * X + .05 * X^2 + .05 * X^3 + rnorm(50, 0, 4))

# draw graph with new points and original models
sim_mse_test %>%
  gather_predictions(sim_mse_1, sim_mse_3, sim_mse_10) %>%
  mutate(model = factor(model, levels = c("sim_mse_1", "sim_mse_3", "sim_mse_10"),
                        labels = c("X^1", "X^3", "X^10"))) %>%
  ggplot(aes(X)) +
  geom_point(aes(y = Y)) +
  geom_line(aes(y = pred, color = model)) +
  scale_color_discrete(labels = c(expression(X^1), expression(X^3), expression(X^10))) +
  labs(color = "Original\nmodel") +
  theme(legend.text.align = 0)

# test set MSE
data_frame(terms = c(1, 3, 20),
                           model = list(sim_mse_1, sim_mse_3, sim_mse_10)) %>%
  mutate(mse = map_dbl(model, mse, data = sim_mse_test)) %>%
  select(-model) %>%
  knitr::kable(caption = "Training MSEs",
               col.names = c("Highest-order term", "MSE"))
```

Well crap. Our problem is that our tenth-order polynomial model **overfit** the training observations. It was great at explaining and predicting the original 50 observations, but collapsed when applied to explaining and predicting new observations **from the same data generating process**. The statistical model is attempting to explain the variation in $Y$ as best as it can, introducing more complex parameters to obtain a better fit. But this pattern is not represented in the larger data generating process, yielding a worse test MSE.

# Resampling methods

How then do we develop a sufficient test set to evaluate our models? Do we wait for the data generating process (aka "life") to produce new observations for us? We could be waiting a long time. Instead, we can turn to **resampling methods** to draw test sets from our existing sample of observations.

Resampling methods are essential to test and evaluate statistical models. Because you likely do not have the resources or capabilities to repeatedly sample from your population of interest, instead you can repeatedly draw from your original sample to obtain additional information about your model. For instance, you could repeatedly draw samples from your data, estimate a linear regression model on each sample, and then examine how the estimated model differs across each sample. This allows you to assess the variability and stability of your model in a way not possible if you can only fit the model once.

# Cross-validation

Cross-validation involves estimating test MSE and other model fit measures by **holding out** a subset of the original training observations from the model training process, then applying that statistical learning method to the held out sets.

## Validation set

Under the **validation set** approach, we randomly split our data into distinct **training** and **test** sets. The training set can be used repeatedly to explore or train different models. Once we have a stable model, we can apply it to the test set of held-out data to determine (unbiasedly) whether the model makes accurate predictions. By splitting our data, we can evaluate the model's effectiveness at predicting the response variable (in the context of either regression or classification) independently of the data used to estimate the model in the first place.

### Classification

We could use the validation set method to estimate the test error rate for the interactive survival model for the Titanic that we estimated above. Here I randomly split the data, assigning 70% of the observations to the training set and holding out the remaining 30% for testing the model.

```{r titanic-test-mse}
titanic_split <- resample_partition(titanic, c(test = 0.3, train = 0.7))

train_model <- glm(Survived ~ Age * Sex, data = titanic_split$train,
                   family = binomial)
summary(train_model)

x_train_accuracy <- titanic_split$train %>%
  tbl_df() %>%
  add_predictions(train_model) %>%
  mutate(pred = logit2prob(pred),
         pred = as.numeric(pred > .5))

x_test_accuracy <- titanic_split$test %>%
  tbl_df() %>%
  add_predictions(train_model) %>%
  mutate(pred = logit2prob(pred),
         pred = as.numeric(pred > .5))

train_err <- mean(x_train_accuracy$Survived != x_train_accuracy$pred, na.rm = TRUE)
test_err <- mean(x_test_accuracy$Survived != x_test_accuracy$pred, na.rm = TRUE)
```

Under the validation set approach, we estimate a test error rate of `r test_err`, compared to the train error rate of `r train_err`.

### Regression

This method also works for regression analysis. Here we reconsider the possible quadratic relationship between horsepower and car mileage in the `Auto` dataset.

For this task, first we can use `modelr::resample_partition()` to create training and test sets (using a 50/50 split), then estimate a series of linear regression models with higher-order polynomial terms.

* I use `set.seed()` in the beginning - whenever you are writing a script that involves randomization (here, random subsetting of the data), always set the seed at the beginning of the script. This ensures the results can be reproduced precisely.^[The actual value you use is irrelevant. Just be sure to set it in the script, otherwise R will randomly pick one each time you start a new session.]
* I also use the `glm()` function rather than `lm()` - if you don't change the `family` parameter, the results of `lm()` and `glm()` are exactly the same.^[The default `family` for `glm()` is `gaussian()`, or the **Gaussian** distribution. You probably know it by its other name, the [**Normal** distribution](https://en.wikipedia.org/wiki/Normal_distribution).]

```{r auto_split}
set.seed(1234)

auto_split <- resample_partition(Auto, c(test = 0.5, train = 0.5))
auto_train <- auto_split$train %>%
  tbl_df()
auto_test <- auto_split$test %>%
  tbl_df()
```

```{r mse_poly, dependson="auto_split"}
auto_poly_results <- data_frame(terms = 1:5,
           model = map(terms, ~ glm(mpg ~ poly(horsepower, .), data = auto_train)),
           MSE = map_dbl(model, mse, data = auto_test))

ggplot(auto_poly_results, aes(terms, MSE)) +
  geom_line() +
  labs(title = "Comparing quadratic linear models",
       subtitle = "Using validation set",
       x = "Highest-order polynomial",
       y = "Mean Squared Error")
```

Based on the MSE for the validation (test) set, a polynomial model with a quadratic term ($\text{horsepower}^2$) produces the lowest average error. Adding cubic or higher-order terms is just not necessary.

## Drawbacks to the validation set approach

There are two main problems with validation sets:

1. Validation estimates of the test MSE can be highly variable depending on which observations are sampled into the training and test sets. See what happens if we repeat the sampling, estimation, and validation procedure for the `Auto` data set:

    ```{r auto_variable_mse}
    mse_variable <- function(Auto){
      auto_split <- resample_partition(Auto, c(test = 0.5, train = 0.5))
      auto_train <- auto_split$train %>%
        tbl_df()
      auto_test <- auto_split$test %>%
        tbl_df()
      
      results <- data_frame(terms = 1:5,
                            model = map(terms,
                                        ~ glm(mpg ~ poly(horsepower, .),
                                              data = auto_train)),
                            MSE = map_dbl(model, mse, data = auto_test))
      
      return(results)
    }
    
    rerun(10, mse_variable(Auto)) %>%
      bind_rows(.id = "id") %>%
      ggplot(aes(terms, MSE, color = id)) +
      geom_line() +
      labs(title = "Variability of MSE estimates",
           subtitle = "Using the validation set approach",
           x = "Degree of Polynomial",
           y = "Mean Squared Error") +
      theme(legend.position = "none")
    ```
    
    Depending on the specific training/test split, our MSE varies by up to 5.

1. If you don't have a large data set, you'll have to dramatically shrink the size of your training set. Most statistical learning methods perform better with more observations - if you don't have enough data in the training set, you might overestimate the error rate in the test set.

# Leave-one-out cross-validation

An alternative method is **leave-one-out cross validation** (LOOCV). Like with the validation set approach, you split the data into two parts. However the difference is that you only remove one observation for the test set, and keep all remaining observations in the training set. The statistical learning method is fit on the $n-1$ training set. You then use the held-out observation to calculate the $MSE = (y_1 - \hat{y}_1)^2$ which should be an unbiased estimator of the test error. Because this MSE is highly dependent on which observation is held out, **we repeat this process for every single observation in the data set**. Mathematically, this looks like:

$$CV_{(n)} = \frac{1}{n} \sum_{i = 1}^{n}{MSE_i}$$

This method produces estimates of the error rate that have minimal bias and are relatively steady (i.e. non-varying), unlike the validation set approach where the test MSE estimate is highly dependent on the sampling process for training/test sets. LOOCV is also highly flexible and works with any kind of predictive modeling.

Of course the downside is that this method is computationally difficult. You have to estimate $n$ different models - if you have a large $n$ or each individual model takes a long time to compute, you may be stuck waiting a long time for the computer to finish its calculations.

### LOOCV in linear regression

We can use the `crossv_kfold()` function in the `modelr` library to compute the LOOCV of any linear or logistic regression model. It takes two arguments: the data frame and the number of $k$-folds (which we will define shortly). For our purposes now, all you need to know is that `k` should equal the number of observations in the data frame which we can retrieve using the `nrow()` function. For the `Auto` dataset, this looks like:

```{r loocv-data, dependson="Auto"}
loocv_data <- crossv_kfold(Auto, k = nrow(Auto))
```

Now we estimate the linear model $k$ times, excluding the holdout test observation, then calculate the test MSE:

```{r loocv, dependson="Auto"}
loocv_models <- map(loocv_data$train, ~ lm(mpg ~ horsepower, data = .))
loocv_mse <- map2_dbl(loocv_models, loocv_data$test, mse)
mean(loocv_mse)
```

The results of the mapped `mse()` function is the MSE for each iteration through the data, so there is one MSE for each observation. Calculating the `mean()` of that vector gives us the LOOCV MSE.

We can also use this method to compare the optimal number of polynomial terms as before.

```{r loocv_poly, dependson="Auto"}
cv_error <- vector("numeric", 5)
terms <- 1:5

for(i in terms){
  loocv_models <- map(loocv_data$train, ~ lm(mpg ~ poly(horsepower, i), data = .))
  loocv_mse <- map2_dbl(loocv_models, loocv_data$test, mse)
  cv_error[[i]] <- mean(loocv_mse)
}

cv_mse <- data_frame(terms = terms,
           cv_MSE = cv_error)
cv_mse

ggplot(cv_mse, aes(terms, cv_MSE)) +
  geom_line() +
  labs(title = "Comparing quadratic linear models",
       subtitle = "Using LOOCV",
       x = "Highest-order polynomial",
       y = "Mean Squared Error")
```

And arrive at a similar conclusion. There may be a very marginal advantage to adding a fifth-order polynomial, but not substantial enough for the additional complexity over a mere second-order polynomial.

### LOOCV in classification

Let's use classification to validate the interactive terms model from before. For technical reasons, we need to use a custom `mse.glm()` function to properly calculate the MSE for binary response variables:^[This function can also be loaded via the [`rcfss`](https://github.com/uc-cfss/rcfss) library. Be sure to update your package to the latest version to make sure the function is available.]

```{r mse-glm}
mse.glm <- function (model, data){
  residuals.glm <- function(model, data) {
    modelr:::response(model, data) - stats::predict(model, data, type = "response")
  }
  
  x <- residuals(model, data)
  mean(x^2, na.rm = TRUE)
}
```

```{r titanic_loocv}
titanic_loocv <- crossv_kfold(titanic, k = nrow(titanic))
titanic_models <- map(titanic_loocv$train, ~ glm(Survived ~ Age * Sex, data = .,
                                               family = binomial))
titanic_mse <- map2_dbl(titanic_models, titanic_loocv$test, mse.glm)
mean(titanic_mse, na.rm = TRUE)
```

In a classification problem, the LOOCV tells us the average error rate based on our predictions. So here, it tells us that the interactive `Age * Sex` model has a `r formatC(mean(titanic_mse, na.rm = TRUE) * 100, digits = 3)`% error rate. This is similar to the validation set result ($`r formatC(mean(x_test_accuracy$Survived != x_test_accuracy$pred, na.rm = TRUE) * 100, digits = 3)`\%$)

## k-fold cross-validation

A less computationally-intensive approach to cross-validation is **$k$-fold cross-validation**. Rather than dividing the data into $n$ groups, one divides the observations into $k$ groups, or **folds**, of approximately equal size. The first fold is treated as the validation set, and the model is estimated on the remaining $k-1$ folds. This process is repeated $k$ times, with each fold serving as the validation set precisely once. The $k$-fold CV estimate is calculated by averaging the MSE values for each fold:

$$CV_{(k)} = \frac{1}{k} \sum_{i = 1}^{k}{MSE_i}$$

As you probably figured out by now, LOOCV is the special case of $k$-fold cross-validation where $k = n$. More typically researchers will use $k=5$ or $k=10$ depending on the size of the data set and the complexity of the statistical model.

### k-fold CV in linear regression

Let's go back to the `Auto` data set. Instead of LOOCV, let's use 10-fold CV to compare the different polynomial models.

```{r 10_fold_auto}
cv10_data <- crossv_kfold(Auto, k = 10)

cv_error_fold10 <- vector("numeric", 5)
terms <- 1:5

for(i in terms){
  cv10_models <- map(cv10_data$train, ~ lm(mpg ~ poly(horsepower, i), data = .))
  cv10_mse <- map2_dbl(cv10_models, cv10_data$test, mse)
  cv_error_fold10[[i]] <- mean(cv10_mse)
}

cv_error_fold10
```

How do these results compare to the LOOCV values?

```{r 10_fold_auto_loocv, dependson=c("10_fold_auto","loocv_poly")}
data_frame(terms = terms,
           loocv = cv_error,
           fold10 = cv_error_fold10) %>%
  gather(method, MSE, loocv:fold10) %>%
  ggplot(aes(terms, MSE, color = method)) +
  geom_line() +
  labs(title = "MSE estimates",
       x = "Degree of Polynomial",
       y = "Mean Squared Error",
       color = "CV Method")
```

Pretty much the same results.

## Computational speed of LOOCV vs. $k$-fold CV

Remember that using LOOCV you have to estimate $n$ models, whereas for $k$-fold CV you only estimate $k$ models. How long does it take to estimate each type of CV?

### LOOCV

```{r loocv_time}
library(profvis)

profvis({
  cv_error <- vector("numeric", 5)
  terms <- 1:5
  
  for(i in terms){
    loocv_models <- map(loocv_data$train, ~ lm(mpg ~ poly(horsepower, i), data = .))
    loocv_mse <- map2_dbl(loocv_models, loocv_data$test, mse)
    cv_error[[i]] <- mean(loocv_mse)
  }
})
```

### 10-fold CV

```{r kfold_time}
library(profvis)

profvis({
  cv_error_fold10 <- vector("numeric", 5)
  terms <- 1:5
  
  for(i in terms){
    cv10_models <- map(cv10_data$train, ~ lm(mpg ~ poly(horsepower, i), data = .))
    cv10_mse <- map2_dbl(cv10_models, cv10_data$test, mse)
    cv_error_fold10[[i]] <- mean(cv10_mse)
  }
})
```

On my machine, 10-fold CV was about 40 times faster than LOOCV. Again, estimating $k=10$ models is going to be much easier than estimating $k=`r nrow(Auto)`$ models.

### k-fold CV in logistic regression

You've gotten the idea by now, but let's do it one more time on our interactive Titanic model.

```{r titanic_kfold}
titanic_kfold <- crossv_kfold(titanic, k = 10)
titanic_models <- map(titanic_kfold$train, ~ glm(Survived ~ Age * Sex, data = .,
                                               family = binomial))
titanic_mse <- map2_dbl(titanic_models, titanic_kfold$test, mse.glm)
mean(titanic_mse, na.rm = TRUE)
```

Not a large difference from the LOOCV approach, but it take much less time to compute.

# The bootstrap

The **bootstrap** is a different resampling-based method for quantifying uncertainty associated with a given estimator or statistical method. It is extremely flexible and can be applied to virtually any statistical method.

## Generating samples

**Sampling without replacement** involves randomly sampling from a population whereby once an observation is drawn, it cannot be drawn again. Here I've drawn 10 random samples without replacement from the vector $`r 1:10`$:

```{r sim-sample-noreplace}
rerun(10, sample.int(10, replace = FALSE)) %>%
  unlist %>%
  matrix(ncol = 10, byrow = TRUE)
```

Drawing $10$ samples of size $10$ from an original population of $10$ observations would produce the exact same sample every time, just in a different order.

**Sampling with replacement** allows us to potentially draw the same observation multiple times, and ignore other observations entirely.

```{r sim-sample-replace}
rerun(10, sample.int(10, replace = TRUE)) %>%
  unlist %>%
  matrix(ncol = 10, byrow = TRUE)
```

Here I've drawn 10 random samples with replacement from the vector $`r 1:10`$. Each row contains a different sample. Notice how some rows contain multiples of the same values and exclude others entirely.

## Why use the bootstrap?

Statistical learning methods are frequently used to draw inferences about a population. Since you cannot directly measure the entire population^[Exception - [the Census](http://www.census.gov/2010census/).], you take a sample and ask a question of it instead. But how do you know your sample answer is close to the population answer? There are two approaches you can take:

1. Make **assumptions** about the shape of the population.
1. Use the **information in the sample** to learn about it.

### Making assumptions

![[When you assume](https://www.xkcd.com/1339/)](images/xkcd_assume.png)

Suppose you decide to make assumptions, e.g. that the sample is distributed normally or [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution) or some other probability distribution. You could learn about how much the answer to your question varies based on the specific sample drawn by repeatedly generating samples of the same size and asking them the same question. If you have a computaionally convenient assumption (such as the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)), you may even be able to bypass the resampling step and use a known formula to estimate your confidence in the original answer.

### Using information in the sample

![](images/sample_pop_meme.jpg)

Provided you are happy to make the assumptions, this seems like a good idea. If you are not willing to make the assumption, you could instead take the sample you have and sample from it. You can do this because the sample you have **is also a population**, just a very small and discrete one. It is identical to the histogram of your data. Sampling with replacement merely allows you to treat the sample like it's a population and sample from it in a way that reflects its shape.

This is a reasonable thing to do for a couple reasons. First, it's the only information you have about the population. Second, randomly chosen samples should look quite similar to the population from which they came, so as long as you drew a random sample it is likely that your's is also similar.

## Estimating the accuracy of a statistic of interest

Suppose you want to know how often Americans eat ice cream in a given month.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Rockyroadicecream.jpg/800px-Rockyroadicecream.jpg)

We decide to estimate this by tracking a sample of 1000 Americans and counting how many times they eat ice cream over the course of a month.

```{r ice-sim}
# simulate the sample
set.seed(1234)
mu <- 5
n_obs <- 1000
ice <- data_frame(sim = rpois(n_obs, lambda = mu))

ggplot(ice, aes(sim)) +
  geom_histogram(binwidth = 1)
```

The mean of this sample is `r mean(ice$sim)`, which we will treat as the population mean $\mu$. Remember that in the real world, we do not know $\mu$ because we have not observed all members of the population. Instead, we use the sample to estimate $\hat{\mu}$ on the assumption that the sample mean approximates the true mean.

The [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) is the most likely population distribution as it describes the count of event over time. The probability mass function of the Poisson distribution is

$$P(Y_i = y_i | \mu) = \frac{\mu^{y_i} e^{-\mu}}{y_i!}$$

where $\mu$ is the event rate (average number of events per interval), $e$ is Euler's number, $y_i$ is an integer with range $[0, \infty]$, and $y_i!$ is the factorial of $y_i$. The mean $\mu$ and variance $\sigma$ of a Poisson distribution are the same parameterm and hence are both defined by $\mu$.

Because we are estimating $\mu$ from a sample, we should also estimate the **standard error** of the sample mean. This is necessary because any random sample drawn from a population will not exactly reproduce the population. We need to account for sampling error by estimating how much our sample mean $\hat{\mu}$ might differ from the true mean $\mu$.

The distribution of the mean of a set of samples is approximately [normally distributed](https://en.wikipedia.org/wiki/Normal_distribution).^[As defined by the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).] Therefore the standard error of the sample mean from a Poisson distribution is

$$\sqrt{\frac{\mu}{n}}$$

```{r ice-samp-mean}
mu_samp <- mean(ice$sim)
sem <- sqrt(mu_samp / n_obs)
```

The standard error of the sample mean is $`r sem`$. This is a good estimate **as long as the data generating process actually follows a Poisson distribution**. The Poisson distribution requires [several assumptions](https://en.wikipedia.org/wiki/Poisson_distribution#Assumptions:_When_is_the_Poisson_distribution_an_appropriate_model.3F). If any of these assumptions are violated, then the formula for estimating the standard error of the sample mean $\mu$ will not be accurate.

In that situation, we can use the bootstrap to estimate the standard error without making any distributional assumptions. In this approach, we draw $B$ samples with replacement from the original sample. To estimate the population mean $\mu$ we calculate the mean of the bootstrapped sample means $\hat{\mu}_1, \hat{\mu}_2, \dots, \hat{\mu}_B$. To estimate the standard error of the sampling mean $\hat{\mu}$ we use the formula

$$SE_{B}(\hat{\mu}) = \sqrt{\frac{1}{B-1} \sum_{r = 1}^{B} \left( \hat{\mu}_r - \frac{1}{B} \sum_{r' = 1}^{B} \hat{\mu}_{r'} \right)^2}$$

What this boils down to is calculating the **standard deviation** of all the bootstrapped sample means. That gives us our standard error.

Let's bootstrap our standard error of the mean for our simulated ice cream data. We'll use $B = 1000$ to produce 1000 bootstrapped estimates of the mean, then calculate the standard deviation of them:

```{r ice-boot}
ice_boot <- ice %>%
  modelr::bootstrap(1000) %>%
  mutate(mean = map_dbl(strap, ~ mean(as_tibble(.)$sim, na.rm = TRUE)))
boot_sem <- sd(ice_boot$mean)
```

```{r ice-boot-plot}
ggplot(ice_boot, aes(mean)) +
  geom_histogram(binwidth = .01) +
  geom_vline(aes(xintercept = mu, color = "Population mean"), size = 1) +
  geom_vline(aes(xintercept = mu_samp, color = "Sample mean"), size = 1) +
  geom_vline(aes(xintercept = mean(ice_boot$mean),
                 color = "Bootstrapped mean"), size = 1) +
  geom_vline(aes(xintercept = mean(ice_boot$mean) + 1.96 * boot_sem,
                 color = "Bootstrapped mean"), linetype = 2) +
  geom_vline(aes(xintercept = mean(ice_boot$mean) - 1.96 * boot_sem,
                 color = "Bootstrapped mean"), linetype = 2) +
  geom_vline(aes(xintercept = mu_samp + 1.96 * sem, color = "Sample mean"),
             linetype = 2) +
  geom_vline(aes(xintercept = mu_samp - 1.96 * sem, color = "Sample mean"),
             linetype = 2) +
  scale_color_manual(name = NULL, breaks = c("Population mean", "Sample mean",
                                             "Bootstrapped mean"),
                     values = c("blue", "green", "orange")) +
  labs(x = "Bootstrapped sample mean",
       y = "Count") +
  theme(legend.position = "bottom")
```

The bootstrap estimate of the standard error of the sample mean is `r boot_sem`. Compared to the original estimate of `r sem`, this is slightly closer to the defined population mean, but not by much. Why bother using the bootstrap? Because the bootstrap estimator will be more accurate **when the distributional assumptions are not met**.

Let's simulate the results once again, but draw the observations from a combination of the Poisson distribution and [uniform distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)).

```{r ice-sim2}
# simulate the sample
set.seed(113)
ice2 <- data_frame(sim = c(rpois(n_obs / 2, lambda = mu),
                           round(runif(n_obs / 2, min = 0, max = 10))))

# plot the sample distribution
ggplot(ice2, aes(sim)) +
  geom_histogram(binwidth = 1)

# calculate sample mean and standard error
mu2_samp <- mean(ice2$sim)
sem2 <- sqrt(mu2_samp / n_obs)

# calculate the bootstrap
ice2_boot <- ice2 %>%
  modelr::bootstrap(1000) %>%
  mutate(mean = map_dbl(strap, ~ mean(as_tibble(.)$sim, na.rm = TRUE)))
boot2_sem <- sd(ice2_boot$mean)

# plot the bootstrapped distribution
ggplot(ice2_boot, aes(mean)) +
  geom_histogram(binwidth = .01) +
  geom_vline(aes(xintercept = mu, color = "Population mean"), size = 1) +
  geom_vline(aes(xintercept = mu2_samp, color = "Sample mean"), size = 1) +
  geom_vline(aes(xintercept = mean(ice2_boot$mean),
                 color = "Bootstrapped mean"), size = 1) +
  geom_vline(aes(xintercept = mean(ice2_boot$mean) + 1.96 * boot2_sem,
                 color = "Bootstrapped mean"), linetype = 2) +
  geom_vline(aes(xintercept = mean(ice2_boot$mean) - 1.96 * boot2_sem,
                 color = "Bootstrapped mean"), linetype = 2) +
  geom_vline(aes(xintercept = mu2_samp + 1.96 * sem2, color = "Sample mean"),
             linetype = 2) +
  geom_vline(aes(xintercept = mu2_samp - 1.96 * sem2, color = "Sample mean"),
             linetype = 2) +
  scale_color_manual(name = NULL, breaks = c("Population mean", "Sample mean",
                                             "Bootstrapped mean"),
                     values = c("blue", "green", "orange")) +
  labs(x = "Bootstrapped sample mean",
       y = "Count") +
  theme(legend.position = "bottom")
```

The population mean $\mu$ is still defined as 5, but now look what happens to the standard errors of the estimates. The estimated means are identical under the formula-based or bootstrapped approaches (`r mu2_samp`), however the standard error for the sample-based approach is $`r sem2`$, compared to `r boot2_sem`. Because the bootstrap approach generates its estimate of the standard error directly from the data, the bootstrapped 95% confidence interval includes the population mean. However the 95% confidence interval under the formula-based method does not include the population mean. In this case we are better off using the bootstrapped standard error rather than using the formula for the Poisson distribution.

## Estimating the accuracy of a linear regression model

In a linear regression model, the standard errors are statistical estimates of the average amount that the estimated parameters $\hat{\beta}$ differ from the true population parameters $\beta$. The formula for estimating standard errors for a linear regression model is:

$$\widehat{s.e.}(\hat{\beta}_j) = \sqrt{\hat{\sigma}^{2} (X^{T}X)^{-1}_{jj}}$$

More simply this is the square root of the diagonal of the [variance-covariance matrix](https://en.wikipedia.org/wiki/Ordinary_least_squares#Finite_sample_properties). For the formula to hold, we make certain assumptions, including that our estimate of $\sigma^2$ is accurate and that any variability in the model after we account for $X$ is the result of the errors $\epsilon$. If these assumptions are wrong, then our estimates of the standard errors will also be wrong.

Let's revisit our horsepower and mpg linear model.

```{r auto-boot}
# plot the data and model
ggplot(Auto, aes(horsepower, mpg)) +
  geom_point() +
  geom_smooth(method = "lm")

# traditional parameter estimates and standard errors
auto_lm <- lm(mpg ~ poly(horsepower, 1), data = Auto)
tidy(auto_lm)

# bootstrapped estimates of the parameter estimates and standard errors
auto_boot <- Auto %>%
  modelr::bootstrap(1000) %>%
  mutate(model = map(strap, ~ lm(mpg ~ horsepower, data = .)),
         coef = map(model, tidy))

auto_boot %>%
  unnest(coef) %>%
  group_by(term) %>%
  summarize(est.boot = mean(estimate),
            se.boot = sd(estimate, na.rm = TRUE))
```

The bootstrapped estimates of parameters are virtually identical, however the standard errors on the bootstrap estimates are slightly larger. This is because they do not rely on any distributional assumptions, whereas the traditional estimates do. Recall from the [demonstration above](#regression) that the relationship between horsepower and mpg is non-linear, so the residuals from a linear model will be inflated, and the residuals are used to estimate $\sigma^2$. The bootstrap method is not biased by these assumptions and gives us a more robust estimate.

If we compare the traditional and bootstrap estimates for the polynomial regression model, we find more similarity in our results:

```{r auto-boot-sq}
# traditional parameter estimates and standard errors
auto2_lm <- lm(mpg ~ poly(horsepower, 2), data = Auto)
tidy(auto2_lm)

# bootstrapped estimates of the parameter estimates and standard errors
auto2_boot <- Auto %>%
  modelr::bootstrap(1000) %>%
  mutate(model = map(strap, ~ lm(mpg ~ poly(horsepower, 2), data = .)),
         coef = map(model, tidy))

auto2_boot %>%
  unnest(coef) %>%
  group_by(term) %>%
  summarize(est.boot = mean(estimate),
            se.boot = sd(estimate, na.rm = TRUE))
```

# Acknowledgements {.toc-ignore}

* Bootstrap standard error of the mean example derived from [A gentle introduction to bootstrapping
](http://t-redactyl.io/blog/2015/09/a-gentle-introduction-to-bootstrapping.html).
* "Why use the bootstrap?" reproduced from [Explaining to laypeople why bootstrapping works - Stack Overflow](http://stats.stackexchange.com/a/26093), licensed under the [CC BY-SA 3.0 Creative Commons License](https://creativecommons.org/licenses/by-sa/3.0/).

# Session Info {.toc-ignore}

```{r child='_sessioninfo.Rmd'}
```



