---
title: "Statistical learning: support vector machines"
author: "MACS 30100 - Perspectives on Computational Modeling"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Objectives

* Define the maximal margin classifier
* Define the support vector classifier and discuss the logic of this approach
* Define support vector machines (SVM) and non-linear decision boundaries
* Apply SVM classification to example data sets and compare with alternative statistical learning models

```{r packages, cache = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(tree)
library(randomForest)
library(stringr)
library(ISLR)
library(titanic)
library(rcfss)
library(pROC)
library(gbm)
library(e1071)
library(grid)
library(gridExtra)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal())
```

**Support vector machines** (SVMs) are a popular statistical learning method for classification tasks.^[Though they can also be applied to regression on continuous response variables.] SVMs build on several important concepts, that while related are distinct from one another. We will first discuss the logic of these individual components, then demonstrate how to estimate and interpret SVMs, and compare model results using this method to other statistical learning procedures we have discussed so far.

# Maximal margin classifier

## Hyperplanes

In $p$-dimensional space, a **hyperplane** is a flat subspace of $p - 1$ dimensions that is *affine* (does not need to pass through the origin). In two dimensions, a hyperplane is a flat one-dimensional subspace (also known as a **line**). In three dimensions, a hyper plane is a flat two-dimensional subspace (also known as a **plane**). In higher dimensions it gets harder to visualize this concept, but the definition still holds true.

In two dimensions, the mathematical equation for a hyperplane is:

$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0$$

Any $X = (X_1, X_2)^T$ for which this equation holds is a point on the hyperplane (line). This functional form generalizes to $p$ dimensions quite easily:

$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p = 0$$

Again, for any point $X = (X_1, X_2, \dots, X_p)^T$ in $p$-dimensional space (i.e. a vector of length $p$) that equals 0, then $X$ lies on the hyperplane.

For $X$ that does not meet this condition, then the data point lies on either side of the hyperplane:

$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p > 0$$

$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p < 0$$

The hyperplane therefore divides the $p$-dimensional space into two halves. To determine on which side of the hyperplane an observation lies, we simply calculate the sign of the corresponding hyperplane equation.

```{r hyperplane}
sim_hyper <- data_frame(x1 = seq(-1.5, 1.5, length.out = 20),
                        x2 = seq(-1.5, 1.5, length.out = 20)) %>%
  expand(x1, x2) %>%
  mutate(y = 1 + 2 * x1 + 3 * x2,
         group = ifelse(y < 0, -1,
                        ifelse(y > 0, 1, 0)),
         group = factor(group))

sim_hyper_line <- data_frame(x1 = seq(-1.5, 1.5, length.out = 20),
                             x2 = (-1 - 2 * x1) / 3)

ggplot(sim_hyper, aes(x1, x2, color = group)) +
  geom_point() +
  geom_line(data = sim_hyper_line, aes(color = NULL)) +
  labs(title = "Hyperplane in two dimensions") +
  theme(legend.position = "none")
```

## Classification using a separating hyperplane

Let's represent a hypothetical classification problem as the following: suppose we have an $n \times p$ data matrix $\mathbf{X}$ that consists of $n$ training observations with $p$ predictors in $p$-dimensional space:

$$x_1 = \begin{pmatrix}
  x_{11} \\
  \vdots \\
  x_{1p}
 \end{pmatrix},
 \dots, x_n = \begin{pmatrix}
  x_{n1} \\
  \vdots \\
  x_{np}
 \end{pmatrix}$$

These observations fall into one of two classes $y_1, \dots, y_n \in \{-1, 1 \}$ where $-1$ and $1$ represent two separate classes or categories. We also have a test observation $x^*$ which is a $p$-vector of observed predictors $x^* = (x_1^*, \dots, x_p^*)$. We want to develop a model that classifies the test observation correctly given our knowledge of the training observations. Previously we have used methods such as logistic regression (where the response variable is coded $\{0, 1 \}$) and decision trees to perform this task. Now we want to use a hyperplane to **separate** the training observations into the two possible classes.

A **separating hyperplane** perfectly separates training observations into their class labels. Observations in the blue class are coded as $y_i = 1$ those from the red class as $y_i = -1$. So a separating hyperplane takes on the properties:

$$\beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} > 0, \text{if } y_i = 1$$
$$\beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} < 0, \text{if } y_i = -1$$

```{r sim}
sim <- data_frame(x1 = runif(20, -2, 2),
                  x2 = runif(20, -2, 2),
                  y = ifelse(1 + 2 * x1 + 3 * x2 < 0, -1, 1)) %>%
  mutate_each(funs(ifelse(y == 1, . + 1.5, .)), x2) %>%
  mutate(y = factor(y, levels = c(-1, 1))) %>%
  mutate(line1 = (-1 - 2 * x1) / 3,
         line2 = .5 + (-1 - 1.5 * x1) / 3,
         line3 = .25 - .05 * x1)

ggplot(sim, aes(x1)) +
  geom_point(aes(y = x2, color = y)) +
  geom_line(aes(y = line1, color = NULL)) +
  geom_line(aes(y = line2, color = NULL)) +
  geom_line(aes(y = line3, color = NULL)) +
  labs(title = "Examples of separating hyperplanes") +
  theme(legend.position = "none")
```

If a separating hyperplane exists, then we can classify test observations based on their location relative to the hyperplane:

```{r sim-decision}
sim_mod <- svm(y ~ x1 + x2, data = sim, kernel = "linear", cost = 1e05,
               scale = FALSE)
sim_coef <- c(sim_mod$rho, t(sim_mod$coefs) %*% sim_mod$SV)

sim_grid <- data_frame(x1 = seq(-2, 2, length.out = 100),
                  x2 = seq(-2, 3.5, length.out = 100)) %>%
  expand(x1, x2) %>%
  mutate(y = ifelse(-sim_coef[[1]] + sim_coef[[2]] * x1 + sim_coef[[3]] * x2 > 0, -1, 1),
         y = factor(y, levels = c(-1, 1)))

sim_plane <- data_frame(x1 = seq(-2, 2, length.out = 100),
                        x2 = (sim_coef[[1]] - sim_coef[[2]] * x1) / sim_coef[[3]])

ggplot(sim, aes(x1)) +
  geom_point(data = sim_grid, aes(x1, x2, color = y), alpha = .25, size = .25) +
  geom_point(aes(y = x2, color = y)) +
  geom_line(data = sim_plane, aes(x1, x2)) +
  labs(title = "Maximal margin classification") +
  theme(legend.position = "none")
```

Classifications are based off the sign of $f(x^*) = \beta_0 + \beta_1 x_1^* + \dots + \beta_p x_p^*$. If $f(x^*)$ is positive, then we predict the test observation is $1$. If $f(x^*)$ is negative, then we predict the test observation is $-1$. We can also consider the **magnitude** of $f(x^*)$: the farther the magnitude is away from zero, then the farther the test observation falls from the hyperplane. We can be more confident of our predictions for observations far from the hyperplane, and less so for observations near the hyperplane (i.e. $f(x^*)$ close to zero). The classifier resulting from the separating hyperplane $f(x^*) = \beta_0 + \beta_1 x_1^* + \dots + \beta_p x_p^*$ is a **linear decision boundary** because the function itself is a linear form.

## Maximal margin classifier

As we saw previously, if the data can be perfectly separated by a hyperplane it is likely true that there are **multiple potential separating hyperplanes**. We need a method for identifying the *optimal* separating hyperplane. This is known as the **maximal margin hyperplane**, which is the separating hyperplane that is farthest from the training observations. The **margin** is the smallest possible (perpendicular) distance between a training observation and the separating hyperplane. This distance is simply $\hat{f}(x_i)$. The maximal margin hyperplane defines the hyperplane that minimizes the marginal distance across all training observations, and can be used to classify the test observation $x^*$ based on which side of the hyperplane it lies. This is known as the **maximal margin classifier**. The expectation is that a classifier with a large margin for the training observations will also have a large margin for the test observations, leading to accurate classifications. As with the other methods we have discussed so far, this is an assumption and it is still possible to overfit the training data using the maximal margin classifier.

```{r sim-margin}
sim_pred <- predict(sim_mod, sim, decision.values = TRUE)
sim_dist <- attr(sim_pred, "decision.values")

ggplot(sim, aes(x1)) +
  geom_point(aes(y = x2, color = y)) +
  geom_point(data = sim_grid, aes(x1, x2, color = y), alpha = .1, size = .25) +
  geom_line(data = sim_plane, aes(x1, x2)) +
    geom_line(data = mutate(sim_plane, x2 = x2 - min(abs(sim_dist))),
              aes(x1, x2), linetype = 2) +
    geom_line(data = mutate(sim_plane, x2 = x2 + min(abs(sim_dist))),
              aes(x1, x2), linetype = 2) +
  labs(title = "Maximal margin classification") +
  theme(legend.position = "none")
```

Two observations are equidistant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin. These observations are called the **support vectors**. They are vectors in $p$-dimensional space and "support" the maximal margin hyperplane because if the observations shifted at all in their predictor values $X$, then the maximal margin hyperplane would shift as well. In fact, the maximal margin hyperplane is defined entirely by the support vectors; changes to the other observations would not effect the separating hyperplane as long as the changed observations do not cross the boundary set by the margin.

### Constructing the maximal margin hyperplane

Constructing the maximal margin hyperplane is a (relatively) straight forward affair. Consider a set of $n$ training observations with some number of real number predictors $x_1, \dots, x_n \in \mathbb{R}^p$ and associated class labels $y_1, \dots, y_n \in \{-1, 1\}$. We want to solve the optimization problem:

$$\begin{aligned}
& \underset{\beta_0, \beta_1, \dots, \beta_p}{\text{maximize}} & & M \\
& \text{s.t.} & &  \sum_{j=1}^p \beta_j^2 = 1, \\
& & & y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) \geq M \; \forall \; i = 1, \dots, n \\
\end{aligned}$$

This is simpler than it looks. $y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) \geq M \; \forall \; i = 1, \dots, n$ requires the maximal margin hyperplane to sort observations on the correct side of the hyperplane with some amount of cushion, provided $M$ is positive. The requirement $\sum_{j=1}^p \beta_j^2 = 1$ means that not only are the observations sorted onto the correct sides of the hyperplane, but that the function $y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip})$ defines the **perpendicular distance** between the observation $y_i$ and the hyperplane. Therefore $M$ defines the margin of the hyperplane (i.e. the amount of cushion between the hyperplane and the closest training observations), so we select values for the parameters $\beta_0, \beta_1, \dots, \beta_p$ to maximize $M$; that is, obtain the largest amount of cushion possible given the training observations.

### Non-separable cases

Unfortunately the maximal margin classifier only works if there exists a separating hyperplane for the data. If the cases cannot be perfectly separated by a hyperplane, then we can never satisfy the conditions of the maximal margin classifier.

```{r sim-nosep}
data_frame(x1 = runif(20, -2, 2),
           x2 = runif(20, -2, 2),
           y = c(rep(-1, 10), rep(1, 10))) %>%
  mutate(y = factor(y, levels = c(-1, 1))) %>%
  ggplot(aes(x1, x2, color = y)) +
  geom_point() +
  labs(title = "Non-separable data") +
  theme(legend.position = "none")
```

# Support vector classifier

**Support vector classifiers** relax the requirement of the maximal margin classifier by allowing the separating hyperplane to not **perfectly** separate the observations; instead, it can make some errors. This is reasonable when:

1. There exists no perfectly separating hyperplane
1. A perfectly separating hyperplane is too sensitive to individual training observations, generating potentially very small margins or overfitting the training set.^[Remember that we can use the perpendicular distance from the hyperplane as a measure of confidence in our predictions, so the new training observation diminishes our confidence for quite a few of the red training observations.]

```{r sim-sensitive}
# original model
sensitive <- data_frame(x1 = runif(20, -2, 2),
                  x2 = runif(20, -2, 2),
                  y = ifelse(1 + 2 * x1 + 3 * x2 < 0, -1, 1)) %>%
  mutate_each(funs(ifelse(y == 1, . + .5, .)), x2) %>%
  mutate(y = factor(y, levels = c(-1, 1)))

sens_mod <- svm(y ~ x1 + x2, data = sensitive, kernel = "linear",
                cost = 1e05, scale = FALSE)
sens_coef <- c(sens_mod$rho, t(sens_mod$coefs) %*% sens_mod$SV)
sens_plane <- data_frame(x1 = seq(-2, 2, length.out = 100),
                        x2 = (sens_coef[[1]] - sens_coef[[2]] * x1) / sens_coef[[3]])

ggplot(sensitive, aes(x1)) +
  geom_point(aes(y = x2, color = y)) +
  geom_line(data = sens_plane, aes(x1, x2)) +
  labs(title = "Maximal margin classification") +
  theme(legend.position = "none")

# slight tweak
sensitive2 <- data_frame(x1 = with(sensitive, x1[which(x2 == max(x2[y == -1]))]),
                         x2 = with(sensitive, max(x2[y == -1])) + .1,
                         y = factor(1, levels = c(-1, 1))) %>%
  bind_rows(sensitive)

sens2_mod <- svm(y ~ x1 + x2, data = sensitive2, kernel = "linear",
                cost = 1e05, scale = FALSE)
sens2_coef <- c(sens2_mod$rho, t(sens2_mod$coefs) %*% sens2_mod$SV)
sens2_plane <- data_frame(x1 = seq(-2, 2, length.out = 100),
                        x2 = (sens2_coef[[1]] - sens2_coef[[2]] * x1) / sens2_coef[[3]])

ggplot(sensitive2, aes(x1)) +
  geom_point(aes(y = x2, color = y)) +
  geom_line(data = sens2_plane, aes(x1, x2)) +
  geom_line(data = sens_plane, aes(x1, x2), linetype = 2) +
  labs(title = "Maximal margin classification") +
  theme(legend.position = "none")
```

Instead, we want a separating hyperplane that does not perfectly separate the two classes but provides greater robustness to individual observations and better classification of **most** training observations. We are willing to sacrifice accuracy on a few observations if the resulting hyperplane performs better across the remaining observations.

This approach is called the **support vector classifier**. It allows observations to not only exist on the wrong side of the margin (i.e. inside the cushion defined by $M$), but also exist on the wrong side of the hyperplane.

The approach is the same as the maximal margin classifier but the optimization problem is slightly different:

$$\begin{aligned}
& \underset{\beta_0, \beta_1, \dots, \beta_p, \epsilon_1, \dots, \epsilon_n}{\text{maximize}} & & M \\
& \text{s.t.} & &  \sum_{j=1}^p \beta_j^2 = 1, \\
& & & y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) \geq M(1 - \epsilon_i), \\
& & & \epsilon_i \geq 0, \sum_{i = 1}^n \epsilon_i \leq C \\
\end{aligned}$$

As in the maximal margin classifier, we attempt to optimize $M$ to generate the largest possible margin. However now we allow some error $\epsilon_i$ for each observation so that they can fall on the wrong side of the margin or hyperplane.

* If $\epsilon_i = 0$, then the $i$th observation falls on the correct side of the margin.
* If $\epsilon_i > 0$, then the $i$th observation falls on the wrong side of the margin.
* If $\epsilon_i > 1$, then the $i$th observation falls on the wrong side of the hyperplane.

$C$ defines precisely how much error we are willing to tolerate in the resulting separating hyperplane. The sum of the errors for all training observations cannot exceed $C$. Larger values of $C$ permit more overall error in the separating hyperplane and lead to larger margins, and smaller values of $C$ tolerate less error and produce smaller margins. If $C = 0$ then we do not tolerate any error in the separating hyperplane, in which case $\epsilon_1, \dots, \epsilon_n = 0$ and we estimate the maximal margin classifier (of course this is only possible if the classes are perfectly separable). Once we solve the optimization problem, we generate predictions the same way as for maximal margin classifiers, based on $f(x^*) = \beta_0 + \beta_1 x_1^* + \dots + \beta_p x_p^*$.

Selecting a value for $C$ is tricky and generally determined through a cross-validation approach to compare support vector classifiers under different values for $C$. When $C$ is small, we generate a model with low-bias (it fits the data well) but high-variance (small changes in the training observations can generate substantial changes in the support vector classifier). If $C$ is large, we generate a model with more bias but less variance.

The important thing to realize is that the support vector classifier is robust, like the maximal margin classifier, to changes in observations outside of the margin. Observations that lie directly on the margin or inside the margin but on the correct side of the hyperplane are **support vectors**. The support vector classifier will only change if those observations are adjusted. When $C$ is large, the number of observations falling inside the margin increases and therefore the number of support vectors also increases.

```{r sim-c, fig.asp=1}
sim_c <- data_frame(x1 = rnorm(20),
                    x2 = rnorm(20),
                    y = ifelse(2 * x1 + x2 + rnorm(20, 0, .25) < 0, -1, 1)) %>%
  mutate(y = factor(y, levels = c(-1, 1)))

plot_svm <- function(df, cost = 1){
  # estimate model
  sim_mod <- svm(y ~ x1 + x2, data = df, kernel = "linear",
                 cost = cost,
                 scale = FALSE)
  
  # extract separating hyperplane
  sim_coef <- c(sim_mod$rho, t(sim_mod$coefs) %*% sim_mod$SV)
  sim_plane <- data_frame(x1 = seq(min(df$x1), max(df$x1), length.out = 100),
                          x2 = (-sim_coef[[1]] - sim_coef[[2]] * x1) / sim_coef[[3]])
  
  # extract properties to draw margins
  sim_pred <- predict(sim_mod, df, decision.values = TRUE)
  sim_dist <- attr(sim_pred, "decision.values")
  
  ggplot(df, aes(x1)) +
    geom_point(aes(y = x2, color = y)) +
    geom_line(data = sim_plane, aes(x1, x2)) +
    geom_line(data = mutate(sim_plane, x2 = x2 - min(abs(sim_dist))),
              aes(x1, x2), linetype = 2) +
    geom_line(data = mutate(sim_plane, x2 = x2 + min(abs(sim_dist))),
              aes(x1, x2), linetype = 2) +
    labs(subtitle = str_c("Cost = ", cost)) +
    coord_equal(xlim = range(df$x1),
                    ylim = range(df$x2)) +
    theme(legend.position = "none")
}

grid.arrange(grobs = list(plot_svm(sim_c, cost = 1),
                  plot_svm(sim_c, cost = 10),
                  plot_svm(sim_c, cost = 100),
                  plot_svm(sim_c, cost = 200)), ncol = 2)
```

# Support vector machines

## Non-linear decision boundaries

So far we have only demonstrated the support vector classifier with a **linear decision boundary**. But as with linear regression, we also know there are [methods of extending the linear framework to account for non-linear relationships](persp007_nonlinear.html). Consider the following relationship:

```{r sim-nonlinear}
set.seed(1)
x <- matrix(rnorm(200 * 2), ncol = 2)
x[1:100, ] <- x[1:100, ] + 2
x[101:150, ] <- x[101:150, ] - 2
y <- c(rep(1, 150), rep(2, 50))
sim_nonlm <- data.frame(x = x, y = as.factor(y)) %>%
  as_tibble %>%
  rename(x1 = x.1,
         x2 = x.2)

radial_p <- ggplot(sim_nonlm, aes(x1, x2, color = y)) +
  geom_point() +
  theme(legend.position = "none")
radial_p
```

A support vector classifier with a linear decision boundary would perform very poorly on this data.

We could go the route we discussed before and relax the linearity assumption by adding quadratic or cubic terms to address the non-linearity. For instance, adding a quadratic term would change the optimization problem to using $2p$ features:

$$X_1, X_1^2, X_2, X_2^2, \dots, X_p, X_p^2$$

And therefore the optimization problem becomes:

$$\begin{aligned}
& \underset{\beta_0, \beta_{11}, \beta_{12}, \dots, \beta_{p1}, \beta_{p2}, \epsilon_1, \dots, \epsilon_n}{\text{maximize}} & & M \\
& \text{s.t.} & & y_i \left( \beta_0 + \sum_{j = 1}^p \beta_{j1} x_{ij} + \sum_{j = 1}^p \beta_{j2} x_{ij}^2 \right) \geq M(1 - \epsilon_i), \\
& & & \epsilon_i \geq 0, \sum_{i = 1}^n \epsilon_i \leq C, \sum_{j = 1}^p \sum_{k = 1}^2 \beta_{jk}^2 = 1 \\
\end{aligned}$$

The problem with this approach is that as you add polynomial terms (or interactions or splines) you increase the **feature space** used to generate the decision boundary and the separating hyperplane (i.e. the total number of predictors increases). Maximizing this optimization problem is already computationally intensive: if you continue to increase the number of features, computing the support vector classifier becomes much more difficult and inefficient, and may even become impossible.

## Support vector machines

The **support vector machine** is an extension of the support vector classifier that enlarges the feature space by using **kernels**. Kernels are a computationally efficient method for extending the feature space to accomodate a non-linear decision boundary.

Computing the support vector classifier involves the **inner products** of the observations, rather than the observations themselves.^[Like how boosting uses the residuals of the response variable $Y$, rather than $Y$ itself.] The inner product of two $r$-length vectors $a$ and $b$ is defined as $\langle a,b \rangle = \sum_{i = 1}^r a_i b_i$.

```{r inner-prod}
(x <- 1:5)
(y <- 1:5)

x %*% y
```

So the inner product of two observations is:

$$\langle x_i, x_{i'} \rangle = \sum_{j = 1}^p x_{ij} x_{i'j}$$

The linear support vector can be written as:

$$f(x) = \beta_0 + \sum_{i = 1}^n \alpha_i \langle x, x_i \rangle$$

where there are $n$ parameters $\alpha_i, i = 1, \dots, n$, one per training observation. To estimate the parameters $\alpha_1, \dots, \alpha_n, \beta_0$, we just need to calculate the inner products between all pairs of training observations. However for observations which are not also support vectors, $\alpha_i$ is actually zero. So in fact, we only need to calculate the inner products for support vectors $\mathbb{S}$ which reduces the complexity of this task:

$$f(x) = \beta_0 + \sum_{i \in \mathbb{S}} \alpha_i \langle x, x_i \rangle$$

### Kernels

Now rather than using the actual inner product,

$$\langle x_i, x_{i'} \rangle = \sum_{j = 1}^p x_{ij} x_{i'j}$$

instead we can use a **generalization** of the inner product following some functional form $K$ which we will call a kernel:

$$K(x_i, x_{i'})$$

A kernel calculates the similarity of two observations. For example,

$$K(x_i, x_{i'}) = \sum_{j = 1}^p x_{ij} x_{i'j}$$

generates the support vector classifier, also known as the **linear kernel**. Alternatively, we could use a different kernel function such as:

$$K(x_i, x_{i'}) = (1 + \sum_{j = 1}^p x_{ij} x_{i'j})^d$$

This is called the **polynomial kernel** of degree $d$ where $d$ is some positive integer. This will generate a much more flexible decision boundary, similar to how using a spline in linear regression generates a flexible, non-linear functional form. To use this kernel in a support vector classifier, the functional form becomes:

$$f(x) = \beta_0 + \sum_{i \in \mathbb{S}} \alpha_i K(x,x_i)$$

```{r svm-poly}
sim_nonlm <- data_frame(x1 = runif(100, -2, 2),
                  x2 = runif(100, -2, 2),
                  y = ifelse(x1 + x1^2 + x1^3 - x2 < 0 +
                               rnorm(100, 0, 1), -1, 1)) %>%
  mutate(y = factor(y, levels = c(-1, 1)))

ggplot(sim_nonlm, aes(x1, x2, color = y)) +
  geom_point() +
  theme(legend.position = "none")

svm(y ~ x1 + x2, data = sim_nonlm, kernel = "polynomial", scale = FALSE, cost = 1) %>%
  plot(sim_nonlm, x2 ~ x1)
```

Another choice is the **radial kernel**:

$$K(x_i, x_{i'}) = \exp(- \gamma \sum_{j=1}^p (x_{ij} - x_{i'j})^2)$$

where $\gamma$ is some positive constant. Radial kernels work by localizing predictions for test observations based on their Euclidian distance to nearby training observations.

```{r svm-radial}
sim_rad_mod <- svm(y ~ x1 + x2, data = sim_nonlm,
                     kernel = "radial", cost = 5, scale = FALSE)

radial_p
plot(sim_rad_mod, sim_nonlm, x2 ~ x1)
```

Kernels are better to use for support vector machines than other non-linear approachs because they do not enlarge the feature space. That is, you need to compute $K(x_i, x_{i'})$ for all $\binom{n}{2}$ distinct pairs $i, i'$, but $p$ itself remains the same. **You do not need to explicitly enlarge the feature space to accomplish this task**. The total number of features/predictors/independent variables in the model remains the same, so you can more easily compute the SVM.

# Applying and interpreting SVMs

SVMs are generally used for **prediction models**. They generate predicted classes for test observations and we can assess confidence in the model and overall model fit using standard metric. However SVMs are not good for conducting inference, since there are no easy methods for interpreting the relative importance and influence of individual predictors on the separating hyperplane. Regression coefficients are generally easy to interpret, and even tree-based methods have visual and statistical interpretations (variable importance plots) of the individual predictors. Generally SVMs are interpreted by assessing overall model fit and error rates, using a combination of cross-validation methods and visuals such as ROC curves.

## Titanic

Let's try this method out on our trusty Titanic dataset, using age and gender to predict survival. First we'll split our dataset into training and test sets.

```{r titanic-data}
titanic <- titanic_train %>%
  as_tibble %>%
  select(-Name, -Ticket, -Cabin, -PassengerId) %>%
  mutate_each(funs(as.factor(.)), Survived, Pclass, Embarked) %>%
  na.omit

titanic_split <- resample_partition(titanic, p = c("test" = .3, "train" = .7))
```

Our first attempt will use a linear kernel (i.e. support vector classifier) and we'll use 10-fold cross-validation to determine the optimal cost parameter $C$.

```{r titanic-linear-tune, dependson="titanic-data"}
titanic_tune <- tune(svm, Survived ~ Age + Fare, data = as_tibble(titanic_split$train),
                     kernel = "linear",
                     range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(titanic_tune)
```

$C = 1$ produces the lower CV error rate, so let's use that model for estimating model fit using a [ROC curve](persp004_logistic_regression.html#receiver_operating_characteristics_(roc)_curve).

```{r titanic-linear-pred}
titanic_best <- titanic_tune$best.model
summary(titanic_best)

# get predictions for test set
fitted <- predict(titanic_best, as_tibble(titanic_split$test), decision.values = TRUE) %>%
  attributes

roc_line <- roc(as_tibble(titanic_split$test)$Survived, fitted$decision.values)
plot(roc_line)
auc(roc_line)
```

How does this compare to a polynomial kernel SVM?

```{r titanic-svm-poly}
titanic_poly_tune <- tune(svm, Survived ~ Age + Fare, data = as_tibble(titanic_split$train),
                     kernel = "polynomial",
                     range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(titanic_poly_tune)

titanic_poly_best <- titanic_poly_tune$best.model
summary(titanic_poly_best)

# get predictions for test set
fitted <- predict(titanic_poly_best, as_tibble(titanic_split$test), decision.values = TRUE) %>%
  attributes

roc_poly <- roc(as_tibble(titanic_split$test)$Survived, fitted$decision.values)
plot(roc_poly)
auc(roc_poly)
```

Not quite as good. The optimal cost parameter is smaller ($.1$), but the associated CV error rate is higher than the linear kernel and the resulting test AUC is smaller. How does this stack up against the radial kernel?

```{r titanic-svm-radial}
titanic_rad_tune <- tune(svm, Survived ~ Age + Fare, data = as_tibble(titanic_split$train),
                     kernel = "radial",
                     range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(titanic_rad_tune)

titanic_rad_best <- titanic_rad_tune$best.model
summary(titanic_rad_best)

# get predictions for test set
fitted <- predict(titanic_rad_best, as_tibble(titanic_split$test), decision.values = TRUE) %>%
  attributes

roc_rad <- roc(as_tibble(titanic_split$test)$Survived, fitted$decision.values)
plot(roc_rad)
auc(roc_rad)
```

The radial improves upon both the polynomial and the linear SVMs. The CV error rate is lower and the test AUC is higher.

It's easier to compare if we plot the ROC curves on the same plotting window:

```{r titanic-roc-compare}
plot(roc_line, print.auc = TRUE, col = "blue")
plot(roc_poly, print.auc = TRUE, col = "red", print.auc.y = .4, add = TRUE)
plot(roc_rad, print.auc = TRUE, col = "orange", print.auc.y = .3, add = TRUE)
```

Based on our predictions from the test set, the radial SVM performs the best on the AUC, followed by the linear SVM, and worst of all the polynomial SVM.

## Voter turnout

Let's test the SVM method on our voter turnout data. Again, let's start by splitting the data into training and test sets.^[Why use the validation set approach? We've discussed the [inadequacies](persp006_resampling.html#drawbacks_to_the_validation_set_approach) of it before. We could use $k$-fold cross validation instead, however setting this up with the proper code would be much more complicated. When conducting exploratory analysis, you don't necessarily need to do this on your first pass through the data. Certainly I recommend using CV to validate your models and compare them before publishing anything, but for this application I think the validation set approach works fine.]

```{r vote96}
(mh <- read_csv("data/mental_health.csv") %>%
  mutate_each(funs(as.factor(.)), vote96, black, female, married) %>%
  na.omit)

mh_split <- resample_partition(mh, p = c("test" = .3, "train" = .7))
```

### SVM

Next let's compare a few different SVM models. Again we'll use 10-fold CV on the training set to determine the optimal cost parameter.

#### Linear kernel

```{r vote96-svm-line, dependson="vote96"}
mh_lin_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "linear",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_lin_tune)

mh_lin <- mh_lin_tune$best.model
summary(mh_lin)
fitted <- predict(mh_lin, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_line <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
plot(roc_line)
auc(roc_line)
```

### Polynomial kernel

```{r vote96-svm-poly, dependson="vote96"}
mh_poly_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "polynomial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_poly_tune)

mh_poly <- mh_poly_tune$best.model
summary(mh_poly)
fitted <- predict(mh_poly, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_poly <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
plot(roc_poly)
auc(roc_poly)
```

### Radial kernel

```{r vote96-svm-rad, dependson="vote96"}
mh_rad_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "radial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_rad_tune)

mh_rad <- mh_rad_tune$best.model
summary(mh_rad)
fitted <- predict(mh_rad, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_rad <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
plot(roc_rad)
auc(roc_rad)
```

```{r mh-roc-compare, dependson=c("vote96-svm-line","vote96-svm-poly","vote96-svm-rad")}
plot(roc_line, print.auc = TRUE, col = "blue")
plot(roc_poly, print.auc = TRUE, col = "red", print.auc.y = .4, add = TRUE)
plot(roc_rad, print.auc = TRUE, col = "orange", print.auc.y = .3, add = TRUE)
```

SVM kernel | CV training error rate
-----------|-----------------------
Linear     | `r mh_lin_tune$best.performance`
Polynomial | `r mh_poly_tune$best.performance`
Radial     | `r mh_rad_tune$best.performance`

This time the SVM with the highest AUC is the linear model, followed by the radial and then the polynomial SVM. Interestingly, the linear SVM had the highest training error rate (cross-validated), followed by radial, and then polynomial with the lowest error rate. These are cross-validated measures, so it's not as if they should be heavily biased. However they are all within 1 percentage point of each other, so the differences may not actually be that substantial. Further exploration could be warranted here.

We could tinker with the parameters for the polynomial and radial kernel SVMs, adjusting the number of degrees in the polynomial SVM and testing different constants $\gamma$ for the radial SVM, again using 10-fold CV to select the optimal values. Instead though, let's see how the SVM with the highest AUC (linear) stacks up with some of the other statistical learning methods we could apply.

### Logistic regression

```{r vote96-logit}
mh_logit <- glm(vote96 ~ ., data = as_tibble(mh_split$train), family = binomial)
summary(mh_logit)

fitted <- predict(mh_logit, as_tibble(mh_split$test), type = "response")
logit_err <- mean(as_tibble(mh_split$test)$vote96 != round(fitted))

roc_logit <- roc(as_tibble(mh_split$test)$vote96, fitted)
plot(roc_logit)
auc(roc_logit)
```

The test error rate for the logistic regression model is `r logit_err`.

### Decision tree

```{r vote96-tree}
mh_tree <- tree(vote96 ~ ., data = as_tibble(mh_split$train))
mh_tree

plot(mh_tree)
text(mh_tree, pretty = 0)

fitted <- predict(mh_tree, as_tibble(mh_split$test), type = "class")
tree_err <- mean(as_tibble(mh_split$test)$vote96 != fitted)

roc_tree <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
plot(roc_tree)
auc(roc_tree)
```

The test error rate for the decision tree model is `r tree_err`.

### Bagging

```{r vote96-bag}
mh_bag <- randomForest(vote96 ~ ., data = as_tibble(mh_split$train),
                         mtry = 7)
mh_bag

varImpPlot(mh_bag)

fitted <- predict(mh_bag, as_tibble(mh_split$test), type = "prob")[,2]

roc_bag <- roc(as_tibble(mh_split$test)$vote96, fitted)
plot(roc_bag)
auc(roc_bag)
```

### Random forest

```{r vote96-rf}
mh_rf <- randomForest(vote96 ~ ., data = as_tibble(mh_split$train))
mh_rf

varImpPlot(mh_rf)

fitted <- predict(mh_rf, as_tibble(mh_split$test), type = "prob")[,2]

roc_rf <- roc(as_tibble(mh_split$test)$vote96, fitted)
plot(roc_rf)
auc(roc_rf)
```

### Compare the ROC curves

```{r vote96-compare-roc}
plot(roc_poly, print.auc = TRUE, col = "blue", print.auc.x = .2)
plot(roc_logit, print.auc = TRUE, col = "red", print.auc.x = .2, print.auc.y = .4, add = TRUE)
plot(roc_tree, print.auc = TRUE, col = "orange", print.auc.x = .2, print.auc.y = .3, add = TRUE)
plot(roc_bag, print.auc = TRUE, col = "green", print.auc.x = .2, print.auc.y = .2, add = TRUE)
plot(roc_rf, print.auc = TRUE, col = "purple", print.auc.x = .2, print.auc.y = .1, add = TRUE)
```

* SVM (linear kernel)
* Logistic regression
* Decision tree
* Bagging ($n = 500$)
* Random forest ($n = 500, m = \sqrt{p}$)

Based solely on the test AUC, logistic regression and random forest provides the highest predictive accuracy, slightly better than the linear kernel SVM. Decision tree performs the worst, though admittedly AUC is biased against it since all decision trees produce are predictions, not probabilities, so the ROC "curve" is actually a point.

# Session Info {.toc-ignore}

```{r child='_sessioninfo.Rmd'}
```




