---
title: "p-hacking"
author: "MACS 30200 - Perspectives on Computational Research"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Objectives

* Define a p-value
* Identify methods for obtaining a statistically significant p-value
* Critique the "women wearing pink" article for potential p-hacking
* Identify approaches to reduce the chance of p-hacking

```{r packages, cache = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(broom)
library(forcats)
library(modelr)
library(stringr)
library(car)
library(rcfss)
library(coefplot)
library(RColorBrewer)
library(lme4)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal())
```

# What is a p-value?

## Hypothesis testing

When conducting inference, we test hypotheses about real-world relationships using observed data. Because we generally rely on a **sample** rather than the **population**, there is a chance that the results we observe could be purely driven by random chance. We just happened to draw a fluky sample from the population and the statistic of interest we observe is not in fact representative of the broader population.

In the context of regression, we generally test the **null hypothesis** against the **alternative hypothesis**. The null hypothesis $H_0$ states "there is no relationship between $X$ and $Y$", whereas the alternative hypothesis $H_a$ states "there is some relationship between $X$ and $Y$". Mathematically, this corresponds to

* $H_0$: $\beta_1 = 0$
* $H_a$: $\beta_1 \neq 0$

since if $\beta_1 = 0$, then the model reduces to $Y = \beta_0 + \epsilon$ and $X$ is not associated with $Y$. To test the null hypothesis, we need to determine whether the estimated coefficient $\hat{\beta}_1$ is sufficiently far from zero that we can be confident that $\beta_1$ is non-zero.

This determination is based on the **standard error** of the coefficient. If $\text{SE}(\hat{\beta}_1)$ is small, than even relatively small values of $\hat{\beta}_1$ may provide strong evidence that $\beta_1 \neq 0$ and there is a relationship between $X$ and $Y$. If $\text{SE}(\hat{\beta}_1)$ is large, than $\hat{\beta}_1$ must also be large to reject the null hypothesis. In practice, we compute a **t-statistic** given by

$$t = \frac{\hat{\beta}_1 - 0}{\text{SE}(\hat{\beta}_1)}$$

which measures the number of standard deviations that $\hat{\beta}_1$ is away from 0. If there really is no relationship between $X$ and $Y$, then we expect this function to follow a $t$-distribution with $n-2$ degrees of freedom. From this we can calculate the probability of observing any value equal to $|t|$ or larger, assuming $\beta_1=0$. This is the **p-value**. Informally, a small p-value indicates that it is unlikely to observare such substantial association between the predictor and the response due to chance, in the absence of any real association between the predictor and response.

## The importance of the .05 cutoff

![](http://marginalrevolution.com/wp-content/uploads/2014/05/Type-I-and-II-errors1-625x468.jpg)

p-values can fall anywhere between 0 and 1. Setting the cutoff point determines how likely one is to make a type I (false positive) or type II (false negative) error. Historically, the social sciences adopted a cutoff of $p=.05$. Anything below that value is considered "statistically significant". But there is no scientific basis for this threshold - it is an arbitrary standard, to which many academic journals and scholars continue to adhere. 

## What p-values can and cannot do

p-values are a key component of frequentist inference, dating back to the early 1900s. While their usage is widespread in academia, in practice their meaning is actually very limited.

> A p-value is the probability under a specified statistical model that a statistical summary of the data would be equal to or more extreme than its observed values.

**p-values can indicate how incompatible the data are with a specified statistical model**. This is why they are used for null hypothesis testing. The smaller the p-value, the greater the statistical incompatability of the data with the null hypothesis. This incompatability casts doubt on or provides evidence agains the null hypothesis.

**But it does not prove the alternative hypothesis to be true.**

* p-values do not measure the probability that the observed relationship is true, or the probability that the data were produced by random chance alone. p-values cannot be used to prove the null hypothesis:

    ![](http://www.azquotes.com/picture-quotes/quote-the-absence-of-evidence-is-not-the-evidence-of-absence-carl-sagan-43-51-12.jpg)
    
    p-values do not provide any conclusion about the explanation itself, only the probability of the observed data being generated if the null hypothesis is true.
* p-values below an arbitrary threshold should not be used for scientific, business, or policy conclusions. Again, the .05 threshold is a completely arbitrary standard. A statistic with a p-value of $.04$ is not "true" versus a statistic with a p-value of $.06$ is false. p-values are determined not only by the data itself but also by the researcher's decisions used to formulate and estimate the statistical model, and there are many ways to tweak or search for a significant p-value.
* p-values and statistical significance do not measure the size or importance of a result. Statistical significance is not the same thing as scientific, human, or economic significance. Smaller p-values do not imply important effects. p-values are determined by the precision of the estimated statistic, which is in turn influenced by sample size and measurement accuracy. A statistic can have a very small p-value but have an insignificant substantive effect because of a large sample size. Likewise, even large effects can have unimpressive p-values if the sample size is small.
* The p-value does not provide a good measure of evidence regarding a model or hypothesis. Just because a p-value is large does not mean the null hypothesis is true; many other alternative hypotheses could exist to explain the observed data, you just did not test for it.

# How to find a significant p-value

Consider a sample of 100 observations of a continuous outcome of interest $Y$ measured with 10 continuous covariates $\mathbf{X}$. In truth, none of the variables are actually predictive of $Y$ in the population. That is, all the covariates $\mathbf{X}$ and the outcome $Y$ are drawn independently from a normal distribution $\sim N(0,1)$. If we use [ordinary least squares regression](persp003_linear_regression.html) and focus on just a single variable as a predictor, a test of significance will yield $p < .05$ in approximately 5% of the samples. Below I simulate this process 1000 times, and in each simulation estimate a single regression model between $Y$ and a randomly selected $X_k$:

```{r pval-sim}
n_obs <- 100

pval_dist <- function(n_obs){
  x <- replicate(10, rnorm(n_obs))
  y <- rnorm(n_obs)
  
  mod <- lm(y ~ x[, sample(1:10, 1)])
  
  return(tidy(mod)[2,])
}

pvals <- 1000 %>%
  rerun(pval_dist(n_obs)) %>%
  bind_rows %>%
  as_tibble %>%
  mutate(sig = p.value < .05)

ggplot(pvals, aes(p.value, fill = sig)) +
  geom_histogram(binwidth = .025, boundary = 0) +
  labs(title = "Distribution of p-values when null is true",
       x = expression(P),
       y = NULL) +
  theme(legend.position = "none")
```

The distribution of the p-values is approximately uniform and on average 5% of the p-values are $< .05$. In this situation, the p-value and our inferences drawn from the p-value are as we would expect because we conducted exactly one null hypothesis test against the sample of data.

What happens instead if we evaluate multiple variables during each test? That is, in each iteration of our simulation we regress all 10 predictors individually against $Y$. What is the chance that we will find $p < .05$ for at least one of the predictors?

```{r pval-sim-mult-test}
pval_dist_mult <- function(n_obs){
  # generate simulated data
  x <- replicate(10, rnorm(n_obs))
  y <- rnorm(n_obs)
  
  # estimate a linear model for each column in x and find min pvalue
  x %>%
    as_tibble %>%
    mutate(y = y) %>%
    gather(i, x, -y) %>%
    group_by(i) %>%
    nest() %>%
    mutate(mod = map(data, ~ lm(y ~ x, data = .x)),
           results = map(mod, tidy)) %>%
    unnest(results) %>%
    filter(term == "x") %>%
    filter(p.value == min(p.value))
}

pvals_mult <- 1000 %>%
  rerun(pval_dist_mult(n_obs)) %>%
  bind_rows %>%
  as_tibble %>%
  mutate(sig = p.value < .05)

ggplot(pvals_mult, aes(p.value, fill = sig)) +
  geom_histogram(binwidth = .025, boundary = 0) +
  labs(title = "Distribution of minimmum p-values for 10 tests when null is true",
       x = expression(P),
       y = NULL) +
  theme(legend.position = "none")
```

Now we have a 40% chance of finding a predictor with $p < .05$, and the distribution of the minimum p-values is not uniform. When we search for the most significant result, we do not have a fixed null hypothesis. Instead, we are conducting 10 null hypothesis tests using the same sample of data. The distribution of the minimum of 10 random uniform distributions has a density $k(1 - x)^{k-1}$ for $k$ independent tests. When $k=10$, the probability of observing $p < .05$ is $1 - (1 - 0.05)^{10} = 0.40$.

By failing to correct for the fact that we conducted multiple hypothesis tests on the same sample of data, we risk **false discovery** and is a form of selection bias. Even if the tests were not actually performed, we still risk selection bias when any choice of results is based on the outcome, rather than the prespecified hypotheses.

**This happens all the time in social science.** All scholars and researchers do this. We form a theory, generate hypotheses, collect data to test the hypotheses, and explore multiple model formulations until we settle on the final form. This doesn't mean we are attempting to commit fraud, we are just using our knowledge to try and estimate a "good" model.

## Confidence intervals don't save us

A common suggestion for supplementing p-values is to report the confidence interval of the effect. The problem is that the confidence interval, like the p-value, is directly related to the size of the standard error and the resulting t-statistic. Consider the confidence intervals for the first set of simulation results, where we conduct just a single hypothesis test on each sample of data:

```{r ci-single-test}
ggplot(pvals, aes(p.value, estimate, color = sig)) +
  geom_hline(yintercept = 0) +
  geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                      ymax = estimate + 1.96 * std.error), alpha = .25) +
  labs(title = "95% CIs when null is true",
       x = expression(P),
       y = "Estimated effect size") +
  theme(legend.position = "none")
```

Again, looks correct. However, what about when we conduct 10 tests instead of just one?

```{r ci-mult-test}
ggplot(pvals_mult, aes(p.value, estimate, color = sig)) +
  geom_hline(yintercept = 0) +
  geom_pointrange(aes(ymin = estimate - 1.96 * std.error,
                      ymax = estimate + 1.96 * std.error), alpha = .25) +
  labs(title = "Most significant 95% CIs of 10 tests when null is true",
       x = expression(P),
       y = "Estimated effect size") +
  theme(legend.position = "none")
```

Selection bias of this type is not addressed or corrected merely by reporting both the p-value and the confidence interval.

## Selection of predictors

> For more detail on subset selection methods, see chapter 6.1 in ISLR.

Another common analysis in which p-values can be misinterpreted is the selection of a prediction mdoel for multiple regression or classification. **Subset selection** procedures automate the process of deciding which predictors to include or exclude in a regression model. Under the **best subset selection**:

1. Let $M_0$ denote the null model which contains no predictors. This model simply predicts the sample mean for each observation.
1. For $k = 1, 2, \dots, p$:
    1. Fit all ${p}\choose{k}$ models that contain exactly $k$ predictors
    1. Pick the best among these ${p}\choose{k}$ models and call it $M_k$. Best is defined by the smallest RSS or RMSE, or largest $R^2$
1. Select a single best model from among $M_0, \dots, M_p$ using cross-validated prediction error or similar metrics

Of course this approach can be computationally infeasible as the number of predictors $p$ increases. An alternative (and popular) choice is **stepwise selection**. In **forward stepwise regression**:

1. Let $M_0$ denote the null model which contains no predictors
1. For $k = 1, 2, \dots, p - 1$:
    1. Fit all $p-k$ models that augment the predictors in $M_k$ with one additional predictor
    1. Pick the best among these $p - k$ models and call it $M_{k+1}$. Best is defined by the smallest RSS or RMSE, or largest $R^2$
1. Select a single best model from among $M_0, \dots, M_p$ using cross-validated prediction error or similar metrics

This method avoids estimating all $2^p$ models. Instead, you only have to estimate $1 + \frac{p(p+1)}{2}$. When $p=20$, this is the difference between estimating $p^{20} = 1,048,576$ models versus $1 + \frac{10(10+1)}{2} = 211$ models.

In **backward stepwise selection**, you start with the full model and remove predictors from the model during each iteration.

1. Let $M_0$ denote the full model which contains all $p$ predictors
1. For $k = p, p-1, \dots, 1$:
    1. Fit all $k$ models that contain all but one of the predictors in $M_k$, for a total of $k-1$ predictors
    1. Pick the best among these $k$ models and call it $M_{k-1}$. Best is defined by the smallest RSS or RMSE, or largest $R^2$
1. Select a single best model from among $M_0, \dots, M_p$ using cross-validated prediction error or similar metrics

While these approaches are based on minimizing overall model error, they are still problematic for interpreting the resulting p-values.^[This is probably why they are advertised for building [**prediction** models, not inferential models](persp001_stat_learn.html#why_estimate_(f)).] Consider if we applied forward stepwise selection to our simulated data from earlier. First let's use as our baseline a multiple regression model containing all 10 predictors from $\mathbf{X}$. How often do we reject the null hypothesis of no association between $X_k$ and $Y$?

```{r sim-step-single}
full_mod_sim <- function(n_obs){
  x <- replicate(10, rnorm(n_obs))
  y <- rnorm(n_obs)
  
  mod <- lm(y ~ x)
  
  return(tidy(mod))
}

pvals_full <- 1000 %>%
  rerun(full_mod_sim(n_obs)) %>%
  bind_rows %>%
  as_tibble %>%
  filter(term != "(Intercept)") %>%
  mutate(sig = p.value < .05)

ggplot(pvals_full, aes(p.value, fill = sig)) +
  geom_histogram(binwidth = .025, boundary = 0) +
  labs(title = "Distribution of p-values from full model when null is true",
       subtitle = "All covariates",
       x = expression(P),
       y = NULL) +
  theme(legend.position = "none")
```

As expected, only approximately 5% of the time do we make a false discovery. What happens if we use forward stepwise selection to choose the "best" model? How often will we reject the null hypothesis for each of the variables in the model?

```{r sim-step-mult, eval = FALSE}
step_mod_sim <- function(n_obs){
  x <- replicate(10, rnorm(n_obs)) %>%
    as_tibble()
  y <- rnorm(n_obs)
  
  sim_data <- x %>%
    mutate(y = y)
  
  # estimate full model
  mod <- lm(y ~ ., data = sim_data)
  
  # pick model with lowest aic based on forward stepwise selection
  invisible(MASS::stepAIC(mod))
}

pvals_step <- 1000 %>%
  rerun(step_mod_sim(n_obs))
```

```{r sim-step-mult-real, include = FALSE}
step_mod_sim <- function(n_obs){
  x <- replicate(10, rnorm(n_obs)) %>%
    as_tibble()
  y <- rnorm(n_obs)
  
  sim_data <- x %>%
    mutate(y = y)
  
  # estimate full model
  mod <- lm(y ~ ., data = sim_data)
  
  # pick model with lowest aic based on forward stepwise selection
  invisible(MASS::stepAIC(mod))
}

pvals_step <- 1000 %>%
  rerun(step_mod_sim(n_obs))
```

```{r sim-step-mult-plot}
# tidy
pvals_step_tidy <- pvals_step %>%
  map_df(tidy, .id = "sim") %>%
  as_tibble

# glance
pvals_step_glance <- pvals_step %>%
  map_df(glance, .id = "sim") %>%
  as_tibble

# plot of k
n_k <- pvals_step_tidy %>%
  count(sim) %>%
  count(n) %>%
  # remove intercept
  mutate(n = n - 1)

ggplot(n_k, aes(n, nn)) +
  geom_col() +
  labs(title = "Number of times k variables were selected",
       x = expression(k),
       y = NULL)

# distribution of model fit metrics
pvals_step_glance %>%
  select(r.squared, p.value) %>%
  gather(stat, val) %>%
  ggplot(aes(fct_rev(stat), val)) +
  geom_hline(yintercept = 0.05) +
  geom_boxplot() +
  scale_x_discrete(labels = c(expression(R^2), expression(p))) +
  labs(title = expression(paste("Distribution of ", R^2, " and p-values after variable selection")),
       subtitle = "p-values from F-test",
       x = NULL,
       y = NULL)
```

Using stepwise regression, we correctly identify 0 variables as predictive in just `r n_k[1, 2]` of 1000 simulations. We reject the null hypothesis for at least one predictor in `r (1 - n_k[1, 2] / 1000) * 100`% of the simulations. Our results have a very high false discovery rate, even with just 100 observations and only 10 predictors. Stepwise regression can boost the false discovery rate dramatically.

## Finding p-values without trying

![](https://espnfivethirtyeight.files.wordpress.com/2015/08/truth-vigilantes-soccer-calls2.png?quality=90&strip=info&w=1024&ssl=1)

Researcher decisions on how to operationalize concepts into variables, select statistical learning methods, specify functional form, transform or specify variables, etc., all influence the p-value and resulting conclusions. The major point is that p-hacking and false discovery can occur even with the best of intentions.

# Article critique

![](https://imgs.xkcd.com/comics/significant.png)

* [Beall, A. T., & Tracy, J. L. (2013). Women are more likely to wear red or pink at peak fertility. *Psychological Science*, 0956797613476045.](http://journals.sagepub.com.proxy.uchicago.edu/doi/abs/10.1177/0956797613476045)
* ["Too Good to Be True" by Andrew Gelman. *Slate*.](http://www.slate.com/articles/health_and_science/science/2013/07/statistics_and_psychology_multiple_comparisons_give_spurious_results.html)
* ["Response by Jessica Tracy and Alec Beall to my critique of the methods in their paper, 'Women Are More Likely to Wear Red or Pink at Peak Fertility'" by Andrew Gelman.](http://andrewgelman.com/2013/07/31/response-by-jessica-tracy-and-alec-beall-to-my-criticism-of-their-paper/)

## Researcher degrees of freedom

**Researcher degrees of freedom** - decision points in the research process of collecting and analyzing data that have an influence on the results obtained from analysis. When these decisions are made during the analysis process, they have the potential to corrupt the results and identify statistically significant effects by cherry-picking and p-hacking the data.

### Activity: Identify researcher degrees of freedom in Beall and Tracy (2013) article

* Color of shirt - how to group pink and red (or whether to)
    * Why should red matter? Why not white or gray?
* What garment of clothing to ask about
* How to define high- vs. low-conception risk group
* ???

## Other sources of critique

* Representativeness - should these results generalize outside of UBC students and female American MTurkers?
* Measurement error - self-reporting is not as accurate as hormone assessment
* Effect size - does the 3x effect change with a larger or more representative sample?

## Critique of critique

* Did the authors engage in p-hacking or false discovery? How can we know?
* What steps could the author take to minimize false discovery? Did they do any of this?
* Should a journal publish this article? Is this "good" science?

# Preventing false discovery

## Cross-validation

[**Cross-validation**](persp006_resampling.html) techniques are a useful tool to combat false discovery. You attempt to build an explanatory model using the training set of data, which you can repeatedly use to test different models. You then test the model against the test set of data to evaluate its robustness on out-of-sample data. The one key addition is that you save a third portion of **validation set** data that you only use once. After you build the final model, you test it against the withheld validation set. But you can only use it once. If you use it again, you are conducting multiple hypothesis tests. This is how Kaggle operates for its competitions. They give you a portion of the dataset on which to build a machine learning model. You can split that into a training and test set and build to your heart's content. But it doesn't matter what your test error rate is; instead, you submit the code to generate your final model and that is applied to a validation set to which you do not have access.

## Corrected p-values

One approach is to correct the p-value and account for the fact that you are conducting multiple hypotheses test and should therefore expect to have some amount of false discovery. The **family-wise error rate** (FWER) is the probability of making one or more false discoveries (false positives) when performing multiple hypotheses tests. By assuring $\text{FWER} \leq \alpha$, the probability of making one or more false positives in the family is controlled at level $\alpha$. The **Bonferroni correction** accomplishes this by rejecting the null hypothesis $H_i$ for each $p_i$ if and only if $p_i \leq \frac{\alpha}{m}$ where $m$ is the number of hypotheses being tested. For example, if a study tested $m=20$ hypotheses with a desired $\alpha = 0.05$ (the standard threshold), then the Bonferroni correction would test each individual hypothesis at $\alpha = \frac{0.05}{20} = 0.0025$.

For example, draw 100 observations from a normal distribution $N(0,1)$ and test the null hypothesis $H_0: \mu = 0$ using a t-test.^[Example drawn from [this StackOverflow question](https://stats.stackexchange.com/questions/135279/simulating-a-multiple-comparisons-problem-using-r-and-bonferroni-correction).] If we simulate this process multiple times, we should reject $H_0$ approximately 5% of the time.

```{r sim-norm-null}
sim_norm_null <- 1000 %>%
  rerun(rnorm(n_obs)) %>%
  map(~ t.test(x = .x, mu = 0)) %>%
  map_dbl(~ .x$p.value) %>%
  as_tibble %>%
  mutate(sig = value < .05)

mean(sim_norm_null$value)

ggplot(sim_norm_null, aes(value, fill = sig)) +
  geom_histogram(binwidth = .025, boundary = 0) +
  labs(title = "Distribution of p-values for single test",
       x = expression(P),
       y = NULL) +
  theme(legend.position = "none")
```

Now let's simulate 5 random variables and test the null hypothesis that all means are simultaneously 0, then the probability of at least one significant result is $1 - (1 - 0.05)^5 = 0.226$:

```{r sim-norm-mult}
sim_norm_mult <- 1000 %>%
  rerun(5 %>%
          rerun(rnorm(n_obs)) %>%
          map(~ t.test(x = .x, mu = 0)) %>%
          map_dbl(~ .x$p.value) %>%
          as_tibble %>%
          mutate(sig = value < .05)) %>%
  bind_rows(.id = "sim") %>%
  group_by(sim) %>%
  rename(raw = value) %>%
  mutate(correct = raw < (.05 / n()))

sim_norm_mult %>%
  summarize(sig = any(raw < .05)) %>%
  ungroup %>%
  summarize(mean(sig))
  
sim_norm_mult %>%
  filter(raw == min(raw)) %>%
  ggplot(aes(raw, fill = sig)) +
  geom_histogram(binwidth = .01, boundary = 0) +
  labs(title = "Distribution of p-values for multiple tests",
       x = expression(P),
       y = NULL) +
  theme(legend.position = "none")
```

But if we use the Bonferroni correction:

```{r bonferroni}
sim_norm_mult %>%
  summarize(sig = any(correct)) %>%
  ungroup %>%
  summarize(mean(sig))

sim_norm_mult %>%
  filter(raw == min(raw)) %>%
  ggplot(aes(raw, fill = correct)) +
  geom_histogram(binwidth = .01, boundary = 0) +
  labs(title = "Distribution of p-values for multiple tests",
       subtitle = "With Bonferroni correction",
       x = expression(P),
       y = NULL) +
  theme(legend.position = "none")
```

One can also use the Bonferroni correction to adjust confidence intervals. If one establishes $m$ confidence intervals and wants an overall confidence level of $1 - \alpha$, then the confidence interval should be adjusted to the level of $1 - \frac{\alpha}{m}$.

The Bonferroni correction is a conservative adjustment, and errs on the side of caution. You minimize the risk of a false positive, but therefore increase the risk of a false negative. Alternative correction methods such as the Benjamini–Hochberg procedure are less conservative. See `?p.adjust` for more information on implementing these correction procedures in R.

## Pre-registration of research design

A growing movement within academia (especially within the open-source realm) proposes **pre-registration** of research studies. With pre-registration, researchers submit their research rationale, hypotheses, design, and analytic strategy to a journal for peer review **before beginning the study**. The submission undergoes peer review at this point and can be rejected or R&Red just as in a typical peer review process. If the article is accepted at this stage, then the journal conditionally commits to publishing the article once the study is completed.

This procedure frees researchers from the burden of feeling any pressure to p-hack their results to obtain statistically significant (aka "publishable") results. As long as the methodology is sound, even if the study generates null findings then the journal still publishes the article.

### Benefits of pre-registration

* Improved use of theory and stronger research methods. In fact, this probably makes the research more difficult because researchers have to consider all the potential research design elements before conducting the study. They cannot make corrections or adjustments along the way.
* Decline in false-positive publications. This should also minimize the "replication crisis" throughout the social sciences (most prominently in psychology) by publishing results that, if the p-values lead to correct inference, then the results should be reproducible in independent studies.
* Null findings don't stay hidden in the shadows. If multiple scholars asking the same question get null results, maybe it isn't a question worth asking. Yet if no one publishes on it, future scholars don't realize it's a dead end.
* Forces the hand of funding agencies. Minimizes the potential for interference with the results because the study has to be declared beforehand.

### Concerns of pre-registration

* Minimizes exploratory research. If exploratory research is deemed to be p-hacking, then it becomes less publishable under this new regime.
* Journals may skew towards accepting studies only from researchers with established prestige, since they want to minimize publishing mundane or null results. This hurts graduate students and early career scholars who don't yet have name recognition.
* What happens if when conducting the study, the scholar clearly sees an element that is faulty or needs to be adjusted? If they pre-register the design, they open themselves up to accusations of p-hacking if they change the protocol (and the journal is no longer obligated to publish the article).

# Acknowledgments {.toc-ignore}

* p-value distribution simulation from [Altman, Naomi, and Martin Krzywinski. "Points of significance: P values and the search for significance." *Nature Methods* 14.1 (2017): 3-4.](http://www.nature.com/nmeth/journal/v14/n1/full/nmeth.4120.html)

# Session Info {.toc-ignore}

```{r child='_sessioninfo.Rmd'}
```




