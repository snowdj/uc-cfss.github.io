---
title: "Split-apply-combine and parallel computing"
date: 2019-03-01

type: docs
toc: true
draft: false
aliases: ["/distrib002_sac.html"]
categories: ["distributed-computing"]

menu:
  notes:
    parent: Distributed computing
    weight: 2
---

```{r setup2, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(gapminder)
library(stringr)
set.seed(1234)

theme_set(theme_minimal())
```

A common analytical pattern is to

* **Split** data into pieces,
* **Apply** some function to each piece,
* **Combine** the results back together again.

We have used this technique many times so far without explicitly identifying it as such.

## `dplyr::group_by()` {#group-by}

```{r group_by}
gapminder %>%
  group_by(continent) %>%
  summarize(n = n())

gapminder %>%
  group_by(continent) %>%
  summarize(avg_lifeExp = mean(lifeExp))
```

## `for` loops {#for-loops}

```{r for_loop}
countries <- unique(gapminder$country)
lifeExp_models <- vector("list", length(countries))
names(lifeExp_models) <- countries

for(i in seq_along(countries)){
  lifeExp_models[[i]] <- lm(lifeExp ~ year,
                            data = filter(gapminder,
                                          country == countries[[i]]))
}
head(lifeExp_models)
```

## `nest()` and `map()` {#nest-map}

```{r nest}
# function to estimate linear model for gapminder subsets
le_vs_yr <- function(df) {
  lm(lifeExp ~ year, data = df)
}

# split data into nests
(gap_nested <- gapminder %>% 
   group_by(continent, country) %>% 
   nest())

# apply a linear model to each nested data frame
(gap_nested <- gap_nested %>% 
   mutate(fit = map(data, le_vs_yr)))

# combine the results back into a single data frame
library(broom)
(gap_nested <- gap_nested %>% 
  mutate(tidy = map(fit, tidy)))

(gap_coefs <- gap_nested %>% 
   select(continent, country, tidy) %>% 
   unnest(tidy))
```

## Parallel computing

![From [An introduction to parallel programming using Python's multiprocessing module](http://sebastianraschka.com/Articles/2014_multiprocessing.html)](https://sebastianraschka.com/images/blog/2014/multiprocessing_intro/multiprocessing_scheme.png)

**Parallel computing** (or processing) is a type of computation whereby many calculations or processes are carried out simultaneously.^[["Parallel computing"](https://en.wikipedia.org/wiki/Parallel_computing)] Rather than processing problems in **serial** (or sequential) order, the computer splits the task up into smaller parts that can be processed simultaneously using multiple processors. This is also called **multithreading**. By spliting the job up into simultaneous operations running in **parallel**, you complete your operation quicker, making the code more efficient. This approach works great with split-apply-combine because all the applied operations can be run independently. Why wait for the first chunk to complete if you can perform the operation on the second chunk at the same time?

### Why use parallel computing

* Parallel computing **imitates real life** - in the real world, people use their brains to think in parallel - we multitask all the time without even thinking about it. Institutions are structured to process information in parallel, rather than in serial.
* It can be **more efficient** - by throwing more resources at a problem you can shorten the time to completion.
* You can **tackle larger problems** - more resources enables you to scale up the amount of data you process and potentially solve a larger problem.
* **Distributed resources** are cheaper than upgrading your own equipment. Why spend thousands of dollars beefing up your own laptop when you can instead rent computing resources from Google or Amazon for mere pennies?

### Why not to use parallel computing

* **Limits to efficiency gains** - [Amdahl's law](https://en.wikipedia.org/wiki/Amdahl's_law) defines theoretical limits to how much you can speed up computations via parallel computing. Because of this, you achieve diminishing returns over time.

    ![Amdahl's Law from [Wikipedia](https://en.wikipedia.org/wiki/Amdahl's_law)](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/AmdahlsLaw.svg/768px-AmdahlsLaw.svg.png)
    
* **Complexity** - writing parallel code can be more complicated than writing serial code, especially in R because it does not natively implement parallel computing - you have to explicitly build it into your script.
* **Dependencies** - your computation may rely on the output from the first set of tasks to perform the second tasks. If you compute the problem in parallel fashion, the individual chunks do not communicate with one another.
* **Parallel slowdown** - parallel computing speeds up computations at a price. Once the problem is broken into separate threads, reading and writing data from the threads to memory or the hard drive takes time. Some tasks are not improved by spliting the process into parallel operations.

## `multidplyr` {#multidplyr}

[`multidplyr`](https://github.com/hadley/multidplyr) is a work-in-progress package that implements parallel computing locally using `dplyr`. Rather than performing computations using a single core or processor, it spreads the computation across multiple cores. The basic sequence of steps is:

1. Call `partition()` to split the dataset across multiple cores. This makes a partitioned data frame, or a `party df` for short.
1. Each `dplyr` verb applied to a `party df` performs the operation independently on each core. It leaves each result on each core, and returns another `party df`.
1. When you're done with the expensive operations that need to be done on each core, you call `collect()` to retrieve the data and bring it back to you local computer.

### `nycflights13::flights` {#flights}

Install `multidplyr` if you don't have it already.

```r
devtools::install_github("hadley/multidplyr")
```

```{r multidplyr}
library(multidplyr)
library(nycflights13)
```

Next, partition the flights data by flight number, compute the average delay per flight, and then collect the results:

```{r flights_multi}
flights1 <- partition(flights, flight)
flights2 <- summarize(flights1, dep_delay = mean(dep_delay, na.rm = TRUE))
flights3 <- collect(flights2)
```

The `dplyr` code looks the same as usual, but behind the scenes things are very different. `flights1` and `flights2` are `party df`s. These look like normal data frames, but have an additional attribute: the number of shards. In this example, it tells us that `flights2` is spread across three nodes, and the size on each node varies from 1275 to 1286 rows. `partition()` always makes sure a group is kept together on one node.

```{r flights2}
flights2
```

### Performance

For this size of data, using a local cluster actually makes performance slower.

```{r flights_performance}
system.time({
  flights %>% 
    partition() %>%
    summarise(mean(dep_delay, na.rm = TRUE)) %>% 
    collect()
})
system.time({
  flights %>% 
    group_by() %>%
    summarise(mean(dep_delay, na.rm = TRUE))
})
```

That's because there's some overhead associated with sending the data to each node and retrieving the results at the end. For basic `dplyr` verbs, `multidplyr` is unlikely to give you significant speed ups unless you have 10s or 100s of millions of data points. It might however, if you're doing more complex things.

### `gapminder` {#gapminder}

Let's now return to `gapminder` and estimate separate linear regression models of life expectancy based on year for each country. We will use `multidplyr` to split the work across multiple cores. Note that we need to use `cluster_library()` to load the `purrr` package on every node.

```{r multi_nest}
# split data into nests
gap_nested <- gapminder %>% 
  group_by(continent, country) %>% 
  nest()

# partition gap_nested across the cores
gap_nested_part <- gap_nested %>%
  partition(country)

# apply a linear model to each nested data frame
cluster_library(gap_nested_part, "purrr")

system.time({
  gap_nested_part %>% 
    mutate(fit = map(data, function(df) lm(lifeExp ~ year, data = df)))
})
```

Compared to how long running it locally?

```{r multi_nest_serial}
system.time({
  gap_nested %>% 
    mutate(fit = map(data, function(df) lm(lifeExp ~ year, data = df)))
})
```

So it's roughly 2 times faster to run in parallel. Admittedly you saved only a fraction of a second. In relative terms this is great, but in absolute terms it doesn't mean much. This demonstrates it doesn't always make sense to parallelize operations - only do so if you can make significant gains in computation speed. If each country had thousands of observations, the efficiency gains would have been more dramatic.

### Acknowledgments

* [Parallel Algorithms Advantages and Disadvantages](http://www.slideshare.net/lucky43/parallel-computing-advantages-and-disadvantages)

### Session Info

```{r child = here::here("R", "_session-info.Rmd")}
```
