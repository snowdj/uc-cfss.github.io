---
title: "Importing data into R"
date: 2019-03-01

type: docs
toc: true
draft: false
aliases: ["/datawrangle_import_functions.html"]
categories: ["datawrangle"]

menu:
  notes:
    parent: Data wrangling
    weight: 6
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

```{r packages, cache = FALSE, message = FALSE}
library(tidyverse)
library(here)
theme_set(theme_minimal())

# set seed for reproducibility
set.seed(1234)
```

## `readr` vs. base R

```{r data-gen, include = FALSE}
n_row_s <- 5000

data_small <- tibble(
  x = sample(seq(from = 1, to = 4), n_row_s, replace = TRUE),
  y = sample(letters, n_row_s, replace = TRUE),
  z = str_c(sample(letters, n_row_s, replace = TRUE),
            sample(seq(from = 1, to = 4), n_row_s, replace = TRUE)),
  value = rnorm(n_row_s)
) %>%
  write_csv(here("static", "data", "sim-data-small.csv"))

n_row_l <- 500000

data_large <- tibble(
  x = sample(seq(from = 1, to = 4), n_row_l, replace = TRUE),
  y = sample(letters, n_row_l, replace = TRUE),
  z = str_c(sample(letters, n_row_l, replace = TRUE),
            sample(seq(from = 1, to = 4), n_row_l, replace = TRUE)),
  value = rnorm(n_row_l)
) %>%
  write_csv(here("static", "data", "sim-data-large.csv"))
```

One of the main advantages of `readr` functions over base R functions is that [they are typically much faster](http://r4ds.had.co.nz/data-import.html#compared-to-base-r). For example, let's import a randomly generated CSV file with `r prettyNum(n_row_s, scientific = FALSE, big.mark = ",")` rows and `r ncol(data_small)` columns. How long does it take `read.csv()` to import this file vs. `readr::read_csv()`? To assess the differences, we use the [`microbenchmark`](https://cran.r-project.org/web/packages/microbenchmark/) to run each function 100 times and compare the distributions of the time it takes to complete the data import:

```{r compare-speed-small, dependson = "data-gen", message = FALSE}
library(microbenchmark)

results_small <- microbenchmark(
  read.csv = read.csv(file = here("static", "data", "sim-data-small.csv")),
  read_csv = read_csv(file = here("static", "data", "sim-data-small.csv"))
)
```

```{r compare-speed-small-plot, dependson = "compare-speed-small", message = FALSE}
autoplot(object = results_small) +
  scale_y_log10() +
  labs(y = "Time [milliseconds], logged")
```

`read_csv()` is over 5 times faster than `read.csv()`. Of course with relatively small data files, this isn't a large difference in absolute terms (a difference of 100 milliseconds is only .1 second). However, as the data file increases in size the performance savings will be much larger. Consider the same test with a CSV file with `r prettyNum(n_row_l, scientific = FALSE, big.mark = ",")` rows:

```{r compare-speed-large, dependson = "data-gen", message = FALSE}
library(microbenchmark)

results_large <- microbenchmark(
  read.csv = read.csv(file = here("static", "data", "sim-data-large.csv")),
  read_csv = read_csv(file = here("static", "data", "sim-data-large.csv"))
)
```

```{r compare-speed-large-plot, dependson = "compare-speed-large", message = FALSE}
autoplot(object = results_large) +
  scale_y_log10() +
  labs(y = "Time [milliseconds], logged")
```

Here `read_csv()` is far superior to `read.csv()`.

## `vroom`

[`vroom`](https://vroom.r-lib.org/) is a recently developed package designed specifically for **speed**. It contains one of the fastest functions to import plain-text data files. Its syntax is similar to `readr::read_*()` functions, but works much more quickly.

```{r vroom-compare-speed-large, dependson = "data-gen", message = FALSE}
results_vroom <- microbenchmark(
  read.csv = read.csv(file = here("static", "data", "sim-data-large.csv")),
  read_csv = read_csv(file = here("static", "data", "sim-data-large.csv")),
  vroom = vroom::vroom(file = here("static", "data", "sim-data-large.csv"))
)
```

```{r vroom-compare-speed-large-plot, dependson = "compare-speed-large", message = FALSE}
autoplot(object = results_vroom) +
  scale_y_log10() +
  labs(y = "Time [milliseconds], logged")
```

## Alternative file formats

CSV files, while common, are not the only type of data storage format you will encounter in the wild. Here is a quick primer on other file formats you may encounter and how to import/export them using R. We'll use the `challenge` dataset in `readr` to demonstrate some of these formats.

```{r challenge}
challenge <- read_csv(
  readr_example(path = "challenge.csv"), 
  col_types = cols(
    x = col_double(),
    y = col_date()
  )
)

challenge
```

## RDS

**RDS** is a custom binary format used exclusively by R to store data objects.

```{r rds, dependson = "challenge", message = FALSE}
# write to csv
write_csv(x = challenge, path = here("static", "data", "challenge.csv"))

# write to/read from rds
write_rds(x = challenge, path = here("static", "data", "challenge.csv"))
read_rds(path = here("static", "data", "challenge.csv"))

# compare file size
file.info(here("static", "data", "challenge.rds"))$size %>%
  utils:::format.object_size("auto")

file.info(here("static", "data", "challenge.csv"))$size %>%
  utils:::format.object_size("auto")

# compare read speeds
microbenchmark(
  read_csv = read_csv(
    file = readr_example("challenge.csv"), 
    col_types = cols(
      x = col_double(),
      y = col_date()
    )
  ),
  read_rds = read_rds(path = here("static", "data", "challenge.rds"))
) %>%
  autoplot +
  labs(y = "Time [microseconds], logged")
```

By default, `write_rds()` does not compress the `.rds` file; use the `compress` argument to implement one of several different compression algorithms. `read_rds()` is noticably faster than `read_csv()`, and also has the benefit of [preserving column types](http://r4ds.had.co.nz/data-import.html#writing-to-a-file). The downside is that RDS is only implemented by R; it is not used by any other program so if you need to import/export data files into other languages like Python (or open in Excel), RDS is not a good storage format.

## `arrow`

The `arrow` package implements a binary file format called `featehr` that is cross-compatible with many different programming languages:

```{r feather, dependson = "challenge", message = FALSE}
library(arrow)

write_feather(x = challenge, sink = here("static", "data", "challenge.feather"))
read_feather(file = here("static", "data", "challenge.feather"))

# compare read speeds
microbenchmark(
  read_csv = read_csv(
    file = readr_example("challenge.csv"), 
    col_types = cols(
      x = col_double(),
      y = col_date()
    )
  ),
  read_rds = read_rds(path = here("static", "data", "challenge.rds")),
  read_feather = read_feather(file = here("static", "data", "challenge.feather"))
) %>%
  autoplot +
  scale_y_continuous(labels = scales::comma) +
  labs(y = "Time [microseconds], logged")
```

`read_feather()` is generally faster than RDS and `read_csv()`.^[Notice that the x-axis is logarithmically scaled.] Furthermore, [it has native support for Python, R, and Julia.](http://arrow.apache.org/blog/2019/08/08/r-package-on-cran/), so if you develop an analytics pipeline that switches between R and Python, you can import/export data files in `.feather` without any loss of information.

## `readxl`

[`readxl`](http://readxl.tidyverse.org/) enables you to read (but not write) Excel files (`.xls` and `xlsx`).^[If you need to export data into Excel, use `readr::write_csv_excel()`.]

```{r readxl}
library(readxl)

xlsx_example <- readxl_example(path = "datasets.xlsx")
read_excel(xlsx_example)
```

The nice thing about `readxl` is that you can access multiple sheets from the workbook. List the sheet names with `excel_sheets()`:

```{r readxl-sheets, dependson = "readxl"}
excel_sheets(path = xlsx_example)
```

Then specify which worksheet you want by name or number:

```{r readxl-select-sheet, dependson = "readxl"}
read_excel(path = xlsx_example, sheet = "chickwts")
read_excel(path = xlsx_example, sheet = 2)
```

## `haven`

[`haven`](http://haven.tidyverse.org/) allows you to read and write data from other statistical packages such as SAS (`.sas7bdat` + `.sas7bcat`), SPSS (`.sav` + `.por`), and Stata (`.dta`).

```{r haven}
library(haven)

# SAS
read_sas(data_file = system.file("examples", "iris.sas7bdat", package = "haven"))
write_sas(data = mtcars, path = here("static", "data", "mtcars.sas7bdat"))

# SPSS
read_sav(file = system.file("examples", "iris.sav", package = "haven"))
write_sav(data = mtcars, path = here("static", "data", "mtcars.sav"))

# Stata
read_dta(file = system.file("examples", "iris.dta", package = "haven"))
write_dta(data = mtcars, path = here("static", "data", "mtcars.dta"))
```

That said, if you can obtain your data file in a plain `.csv` or `.tsv` file format, **I strongly recommend it**. SAS, SPSS, and Stata files represent labeled data and missing values differently from R. `haven` attempts to bridge the gap and preserve as much information as possible, but I frequently find myself stripping out all the label information and rebuilding it using `dplyr` functions and the codebook for the data file.

> Need to import a SAS, SPSS, or Stata data file? Read [the documentation](http://haven.tidyverse.org/articles/semantics.html) to learn how to best handle value labels and missing values.

## Session Info

```{r child = here::here("R", "_session-info.Rmd")}
```
