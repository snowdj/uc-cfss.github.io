---
title: "Text analysis: fundamentals and sentiment analysis"
author: "[MACS 30500](https://cfss.uchicago.edu) <br /> University of Chicago"
output: rcfss::xaringan
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  cache = TRUE,
  message = FALSE,
  warning = FALSE,
  echo = TRUE,
  fig.retina = 2, fig.width = 12
)

library(tidyverse)
library(tidytext)
library(scales)
library(here)
library(patchwork)
library(magrittr)

set.seed(1234)
theme_set(theme_minimal(base_size = rcfss::base_size))
```

# Basic workflow for text analysis

* Obtain your text sources
* Extract documents and move into a corpus
* Transformation
* Extract features
* Perform analysis

---

# Obtain your text sources

* Web sites
    * Twitter
* Databases
* PDF documents
* Digital scans of printed materials

---

# Extract documents and move into a corpus

* Text corpus
* Typically stores the text as a raw character string with metadata and details stored with the text

---

# Transformation

* Tag segments of speech for part-of-speech (nouns, verbs, adjectives, etc.) or entity recognition (person, place, company, etc.)
* Standard text processing
    * Convert to lower case
    * Remove punctuation
    * Remove numbers
    * Remove stopwords
    * Remove domain-specific stopwords
    * Stemming

---

# Extract features

* Convert the text string into some sort of quantifiable measures
* Bag-of-words model
    * Term frequency vector
    * Term-document matrix
    * Ignores context
* Word embeddings

---

# Word embeddings

.center[

![](https://blogs.mathworks.com/images/loren/2017/vecs.png)

]

---

# Perform analysis

* Basic
    * Word frequency
    * Collocation
    * Dictionary tagging
* Advanced
    * Document classification
    * Corpora comparison
    * Topic modeling

---

# [`tidytext`](https://github.com/juliasilge/tidytext)

* Tidy text format
* Defined as one-term-per-row
* Differs from the term-document matrix
    * One-document-per-row and one-term-per-column

---

# Get text corpa

```{r janeausten-corpa, echo = FALSE}
library(janeaustenr)

(books <- austen_books() %>%
    group_by(book) %>%
    mutate(linenumber = row_number(),
           chapter = cumsum(str_detect(text,
                                       regex("^chapter [\\divxlc]",
                                             ignore_case = TRUE)))) %>%
    ungroup())
```

---

# Tokenize text

```{r janeausten-token, dependson = "janeausten-corpa"}
(tidy_books <- books %>%
   unnest_tokens(output = word, input = text))
```

---

# Practice using `tidytext`

> How often is each U.S. state mentioned in a popular song?

* Billboard Year-End Hot 100 (1958-present)
* Census Bureau ACS

---

# Download population data for U.S. states

```{r acs-pop}
# retrieve state populations in 2016 from Census Bureau
pop_df <- here("static", "data", "pop2016.csv") %>%
  read_csv()

# do these results make sense?
pop_df %>% 
  arrange(desc(population)) %>%
  top_n(10)
```

---

# Retrieve song lyrics

```{r lyrics-import}
song_lyrics <- here("static", "data", "billboard_lyrics_1964-2015.csv") %>%
  read_csv()
glimpse(song_lyrics)
```

---

# Retrieve song lyrics

* [Reference](https://youtu.be/OPf0YbXqDm0?t=91)

```{r lyrics-example, dependson = "lyrics-import", echo = FALSE}
song_lyrics %>%
  filter(Song == "uptown funk") %$%
  Lyrics %>%
  str_wrap() %>%
  cat()
```

---

> Use `tidytext` to create a data frame with one row for each token in each song

--

```{r lyrics-tidy}
(tidy_lyrics <- bind_rows(song_lyrics %>% 
                            unnest_tokens(output = state_name,
                                          input = Lyrics),
                          song_lyrics %>% 
                            unnest_tokens(output = state_name,
                                          input = Lyrics, 
                                          token = "ngrams", n = 2)))
```

---

> Find all the state names occurring in the song lyrics

--

```{r lyrics-state-distinct}
(tidy_lyrics <- inner_join(tidy_lyrics, pop_df) %>%
   distinct(Rank, Song, Artist, Year, state_name, .keep_all = TRUE))
```

---

> Calculate the frequency for each state's mention in a song and create a new column for the frequency adjusted by the state's population

--

```{r state-counts}
(state_counts <- tidy_lyrics %>% 
   count(state_name) %>% 
   arrange(desc(n)))
```

```{r state-counts-poprel}
pop_df <- pop_df %>% 
  left_join(state_counts) %>% 
  mutate(rate = n / population * 1e6)

pop_df %>%
  arrange(desc(rate)) %>%
  top_n(10)
```

---

> Make a choropleth map for both the raw frequency counts and relative frequency counts

```{r state-map, warning = FALSE, echo = FALSE, fig.height = 6}
library(statebins)

{
  pop_df %>%
    mutate(
      state_name = stringr::str_to_title(state_name),
      state_name = if_else(state_name == "District Of Columbia",
        "District of Columbia", state_name
      )
    ) %>%
    statebins_continuous(state_col = "state_name", value_col = "n") +
    labs(
      title = "Frequency of states mentioned in song lyrics",
      subtitle = "Number of mentions"
    ) +
    theme(legend.position = "bottom")
} + {
  pop_df %>%
    mutate(
      state_name = stringr::str_to_title(state_name),
      state_name = if_else(state_name == "District Of Columbia",
        "District of Columbia", state_name
      )
    ) %>%
    statebins_continuous(state_col = "state_name", value_col = "rate") +
    labs(
      title = "Frequency of states mentioned in song lyrics",
      subtitle = "Number of mentions per capita"
    ) +
    theme(legend.position = "bottom")
}
```

---

# Sentiment analysis

> I am happy

---

# Dictionaries

```{r bing}
get_sentiments("bing")
```

---

# Dictionaries

```{r afinn}
get_sentiments("afinn")
```

---

# `janeaustenr`

.pull-left[

##### Sense and Sensibility

<div style="width:100%;height:0;padding-bottom:55%;position:relative;"><iframe src="https://giphy.com/embed/ZHU9QFsyDeTEQ" width="100%" height="100%" style="position:absolute" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></div>

<div style="width:100%;height:0;padding-bottom:50%;position:relative;"><iframe src="https://giphy.com/embed/LQZc0y35MlYYg" width="100%" height="100%" style="position:absolute" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></div>

]

.pull-right[

##### Pride and Prejudice

<div style="width:100%;height:0;padding-bottom:56%;position:relative;"><iframe src="https://giphy.com/embed/26FL4zFEQlJ2ffxXW" width="100%" height="100%" style="position:absolute" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></div>

<div style="width:100%;height:0;padding-bottom:58%;position:relative;"><iframe src="https://giphy.com/embed/l4JyVmADBclbnDieY" width="100%" height="100%" style="position:absolute" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></div>

]

---

# Calculate sentiment

```{r janeausten-sentiment, dependson = c("janeausten-corpa", "janeausten-token"), echo = FALSE}
janeaustensentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

# plot the sentiment over time in each book
ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) +
        geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
        facet_wrap( ~ book, ncol = 2, scales = "free_x")
```

---

# Load Harry Potter text

```{r hp, echo = FALSE}
library(harrypotter)

# names of each book
hp_books <- c("philosophers_stone", "chamber_of_secrets",
              "prisoner_of_azkaban", "goblet_of_fire",
              "order_of_the_phoenix", "half_blood_prince",
              "deathly_hallows")

# combine books into a list
hp_words <- list(
  philosophers_stone,
  chamber_of_secrets,
  prisoner_of_azkaban,
  goblet_of_fire,
  order_of_the_phoenix,
  half_blood_prince,
  deathly_hallows
) %>%
  # name each list element
  set_names(hp_books) %>%
  # convert each book to a data frame and merge into a single data frame
  map_df(as_tibble, .id = "book") %>%
  # convert book to a factor
  mutate(book = factor(book, levels = hp_books)) %>%
  # remove empty chapters
  drop_na(value) %>%
  # create a chapter id column
  group_by(book) %>%
  mutate(chapter = row_number(book)) %>%
  # tokenize the data frame
  unnest_tokens(word, value)

hp_words
```

---

# Most frequent words, by book

```{r word-freq, echo = FALSE, fig.height = 8}
hp_words %>%
  # delete stopwords
  anti_join(stop_words) %>%
  # summarize count per word per book
  count(book, word, sort = TRUE) %>%
  # get top 15 words per book
  group_by(book) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(word = reorder_within(word, n, book)) %>%
  # create barplot
  ggplot(aes(x = word, y = n, fill = book)) + 
  geom_col(color = "black") +
  scale_x_reordered() +
  labs(title = "Most frequent words in Harry Potter",
       x = NULL,
       y = "Word count") +
  facet_wrap(~ book, scales = "free", nrow = 2) +
  coord_flip() +
  theme_minimal(base_size = rcfss::base_size * .65) +
  theme(legend.position = "none")
```

---

# Exercises

.center[

![](https://img.cinemablend.com/filter:scale/quill/b/b/f/b/3/c/bbfb3c1ed393ec47b44de8709af71c8589cfa1db.jpg?mw=600)

]
