---
title: "Text analysis: classification and topic modeling"
author: "[MACS 30500](https://cfss.uchicago.edu) <br /> University of Chicago"
output: rcfss::xaringan
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  cache = TRUE,
  message = FALSE,
  warning = FALSE,
  collapse = TRUE,
  fig.retina = 2, fig.width = 12
)

library(tidyverse)
library(tidymodels)
library(tidytext)
library(rjson)
library(topicmodels)
library(here)
library(patchwork)
library(tictoc)

set.seed(1234)
theme_set(theme_minimal(base_size = rcfss::base_size))
```

# Supervised learning

1. Hand-code a small set of documents $N = 1000$
1. Train a statistical learning model on the hand-coded data
1. Evaluate the effectiveness of the statistical learning model
1. Apply the final model to the remaining set of documents $N = 1000000$

---

# `USCongress`

```{r get-docs, echo = FALSE}
# get USCongress data
data(USCongress, package = "rcfss")

# topic labels
major_topics <- tibble(
  major = c(1:10, 12:21, 99),
  label = c(
    "Macroeconomics", "Civil rights, minority issues, civil liberties",
    "Health", "Agriculture", "Labor and employment", "Education", "Environment",
    "Energy", "Immigration", "Transportation", "Law, crime, family issues",
    "Social welfare", "Community development and housing issues",
    "Banking, finance, and domestic commerce", "Defense",
    "Space, technology, and communications", "Foreign trade",
    "International affairs and foreign aid", "Government operations",
    "Public lands and water management", "Other, miscellaneous"
  )
) %>%
  mutate(label = factor(major, levels = major, labels = label))

congress <- as_tibble(USCongress) %>%
  mutate(text = as.character(text)) %>%
  left_join(major_topics)
glimpse(congress)
```

```{r docs-example, dependson = "get-docs", echo = FALSE}
head(congress$text)
```

---

# Split the data set

```{r split, dependson = "get-docs"}
set.seed(123)

congress <- congress %>%
  mutate(major = factor(x = major, levels = major, labels = label))

congress_split <- initial_split(data = congress, strata = major, prop = .8)
congress_split

congress_train <- training(congress_split)
congress_test <- testing(congress_split)
```

---

# Preprocessing the data frame

```{r recipe, dependson = "split"}
congress_rec <- recipe(major ~ text, data = congress_train)
```

```{r recipe-steps, dependson = "recipe"}
library(textrecipes)

congress_rec <- congress_rec %>%
  step_tokenize(text) %>%
  step_stopwords(text) %>%
  step_tokenfilter(text, max_tokens = 500) %>%
  step_tfidf(text)
```

---

# Train a model

```{r nb-model}
library(discrim)
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")

nb_spec
```

---

# Train a model

```{r nb-fit, dependson = c("recipe-steps", "nb-model")}
nb_wf <- workflow() %>%
  add_recipe(congress_rec) %>%
  add_model(nb_spec)

nb_wf %>%
  fit(data = congress_train)
```

---

# Evaluation

```{r congress-cv, dependson = "split"}
set.seed(123)

congress_folds <- vfold_cv(data = congress_train, strata = major)
```

```{r nb-cv, dependson = c("congress-cv", "nb-fit")}
nb_cv <- nb_wf %>%
  fit_resamples(
    congress_folds,
    control = control_resamples(save_pred = TRUE)
  )
```

```{r nb-cv-info, dependson = "nb-cv"}
nb_cv_metrics <- collect_metrics(nb_cv)
nb_cv_predictions <- collect_predictions(nb_cv)

nb_cv_metrics
```

---

# Receiver operator curve

```{r nb-roc-curve, dependson = "nb-cv-info", echo = FALSE}
nb_cv_predictions %>%
  group_by(id) %>%
  roc_curve(truth = major, c(starts_with(".pred"), -.pred_class)) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "Receiver operator curve for Congressional bills",
    subtitle = "Each resample fold is shown in a different color"
  )
```

---

# Confusion matrix

```{r nb-confusion, dependson = "nb-cv-info", echo = FALSE}
nb_cv_predictions %>%
  filter(id == "Fold01") %>%
  conf_mat(major, .pred_class) %>%
  autoplot(type = "heatmap") +
  scale_y_discrete(labels = function(x) str_wrap(x, 20)) +
  scale_x_discrete(labels = function(x) str_wrap(x, 20))
```

---

# Compare to the null model

```{r null-mod, dependson = c("recipe-steps", "congress-cv")}
null_classification <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")

null_cv <- workflow() %>%
  add_recipe(congress_rec) %>%
  add_model(null_classification) %>%
  fit_resamples(
    congress_folds
  )

null_cv %>%
  collect_metrics()
```

---

# Class imbalance

```{r major-topic-dist, dependson = "get-docs", echo = FALSE}
ggplot(data = congress, mapping = aes(x = fct_infreq(major) %>% fct_rev())) +
  geom_bar() +
  coord_flip() +
  labs(
    title = "Distribution of legislation",
    subtitle = "By major policy topic",
    x = NULL,
    y = "Number of bills"
  )
```

---

# Downsampling

```{r recipe-downsample, dependson = "recipe-steps"}
library(themis)

# build on existing recipe
congress_rec <- congress_rec %>%
  step_downsample(major)
congress_rec
```

---

# Support vector machine

```{r svm-spec}
svm_spec <- svm_rbf() %>%
  set_mode("classification") %>%
  set_engine("liquidSVM")

svm_spec
```

---

# Downsampling

```{r svm-wf, dependson = c("recipe-steps", "svm-spec")}
svm_wf <- workflow() %>%
  add_recipe(congress_rec) %>%
  add_model(svm_spec)
```

```{r svm-fit, dependson = "svm-wf"}
set.seed(123)

svm_cv <- fit_resamples(
  svm_wf,
  congress_folds,
  metrics = metric_set(accuracy),
  control = control_resamples(save_pred = TRUE)
)
```

```{r svm-metrics, dependson = "svm-fit"}
svm_cv_metrics <- collect_metrics(svm_cv)
svm_cv_predictions <- collect_predictions(svm_cv)

svm_cv_metrics
```

---

# Confusion matrix

```{r svm-confusion, dependson = "svm-metrics", echo = FALSE}
svm_cv_predictions %>%
  filter(id == "Fold01") %>%
  conf_mat(major, .pred_class) %>%
  autoplot(type = "heatmap") +
  scale_y_discrete(labels = function(x) str_wrap(x, 20)) +
  scale_x_discrete(labels = function(x) str_wrap(x, 20))
```


---

# Topic modeling

* Themes
* Probabilistic topic models
* Latent Dirichlet allocation

---

# Food and animals

1. I ate a banana and spinach smoothie for breakfast.
1. I like to eat broccoli and bananas.
1. Chinchillas and kittens are cute.
1. My sister adopted a kitten yesterday.
1. Look at this cute hamster munching on a piece of broccoli.

---

# LDA document structure

* Decide on the number of words N the document will have
    * [Dirichlet probability distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution)
    * Fixed set of $k$ topics
* Generate each word in the document:
    * Pick a topic
    * Generate the word
* LDA backtracks from this assumption

---

# `r/jokes`

<blockquote class="reddit-card" data-card-created="1552319072"><a href="https://www.reddit.com/r/Jokes/comments/a593r0/twenty_years_from_now_kids_are_gonna_think_baby/">Twenty years from now, kids are gonna think "Baby it's cold outside" is really weird, and we're gonna have to explain that it has to be understood as a product of its time.</a> from <a href="http://www.reddit.com/r/Jokes">r/Jokes</a></blockquote>
<script async src="//embed.redditmedia.com/widgets/platform.js" charset="UTF-8"></script>

---

# `r/jokes` dataset

```{r jokes, echo = FALSE}
# obtain r/jokes and extract values from the JSON file
jokes_json <- fromJSON(file = "https://github.com/taivop/joke-dataset/raw/master/reddit_jokes.json")

jokes <- tibble(jokes = jokes_json) %>%
  unnest_wider(col = jokes)
glimpse(jokes)
```

---

# Create the recipe

```{r jokes-recipe, dependson = "jokes"}
set.seed(123) # set seed for random sampling

jokes_rec <- recipe(~., data = jokes) %>%
  step_sample(size = 1e04) %>%
  step_tokenize(title, body) %>%
  step_tokenmerge(title, body, prefix = "joke") %>%
  step_stopwords(joke) %>%
  step_ngram(joke, num_tokens = 5, min_num_tokens = 1) %>%
  step_tokenfilter(joke, max_tokens = 2500) %>%
  step_tf(joke)
```

---

# Bake the recipe

```{r jokes-bake, dependson = "jokes-recipe"}
jokes_prep <- prep(jokes_rec)

jokes_df <- bake(jokes_prep, new_data = NULL)
jokes_df %>%
  slice(1:5)
```

---

# Convert to document-term matrix

```{r jokes-dtm, dependson = "jokes-bake"}
jokes_dtm <- jokes_df %>%
  pivot_longer(cols = -c(id, score),
               names_to = "token",
               values_to = "n") %>%
  filter(n != 0) %>%
  # clean the token column so it just includes the token
  # drop empty levels from id - this includes jokes which did not
  # have any tokens retained after step_tokenfilter()
  mutate(token = str_remove(string = token, pattern = "tf_joke_"),
         id = fct_drop(f = id)) %>%
  cast_dtm(document = id, term = token, value = n)
jokes_dtm
```

---

# $k=4$

```{r jokes-topic-4, dependson = "jokes-dtm"}
jokes_lda4 <- LDA(jokes_dtm, k = 4, control = list(seed = 123))
```

```{r jokes-4-topn, dependson = "jokes-topic-4", echo = FALSE}
jokes_lda4_td <- tidy(jokes_lda4)

top_terms <- jokes_lda4_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(
    topic = factor(topic),
    term = reorder_within(term, beta, topic)
  ) %>%
  ggplot(aes(term, beta, fill = topic)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~topic, scales = "free", ncol = 2) +
  coord_flip()
```

---

# $k=12$

```{r jokes-topic-12, dependson = "jokes-dtm", echo = FALSE}
jokes_lda12 <- LDA(jokes_dtm, k = 12, control = list(seed = 123))
```

```{r jokes-12-topn, dependson = "jokes-topic-12", echo = FALSE}
jokes_lda12_td <- tidy(jokes_lda12)

top_terms <- jokes_lda12_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(
    topic = factor(topic),
    term = reorder_within(term, beta, topic)
  ) %>%
  ggplot(aes(term, beta, fill = topic)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~topic, scales = "free", ncol = 3) +
  coord_flip() +
  theme_minimal(base_size = rcfss::base_size * .8)
```

---

# Perplexity

* A statistical measure of how well a probability model predicts a sample
* Given the theoretical word distributions represented by the topics, compare that to the actual topic mixtures, or distribution of words in your documents
* Perplexity for LDA model with 12 topics
    * `r perplexity(jokes_lda12)`

---

# Perplexity

```{r jokes-lda-compare, dependson = "jokes-dtm", echo = FALSE}
n_topics <- c(2, 4, 10, 20, 50, 100)

# cache the models and only estimate if they don't already exist
if (file.exists(here("static", "extras", "jokes_lda_compare.Rdata"))) {
  load(file = here("static", "extras", "jokes_lda_compare.Rdata"))
} else {
  plan(multiprocess)

  tic()
  jokes_lda_compare <- n_topics %>%
    future_map(LDA, x = jokes_dtm, control = list(seed = 123))
  toc()
  save(jokes_lda_compare, file = here("static", "extras", "jokes_lda_compare.Rdata"))
}
```

```{r jokes_lda_compare_viz, dependson="jokes_lda_compare", echo = FALSE} 
tibble(
  k = n_topics,
  perplex = map_dbl(jokes_lda_compare, perplexity)
) %>%
  ggplot(aes(k, perplex)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Evaluating LDA topic models",
    subtitle = "Optimal number of topics (smaller is better)",
    x = "Number of topics",
    y = "Perplexity"
  )
```

---

# $k=100$

```{r jokes-100-topn, dependson = "jokes-lda-compare", echo = FALSE}
jokes_lda_td <- tidy(jokes_lda_compare[[6]])

top_terms <- jokes_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  filter(topic <= 12) %>%
  mutate(
    topic = factor(topic),
    term = reorder_within(term, beta, topic)
  ) %>%
  ggplot(aes(term, beta, fill = topic)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~topic, scales = "free", ncol = 3) +
  coord_flip() +
  theme_minimal(base_size = rcfss::base_size * .8)
```

---

# LDAvis

* Interactive visualization of LDA model results
1. What is the meaning of each topic?
1. How prevalent is each topic?
1. How do the topics relate to each other?
