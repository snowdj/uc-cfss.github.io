---
title: "Text analysis: classification and topic modeling"
author: "[MACS 30500](https://cfss.uchicago.edu) <br /> University of Chicago"
output: rcfss::xaringan
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  cache = TRUE,
  message = FALSE,
  warning = FALSE,
  echo = FALSE,
  fig.retina = 2, fig.width = 12
)

library(tidyverse)
library(tidytext)
library(caret)
library(rjson)
library(topicmodels)
library(tm)
library(here)
library(patchwork)
library(tictoc)

set.seed(1234)
theme_set(theme_minimal(base_size = rcfss::base_size))
```

# Supervised learning

1. Hand-code a small set of documents $N = 1000$
1. Train a statistical learning model on the hand-coded data
1. Evaluate the effectiveness of the statistical learning model
1. Apply the final model to the remaining set of documents $N = 1000000$

---

# `USCongress`

```{r get-docs}
# get USCongress data
data(USCongress, package = "rcfss")

# topic labels
major_topics <- tibble(
  major = c(1:10, 12:21, 99),
  label = c("Macroeconomics", "Civil rights, minority issues, civil liberties",
            "Health", "Agriculture", "Labor and employment", "Education", "Environment",
            "Energy", "Immigration", "Transportation", "Law, crime, family issues",
            "Social welfare", "Community development and housing issues",
            "Banking, finance, and domestic commerce", "Defense",
            "Space, technology, and communications", "Foreign trade",
            "International affairs and foreign aid", "Government operations",
            "Public lands and water management", "Other, miscellaneous")
) %>%
  mutate(label = factor(major, levels = major, labels = label))

congress <- as_tibble(USCongress) %>%
  mutate(text = as.character(text)) %>%
  left_join(major_topics)
glimpse(congress)
```

```{r docs-example, dependson = "get-docs"}
head(congress$text)
```

---

# Create tidy text data frame

```{r convert-tidytext, dependson = "get-docs", echo = TRUE}
(congress_tokens <- congress %>%
   unnest_tokens(output = word, input = text) %>%
   # remove numbers
   filter(!str_detect(word, "^[0-9]*$")) %>%
   # remove stop words
   anti_join(stop_words) %>%
   # stem the words
   mutate(word = SnowballC::wordStem(word)))
```

---

# Create document-term matrix

```{r dtm, dependson = "convert-tidytext", echo = TRUE}
(congress_dtm <- congress_tokens %>%
   # get count of each token in each document
   count(ID, word) %>%
   # create a document-term matrix with all features and tf weighting
   cast_dtm(document = ID, term = word, value = n))
```

---

# Weighting

* Term frequency (tf)
* Term frequency-inverse document frequency (tf-idf)

---

# Weighting

```{r dtm-tf-idf, dependson = "convert-tidytext", echo = TRUE}
congress_tokens %>%
  count(ID, word) %>%
  cast_dtm(document = ID, term = word, value = n,
           weighting = tm::weightTfIdf)
```

---

# Sparsity

```{r removesparseterms, dependson = "convert-tidytext", echo = TRUE, collapse = TRUE}
removeSparseTerms(congress_dtm, sparse = .95)
removeSparseTerms(congress_dtm, sparse = .90)
```

```{r remove-sparse-terms-final, dependson = "convert-tidytext", echo = TRUE}
(congress_dtm <- removeSparseTerms(congress_dtm, sparse = .99))
```

---

# Exploratory analysis

```{r bind-tf-idf, dependson = "convert-tidytext"}
congress_tfidf <- congress_tokens %>%
  count(label, word) %>%
  bind_tf_idf(term = word, document = label, n = n)
```

```{r plot-tf-idf, dependson = "bind-tf-idf"}
# sort the data frame and convert word to a factor column
plot_congress <- congress_tfidf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))

# graph the top 10 tokens for 4 categories
plot_congress %>%
  filter(label %in% c("Macroeconomics",
                      "Civil rights, minority issues, civil liberties",
                      "Health", "Education")) %>%
  group_by(label) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder_within(word, tf_idf, label)) %>%
  ggplot(aes(word, tf_idf)) +
  geom_col() +
  scale_x_reordered() +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~ label, scales = "free") +
  coord_flip()
```

---

# Estimate model

```r
congress_rf <- train(x = as.matrix(congress_dtm),
                     y = factor(congress$major),
                     method = "ranger",
                     num.trees = 200,
                     trControl = trainControl(method = "oob"))
```

```{r rf-train-10, dependson = "remove-sparse-terms-final"}
# some documents are lost due to not having any relevant tokens after tokenization
# make sure to remove their associated labels so we have the same number of observations
congress_slice <- slice(congress, as.numeric(congress_dtm$dimnames$Docs))

congress_rf_10 <- train(x = as.matrix(congress_dtm),
                        y = factor(congress_slice$major),
                        method = "ranger",
                        num.trees = 10,
                        importance = "impurity",
                        trControl = trainControl(method = "oob"))
```

```{r rf-train-200, dependson = "remove-sparse-terms-final", results = "hide"}
congress_rf_200 <- train(x = as.matrix(congress_dtm),
                         y = factor(congress_slice$major),
                         method = "ranger",
                         num.trees = 200,
                         importance = "impurity",
                         trControl = trainControl(method = "oob"))
```

---

# Evaluate model

```{r rf-error, dependson = "rf-train-200"}
congress_rf_200$finalModel
```

---

# Evaluate model

```{r rf-varimp, dependson = "rf-train-200"}
congress_rf_10$finalModel %>%
  # extract variable importance metrics
  ranger::importance() %>%
  # convert to a data frame
  enframe(name = "variable", value = "varimp") %>%
  top_n(n = 20, wt = varimp) %>%
  # plot the metrics
  ggplot(aes(x = fct_reorder(variable, varimp), y = varimp)) +
  geom_col() +
  coord_flip() +
  labs(x = "Token",
       y = "Variable importance (higher is more important)")
```

---

# Topic modeling

* Themes
* Probabilistic topic models
* Latent Dirichlet allocation

---

# Food and animals

1. I ate a banana and spinach smoothie for breakfast.
1. I like to eat broccoli and bananas.
1. Chinchillas and kittens are cute.
1. My sister adopted a kitten yesterday.
1. Look at this cute hamster munching on a piece of broccoli.

---

# LDA document structure

* Decide on the number of words N the document will have
    * [Dirichlet probability distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution)
    * Fixed set of $k$ topics
* Generate each word in the document:
    * Pick a topic
    * Generate the word
* LDA backtracks from this assumption

---

```{r dtm-lda, dependson = "convert-tidytext"}
# remove terms with low tf-idf for future LDA model
congress_tokens_lite <- congress_tokens %>%
  count(major, word) %>%
  bind_tf_idf(term = word, document = major, n = n) %>%
  group_by(major) %>%
  top_n(40, wt = tf_idf) %>%
  ungroup %>%
  count(word) %>%
  select(-n) %>%
  left_join(congress_tokens)

congress_dtm <- congress_tokens_lite %>%
  # get count of each token in each document
  count(ID, word) %>%
  # create a document-term matrix with all features and tf weighting
  cast_dtm(document = ID, term = word, value = n)
```

# 20 topic LDA model

```{r congress-lda, dependson = "dtm", echo = TRUE}
library(topicmodels)
congress_lda <- LDA(congress_dtm, k = 20, control = list(seed = 123))
congress_lda
```

---

# Top terms per topic

```{r congress-20-topn, dependson = "congress-20-topic", fig.height = 8}
congress_lda_td <- tidy(congress_lda)

top_terms <- congress_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(topic = factor(topic),
         term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = topic)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  coord_flip() +
  theme_minimal(base_size = rcfss::base_size * .7)
```

---

# Document classification

```{r congress-gamma, dependson = "congress-lda"}
congress_gamma <- tidy(congress_lda, matrix = "gamma")
```

```{r congress-model-compare, dependson = "congress-gamma"}
congress_tokens_lite %>%
  mutate(document = as.character(row_number())) %>%
  # join with the gamma values
  left_join(congress_gamma) %>%
  # remove missing values
  drop_na() %>%
  group_by(topic, major, label) %>%
  summarize(gamma = median(gamma)) %>%
  # plot the topic distributions for each policy topic
  ggplot(aes(factor(topic), gamma)) +
  geom_segment(aes(x = factor(topic), xend = factor(topic), y = 0, yend = gamma), color = "grey50") +
  geom_point() +
  facet_wrap(~ label) +
  labs(x = "LDA topic",
       y = expression(gamma)) +
  theme_minimal(base_size = rcfss::base_size * .6)
```

---

# `r/jokes`

<blockquote class="reddit-card" data-card-created="1552319072"><a href="https://www.reddit.com/r/Jokes/comments/a593r0/twenty_years_from_now_kids_are_gonna_think_baby/">Twenty years from now, kids are gonna think "Baby it's cold outside" is really weird, and we're gonna have to explain that it has to be understood as a product of its time.</a> from <a href="http://www.reddit.com/r/Jokes">r/Jokes</a></blockquote>
<script async src="//embed.redditmedia.com/widgets/platform.js" charset="UTF-8"></script>

---

# `r/jokes` dataset

```{r jokes}
# obtain r/jokes and extract values from the JSON file
jokes_json <- fromJSON(file = "https://github.com/taivop/joke-dataset/raw/master/reddit_jokes.json")

jokes <- jokes_json %>%
  {
    tibble(
      id = map_chr(., "id"),
      title = map_chr(., "title"),
      body = map_chr(., "body"),
      score = map_dbl(., "score")
    )
  }
glimpse(jokes)
```

```{r jokes-dtm, dependson = "jokes"}
# convert into a document-term matrix
set.seed(123)
n_grams <- 1:5                          # extract n-grams for n=1,2,3,4,5
jokes_lite <- sample_n(jokes, 50000)    # randomly sample only 50,000 jokes

jokes_tokens <- map_df(n_grams, ~ jokes_lite %>%
                         # combine title and body
                         unite(col = joke, title, body, sep = " ") %>%
                         # tokenize
                         unnest_tokens(output = word,
                                       input = joke,
                                       token = "ngrams",
                                       n = .x) %>%
                         mutate(ngram = .x,
                                token_id = row_number()) %>%
                         # remove tokens that are missing values
                         drop_na(word))

# remove stop words or n-grams beginning or ending with stop word
jokes_stop_words <- jokes_tokens %>%
  # separate ngrams into separate columns
  separate(col = word,
           into = c("word1", "word2", "word3", "word4", "word5"),
           sep = " ") %>%
  # find last word
  mutate(last = if_else(ngram == 5, word5,
                        if_else(ngram == 4, word4,
                                if_else(ngram == 3, word3,
                                        if_else(ngram == 2, word2, word1))))) %>%
  # remove tokens where the first or last word is a stop word
  filter(word1 %in% stop_words$word |
           last %in% stop_words$word) %>%
  select(ngram, token_id)

# convert to dtm
jokes_dtm <- jokes_tokens %>%
  # remove stop word tokens
  anti_join(jokes_stop_words) %>%
  # get count of each token in each document
  count(id, word) %>%
  # create a document-term matrix with all features and tf weighting
  cast_dtm(document = id, term = word, value = n) %>%
  removeSparseTerms(sparse = .999)

# remove documents with no terms remaining
jokes_dtm <- jokes_dtm[unique(jokes_dtm$i),]
jokes_dtm
```

---

# $k=4$

```{r jokes-topic-4, dependson = "jokes-dtm"}
jokes_lda4 <- LDA(jokes_dtm, k = 4, control = list(seed = 123))
```

```{r jokes-4-topn, dependson = "jokes-topic-4"}
jokes_lda4_td <- tidy(jokes_lda4)

top_terms <- jokes_lda4_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(topic = factor(topic),
         term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = topic)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~ topic, scales = "free", ncol = 2) +
  coord_flip()
```

---

# $k=12$

```{r jokes-topic-12, dependson = "jokes-dtm"}
jokes_lda12 <- LDA(jokes_dtm, k = 12, control = list(seed = 123))
```

```{r jokes-12-topn, dependson = "jokes-topic-12"}
jokes_lda12_td <- tidy(jokes_lda12)

top_terms <- jokes_lda12_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(topic = factor(topic),
         term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = topic)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~ topic, scales = "free", ncol = 3) +
  coord_flip() +
  theme_minimal(base_size = rcfss::base_size * .8)
```

---

# Perplexity

* A statistical measure of how well a probability model predicts a sample
* Given the theoretical word distributions represented by the topics, compare that to the actual topic mixtures, or distribution of words in your documents
* Perplexity for LDA model with 12 topics
    * `r perplexity(jokes_lda12)`

---

# Perplexity

```{r jokes-lda-compare, dependson = "jokes-dtm"}
n_topics <- c(2, 4, 10, 20, 50, 100)

# cache the models and only estimate if they don't already exist
if (file.exists(here("static", "extras", "jokes_lda_compare.Rdata"))) {
  load(file = here("static", "extras", "jokes_lda_compare.Rdata"))
} else {
  plan(multiprocess)

  tic()
  jokes_lda_compare <- n_topics %>%
    future_map(LDA, x = jokes_dtm, control = list(seed = 1234))
  toc()
  save(jokes_lda_compare, file = here("static", "extras", "jokes_lda_compare.Rdata"))
}
```

```{r jokes_lda_compare_viz, dependson="jokes_lda_compare"} 
tibble(k = n_topics,
       perplex = map_dbl(jokes_lda_compare, perplexity)) %>%
  ggplot(aes(k, perplex)) +
  geom_point() +
  geom_line() +
  labs(title = "Evaluating LDA topic models",
       subtitle = "Optimal number of topics (smaller is better)",
       x = "Number of topics",
       y = "Perplexity")
```

---

# $k=100$

```{r jokes-100-topn, dependson = "jokes-lda-compare"}
jokes_lda_td <- tidy(jokes_lda_compare[[6]])

top_terms <- jokes_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  filter(topic <= 12) %>%
  mutate(topic = factor(topic),
         term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = topic)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~ topic, scales = "free", ncol = 3) +
  coord_flip() +
  theme_minimal(base_size = rcfss::base_size * .8)
```

---

# LDAvis

* Interactive visualization of LDA model results
1. What is the meaning of each topic?
1. How prevalent is each topic?
1. How do the topics relate to each other?

