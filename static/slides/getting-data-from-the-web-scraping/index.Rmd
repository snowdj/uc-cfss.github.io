---
title: "Getting data from the web: scraping"
author: "[MACS 30500](https://cfss.uchicago.edu) <br /> University of Chicago"
output: rcfss::xaringan
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, htmltools.preserve.raw = FALSE)
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      fig.retina = 2, fig.width = 12)

library(tidyverse)
library(rvest)
library(countdown)

set.seed(1234)
theme_set(theme_minimal(base_size = rcfss::base_size))
```

# Web scraping

* Data on a website with no API
* Still want a programatic, reproducible way to obtain data
* Ability to scrape depends on the quality of the website

---

# HTML

.center[

![[tags](https://xkcd.com/1144/)](http://imgs.xkcd.com/comics/tags.png)

]

---

# Process of HTML

1. The web browser sends a request to the server that hosts the website
1. The server sends the browser an HTML document
1. The browser uses instructions in the HTML to render the website

---

# Components of HTML code

```html
<html>
  <head>
    <title>Title</title>
    <link rel="icon" type="icon" href="http://a" />
    <script src="https://c.js"></script>
  </head>
  <body>
    <div>
      <p>Click <b>here</b> now.</p>
      <span>Frozen</span>
    </div>
    <table style="width:100%">
      <tr>
        <td>Kristen</td>
        <td>Bell</td>
      </tr>
    </table>
  <img src="http://ia.media-imdb.com/images.png"/>
  </body>
</html>
```

---

# Components of HTML code

```html
<a href="http://github.com">GitHub</a>
```

* `<a></a>` - tag name
* `href` - attribute (name)
* `"http://github.com"` - attribute (value)
* `GitHub` - content

---

# Nested structure of HTML

* `html`
    * `head`
        * `title`
        * `link`
        * `script`
    * `body`
        * `div`
            * `p`
                * `b`
            * `span`
        * `table`
            * `tr`
                * `td`
                * `td`
        * `img`

---

# Find the content "here"

* `html`
    * `head`
        * `title`
        * `link`
        * `script`
    * `body`
        * `div`
            * `p`
                * <span style="color:red">**`b`**</span>
            * `span`
        * `table`
            * `tr`
                * `td`
                * `td`
        * `img`

---

# Find the source code

.center[

![scale: 50%](/img/frozen_bell.png)

]

```{r echo = FALSE}
countdown(minutes = 3)
```

---

# HTML only

![HTML only](/img/shiny-css-none.png)

---

# HTML + CSS

![HTML + CSS](/img/shiny-css.png)

---

# CSS code

```css
span {
  color: #ffffff;
}

.num {
  color: #a8660d;
}

table.data {
  width: auto;
}

#firstname {
  background-color: yellow;
}
```

---

# CSS code

```html
<span class="bigname" id="shiny">Shiny</span>
```

* `<span></span>` - tag name
* `bigname` - class (optional)
* `shiny` - id (optional)

---

# CSS selectors

```css
span
```

```css
.bigname
```

```css
span.bigname
```

```css
#shiny
```

---

# CSS selectors

Prefix | Matches
-------|--------
none   | tag
.      | class
#      | id

> [CSS diner](http://flukeout.github.io)

---

# Find the CSS selector

.center[

![scale: 50%](/img/frozen_bell.png)

]

```{r echo = FALSE}
countdown(minutes = 3)
```

---

# `rvest`

1. Download the HTML and turn it into an XML file with `read_html()`
1. Extract specific nodes with `html_nodes()`
1. Extract content from nodes with various functions

---

# Download the HTML

```{r frozen-dl}
library(rvest)
frozen <- read_html("http://www.imdb.com/title/tt2294629/")
frozen
```

---

# Extract nodes

```{r frozen-nodes, dependson = "frozen-dl"}
itals <- html_nodes(frozen, "em")
itals
```

---

# Extract content from nodes

```{r frozen-content, dependson = c("frozen-dl", "frozen-nodes"), collapse = TRUE}
itals
html_text(itals)
html_attr(itals, "class")
```

---

# Extract content

1. Read in the *Frozen* HTML
1. Select the nodes that are both `id = "titleCast"` and `a`s
1. Extract the text from the nodes

```{r echo = FALSE}
countdown(minutes = 3)
```

---

# Extract content

```{r frozen-extract}
library(rvest)
frozen <- read_html("http://www.imdb.com/title/tt2294629/")
cast <- html_nodes(frozen, "#titleCast a")
html_text(cast)
```

---

# SelectorGadget

* GUI tool used to identify CSS selector combinations from a webpage
1. Run `vignette("selectorgadget")`
1. Drag **SelectorGadget** link into your browser's bookmark bar

---

# Using SelectorGadget

1. Navigate to a webpage
1. Open the SelectorGadget bookmark
1. Click on the item to scrape
1. Click on yellow items you do not want to scrape
1. Click on additional items that you do want to scrape
1. Rinse and repeat until only the items you want to scrape are highlighted in yellow
1. Copy the selector to use with `html_nodes()`

---

# Practice using SelectorGadget

> Use SelectorGadget to find a CSS selector combination that identifies just the cast member names

```{r echo = FALSE}
countdown(minutes = 3)
```

---

# Practice using SelectorGadget

```{r frozen-selectorgadget, dependson = "frozen-extract"}
cast2 <- html_nodes(frozen, "#titleCast td:nth-child(2) a")
html_text(cast2)

# remove whitespace
html_text(cast2) %>% str_trim(side = "both")
```

---

# Practice scraping data

1. Look up the cost of living for your hometown on [Sperling's Best Places](http://www.bestplaces.net/)
1. Extract it with `html_nodes()` and `html_text()`

```{r echo = FALSE}
countdown(minutes = 4)
```

---

# Practice scraping data

```{r sterling}
sterling <- read_html("http://www.bestplaces.net/cost_of_living/city/virginia/sterling")

col <- html_nodes(sterling, css = "#mainContent_dgCostOfLiving tr:nth-child(2) td:nth-child(2)")
html_text(col)

# or use a piped operation
sterling %>%
  html_nodes(css = "#mainContent_dgCostOfLiving tr:nth-child(2) td:nth-child(2)") %>%
  html_text()
```

---

# Tables

```{r sterling-table, dependson = "sterling"}
tables <- html_nodes(sterling, css = "table")

tables %>%
  # get the first table
  nth(1) %>%
  # convert to data frame
  html_table(header = TRUE)
```

---

# Extract climate statistics

> Extract the climate statistics of your hometown as a data frame with useful column names

```{r echo = FALSE}
countdown(minutes = 3)
```

---

# Extract climate statistics

```{r sterling-climate, collapse = TRUE}
sterling_climate <- read_html("http://www.bestplaces.net/climate/city/virginia/sterling")

climate <- html_nodes(sterling_climate, css = "table")
html_table(climate, header = TRUE, fill = TRUE)[[1]]

sterling_climate %>%
  html_nodes(css = "table") %>%
  nth(1) %>%
  html_table(header = TRUE)
```

---

# Random observations on scraping

* Make sure you've obtained only what you want
* If you are having trouble parsing, try selecting a smaller subset of the thing you are seeking
* Confirm that there is no R package and no API





